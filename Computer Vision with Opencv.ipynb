{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ab9ec5-79e8-4a3b-b904-f7a9c648d628",
   "metadata": {},
   "source": [
    "# Computer Vision with Opencv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf3953-a1bc-430f-a72a-42c2c9feb631",
   "metadata": {},
   "source": [
    "OpenCV (Open Source Computer Vision Library) is an open-source computer vision and machine learning software library. It provides a wide range of tools and algorithms for various computer vision tasks, including image and video manipulation, object detection, tracking, machine learning, and more. Below, I'll provide an overview of OpenCV along with some key equations and information:\r\n",
    "\r\n",
    "### 1. **Image Representation in OpenCV**:\r\n",
    "   - **Color Spaces**: OpenCV supports various color spaces, including RGB, BGR, grayscale, HSV, LAB, YUV, etc.\r\n",
    "   - **Image Storage**: Images are represented as NumPy arrays, with dimensions (height, width, channels) for color images, and (height, width) for grayscale images.\r\n",
    "   - **Data Types**: Pixel values are typically represented as unsigned 8-bit integers (0-255) or floating-point numbers (0.0-1.0).\r\n",
    "\r\n",
    "### 2. **Image Input and Output**:\r\n",
    "   - **Reading and Writing Images**: OpenCV provides functions like `cv2.imread()` and `cv2.imwrite()` for reading and writing images in various formats (JPEG, PNG, BMP, etc.).\r\n",
    "\r\n",
    "### 3. **Image Processing**:\r\n",
    "   - **Filtering and Convolution**: OpenCV offers functions for applying linear filters and convolutions to images, including Gaussian blur, median blur, Sobel edge detection, etc.\r\n",
    "   - **Thresholding**: Thresholding techniques can be used to segment images into binary masks based on pixel intensity.\r\n",
    "   - **Morphological Operations**: Erosion, dilation, opening, and closing operations for shape analysis and noise reduction.\r\n",
    "\r\n",
    "### 4. **Feature Detection and Description**:\r\n",
    "   - **Feature Detection**: OpenCV provides implementations of feature detection algorithms like Harris corner detector, Shi-Tomasi corner detector, FAST, etc.\r\n",
    "   - **Feature Description**: Feature descriptors like SIFT, SURF, ORB, and BRISK are available for describing keypoints detected in images.\r\n",
    "\r\n",
    "### 5. **Object Detection and Recognition**:\r\n",
    "   - **Cascade Classifiers**: OpenCV includes pre-trained Haar cascade classifiers for detecting objects like faces, eyes, and cars.\r\n",
    "   - **Deep Learning Models**: Integration with deep learning frameworks like TensorFlow and PyTorch allows using state-of-the-art object detection models (e.g., SSD, YOLO, Faster R-CNN).\r\n",
    "\r\n",
    "### 6. **Image Transformation and Geometric Operations**:\r\n",
    "   - **Image Resizing and Scaling**: Functions for resizing and scaling images to desired dimensions.\r\n",
    "   - **Rotation and Affine Transformation**: OpenCV provides functions for rotating images and applying affine transformations.\r\n",
    "\r\n",
    "### 7. **Camera Calibration and 3D Vision**:\r\n",
    "   - **Camera Calibration**: Functions for camera calibration, including intrinsic and extrinsic parameter estimation using chessboard or circle grid patterns.\r\n",
    "   - **Stereo Vision**: OpenCV supports stereo vision techniques for depth estimation and 3D reconstruction using calibrated stereo camera systems.\r\n",
    "\r\n",
    "### 8. **Video Processing**:\r\n",
    "   - **Video Input and Output**: Functions for reading and writing videos, and capturing frames from live camera streams.\r\n",
    "   - **Video Analysis**: Techniques for object tracking, motion detection, and optical flow estimation in video sequences.\r\n",
    "\r\n",
    "### 9. **Machine Learning and Deep Learning**:\r\n",
    "   - **Integration with ML Libraries**: OpenCV provides interfaces for using machine learning algorithms from libraries like scikit-learn.\r\n",
    "   - **Deep Neural Networks**: Support for deep learning models and frameworks like TensorFlow, Keras, and PyTorch.\r\n",
    "\r\n",
    "### 10. **Graphical User Interface (GUI)**:\r\n",
    "   - **HighGUI Module**: OpenCV's HighGUI module provides functions for creating graphical user interfaces for visualizing images, videos, and processing results.\r\n",
    "\r\n",
    "### 11. **Performance Optimization**:\r\n",
    "   - **Hardware Acceleration**: OpenCV supports hardware acceleration using technologies like Intel OpenVINO and CUDA for NVIDIA GPUs.\r\n",
    "   - **Multithreading**: Functions are optimized for multithreading to leverage multicore CPUs.\r\n",
    "\r\n",
    "OpenCV documentation and tutorials provide detailed explanations of algorithms and functions along with examples of usage. While I've mentioned some common equations and concepts, specific equations may vary depending on the algorithm or technique being implemented. If you have any specific equations or concepts you'd like to explore further, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1260041a-4efe-4248-b415-634d7f6679b0",
   "metadata": {},
   "source": [
    "# Lists of all the topics in Computer Vision with Opencv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf8b3b-ce04-43a7-9b69-bd350bcae4f3",
   "metadata": {},
   "source": [
    "Certainly! Here's a comprehensive list of topics in computer vision with OpenCV:\r\n",
    "\r\n",
    "1. **Image Input and Output**:\r\n",
    "   - Reading and writing images in various formats\r\n",
    "   - Displaying images in windows\r\n",
    "   - Video input and output\r\n",
    "\r\n",
    "2. **Image Processing**:\r\n",
    "   - Filtering and convolution\r\n",
    "   - Thresholding and binary image operations\r\n",
    "   - Morphological operations (erosion, dilation, opening, closing)\r\n",
    "   - Image transformations (resizing, rotation, cropping)\r\n",
    "   - Histogram equalization and image enhancement\r\n",
    "   - Image gradients and edge detection\r\n",
    "\r\n",
    "3. **Feature Detection and Description**:\r\n",
    "   - Corner detection (Harris corner detector, Shi-Tomasi corner detector)\r\n",
    "   - Blob detection\r\n",
    "   - Scale-invariant feature detection (SIFT, SURF)\r\n",
    "   - Binary robust independent elementary features (BRISK)\r\n",
    "   - Oriented FAST and rotated BRIEF (ORB)\r\n",
    "\r\n",
    "4. **Object Detection and Tracking**:\r\n",
    "   - Haar cascade classifiers for object detection (e.g., face detection)\r\n",
    "   - Histogram of Oriented Gradients (HOG) for object detection\r\n",
    "   - Deep learning-based object detection models (YOLO, SSD, Faster R-CNN)\r\n",
    "   - Object tracking algorithms (KCF, CSRT, MIL)\r\n",
    "\r\n",
    "5. **Image Segmentation**:\r\n",
    "   - Thresholding techniques\r\n",
    "   - Contour detection and extraction\r\n",
    "   - Region-based segmentation\r\n",
    "   - GrabCut algorithm for interactive segmentation\r\n",
    "\r\n",
    "6. **Camera Calibration and 3D Vision**:\r\n",
    "   - Camera calibration (intrinsic and extrinsic parameters)\r\n",
    "   - Stereo vision and depth estimation\r\n",
    "   - Structure from Motion (SfM)\r\n",
    "   - Epipolar geometry and fundamental matrix estimation\r\n",
    "\r\n",
    "7. **Motion Analysis**:\r\n",
    "   - Optical flow estimation\r\n",
    "   - Dense optical flow algorithms (Farneback, Lucas-Kanade)\r\n",
    "   - Background subtraction and motion detection\r\n",
    "\r\n",
    "8. **Feature Matching and Homography**:\r\n",
    "   - Feature matching algorithms (Brute-Force, FLANN)\r\n",
    "   - Homography estimation for image registration\r\n",
    "   - Perspective transformation and image warping\r\n",
    "\r\n",
    "9. **Machine Learning and Deep Learning Integration**:\r\n",
    "   - Interface with machine learning libraries (scikit-learn)\r\n",
    "   - Integration with deep learning frameworks (TensorFlow, Keras, PyTorch)\r\n",
    "   - Fine-tuning pre-trained deep learning models\r\n",
    "\r\n",
    "10. **Graphical User Interface (GUI)**:\r\n",
    "    - Displaying images and results in windows\r\n",
    "    - Handling mouse events for interactive applications\r\n",
    "    - Creating custom GUI elements for user interaction\r\n",
    "\r\n",
    "11. **Performance Optimization**:\r\n",
    "    - Hardware acceleration (OpenVINO, CUDA)\r\n",
    "    - Multithreading for parallel processing\r\n",
    "    - Memory management and efficient data structures\r\n",
    "\r\n",
    "12. **Augmented Reality (AR) and Virtual Reality (VR)**:\r\n",
    "    - Marker-based AR applications\r\n",
    "    - Image and pattern recognition for AR\r\n",
    "    - Integration with game engines for VR development\r\n",
    "\r\n",
    "13. **Object Recognition and Scene Understanding**:\r\n",
    "    - Scene classification and recognition\r\n",
    "    - Object counting and density estimation\r\n",
    "    - Text detection and recognition\r\n",
    "\r\n",
    "14. **Deep Learning-based Image Generation**:\r\n",
    "    - Generative Adversarial Networks (GANs) for image synthesis\r\n",
    "    - Style transfer and artistic rendering\r\n",
    "    - Super-resolution and image inpainting\r\n",
    "\r\n",
    "15. **Embedded and IoT Applications**:\r\n",
    "    - Optimization for embedded platforms (Raspberry Pi, Arduino)\r\n",
    "    - Real-time processing for IoT devices\r\n",
    "    - Edge computing and deployment on resource-constrained devices\r\n",
    "\r\n",
    "These topics cover a wide range of computer vision techniques and applications that can be implemented using OpenCV. Each topic can be explored further with tutorials, documentation, and examples provided by the OpenCV community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e5a14-6f93-4125-b715-0370a4af1621",
   "metadata": {},
   "source": [
    "# Image Representation in OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cea03ca-a10e-4e41-8826-e46376fea66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color image\n",
      "Image dimensions: 1920 x 1080\n",
      "Pixel value at (100, 50): [ 2 31  6]\n",
      "Pixel values in the entire image:\n",
      "[[[  1  32   3]\n",
      "  [ 10  41  12]\n",
      "  [ 12  43  12]\n",
      "  ...\n",
      "  [ 22  39  48]\n",
      "  [ 42  54  64]\n",
      "  [ 30  43  51]]\n",
      "\n",
      " [[  2  33   6]\n",
      "  [  0  31   4]\n",
      "  [  0  28   0]\n",
      "  ...\n",
      "  [ 35  51  63]\n",
      "  [ 51  66  75]\n",
      "  [ 28  44  51]]\n",
      "\n",
      " [[  5  34  11]\n",
      "  [  0  23   0]\n",
      "  [  8  37  12]\n",
      "  ...\n",
      "  [  0  15  27]\n",
      "  [  5  21  33]\n",
      "  [  0  16  25]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[102 178 184]\n",
      "  [104 180 186]\n",
      "  [106 180 184]\n",
      "  ...\n",
      "  [ 75 149 173]\n",
      "  [ 71 147 170]\n",
      "  [ 79 155 178]]\n",
      "\n",
      " [[ 98 175 178]\n",
      "  [101 178 181]\n",
      "  [110 184 188]\n",
      "  ...\n",
      "  [ 83 157 183]\n",
      "  [ 83 158 184]\n",
      "  [ 93 168 194]]\n",
      "\n",
      " [[105 182 185]\n",
      "  [102 179 182]\n",
      "  [107 181 185]\n",
      "  ...\n",
      "  [ 98 172 198]\n",
      "  [ 90 165 191]\n",
      "  [ 93 168 194]]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read an image using OpenCV\n",
    "image_path = \"My.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if the image was successfully loaded\n",
    "if image is None:\n",
    "    print(\"Error: Unable to load image.\")\n",
    "else:\n",
    "    # Get image shape (height, width, channels)\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    # Check if the image is grayscale or color\n",
    "    if channels == 1:\n",
    "        print(\"Grayscale image\")\n",
    "    elif channels == 3:\n",
    "        print(\"Color image\")\n",
    "\n",
    "    # Display image dimensions\n",
    "    print(\"Image dimensions: {} x {}\".format(width, height))\n",
    "\n",
    "    # Display pixel values at specific coordinates\n",
    "    x, y = 100, 50\n",
    "    pixel_value = image[y, x]\n",
    "    print(\"Pixel value at ({}, {}): {}\".format(x, y, pixel_value))\n",
    "\n",
    "    # Display pixel values in the entire image\n",
    "    print(\"Pixel values in the entire image:\")\n",
    "    print(image)\n",
    "\n",
    "    # Display the image using OpenCV\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d06b63c-9d3a-4646-8122-3116d9cc1ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read an image using OpenCV\n",
    "image_path = \"My.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if the image was successfully loaded\n",
    "if image is None:\n",
    "    print(\"Error: Unable to load image.\")\n",
    "else:\n",
    "    # Convert BGR (default) to RGB color space\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert BGR to Grayscale\n",
    "    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert BGR to HSV\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Convert BGR to LAB\n",
    "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # Convert BGR to YUV\n",
    "    yuv_image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "\n",
    "    # Display the original and converted images\n",
    "    cv2.imshow(\"Original Image\", image)\n",
    "    cv2.imshow(\"RGB Image\", rgb_image)\n",
    "    cv2.imshow(\"Grayscale Image\", grayscale_image)\n",
    "    cv2.imshow(\"HSV Image\", hsv_image)\n",
    "    cv2.imshow(\"LAB Image\", lab_image)\n",
    "    cv2.imshow(\"YUV Image\", yuv_image)\n",
    "\n",
    "    # Wait for a key press and close all windows\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7514306f-7600-465c-94e1-a0bdab45c89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dimensions: 1920 x 1080\n",
      "Color image with 3 channels\n",
      "Pixel value at (100, 50): [ 2 31  6]\n",
      "Pixel values in the entire image:\n",
      "[[[  1  32   3]\n",
      "  [ 10  41  12]\n",
      "  [ 12  43  12]\n",
      "  ...\n",
      "  [ 22  39  48]\n",
      "  [ 42  54  64]\n",
      "  [ 30  43  51]]\n",
      "\n",
      " [[  2  33   6]\n",
      "  [  0  31   4]\n",
      "  [  0  28   0]\n",
      "  ...\n",
      "  [ 35  51  63]\n",
      "  [ 51  66  75]\n",
      "  [ 28  44  51]]\n",
      "\n",
      " [[  5  34  11]\n",
      "  [  0  23   0]\n",
      "  [  8  37  12]\n",
      "  ...\n",
      "  [  0  15  27]\n",
      "  [  5  21  33]\n",
      "  [  0  16  25]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[102 178 184]\n",
      "  [104 180 186]\n",
      "  [106 180 184]\n",
      "  ...\n",
      "  [ 75 149 173]\n",
      "  [ 71 147 170]\n",
      "  [ 79 155 178]]\n",
      "\n",
      " [[ 98 175 178]\n",
      "  [101 178 181]\n",
      "  [110 184 188]\n",
      "  ...\n",
      "  [ 83 157 183]\n",
      "  [ 83 158 184]\n",
      "  [ 93 168 194]]\n",
      "\n",
      " [[105 182 185]\n",
      "  [102 179 182]\n",
      "  [107 181 185]\n",
      "  ...\n",
      "  [ 98 172 198]\n",
      "  [ 90 165 191]\n",
      "  [ 93 168 194]]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read an image using OpenCV\n",
    "image_path = \"My.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if the image was successfully loaded\n",
    "if image is None:\n",
    "    print(\"Error: Unable to load image.\")\n",
    "else:\n",
    "    # Display image dimensions\n",
    "    height, width = image.shape[:2]\n",
    "    print(\"Image dimensions: {} x {}\".format(width, height))\n",
    "\n",
    "    # Check if the image is grayscale or color\n",
    "    if len(image.shape) == 2:\n",
    "        print(\"Grayscale image\")\n",
    "    elif len(image.shape) == 3:\n",
    "        print(\"Color image with {} channels\".format(image.shape[2]))\n",
    "\n",
    "    # Display pixel values at specific coordinates\n",
    "    x, y = 100, 50\n",
    "    pixel_value = image[y, x]\n",
    "    print(\"Pixel value at ({}, {}): {}\".format(x, y, pixel_value))\n",
    "\n",
    "    # Display pixel values in the entire image\n",
    "    print(\"Pixel values in the entire image:\")\n",
    "    print(image)\n",
    "\n",
    "    # Display the image using OpenCV\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "542262ba-f4ef-483f-8cea-d73956bda983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values before normalization (unsigned 8-bit integers):\n",
      "[[[  1  32   3]\n",
      "  [ 10  41  12]\n",
      "  [ 12  43  12]\n",
      "  ...\n",
      "  [ 22  39  48]\n",
      "  [ 42  54  64]\n",
      "  [ 30  43  51]]\n",
      "\n",
      " [[  2  33   6]\n",
      "  [  0  31   4]\n",
      "  [  0  28   0]\n",
      "  ...\n",
      "  [ 35  51  63]\n",
      "  [ 51  66  75]\n",
      "  [ 28  44  51]]\n",
      "\n",
      " [[  5  34  11]\n",
      "  [  0  23   0]\n",
      "  [  8  37  12]\n",
      "  ...\n",
      "  [  0  15  27]\n",
      "  [  5  21  33]\n",
      "  [  0  16  25]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[102 178 184]\n",
      "  [104 180 186]\n",
      "  [106 180 184]\n",
      "  ...\n",
      "  [ 75 149 173]\n",
      "  [ 71 147 170]\n",
      "  [ 79 155 178]]\n",
      "\n",
      " [[ 98 175 178]\n",
      "  [101 178 181]\n",
      "  [110 184 188]\n",
      "  ...\n",
      "  [ 83 157 183]\n",
      "  [ 83 158 184]\n",
      "  [ 93 168 194]]\n",
      "\n",
      " [[105 182 185]\n",
      "  [102 179 182]\n",
      "  [107 181 185]\n",
      "  ...\n",
      "  [ 98 172 198]\n",
      "  [ 90 165 191]\n",
      "  [ 93 168 194]]]\n",
      "\n",
      "Pixel values after normalization (floating-point numbers):\n",
      "[[[0.00392157 0.1254902  0.01176471]\n",
      "  [0.03921569 0.16078432 0.04705882]\n",
      "  [0.04705882 0.16862746 0.04705882]\n",
      "  ...\n",
      "  [0.08627451 0.15294118 0.1882353 ]\n",
      "  [0.16470589 0.21176471 0.2509804 ]\n",
      "  [0.11764706 0.16862746 0.2       ]]\n",
      "\n",
      " [[0.00784314 0.12941177 0.02352941]\n",
      "  [0.         0.12156863 0.01568628]\n",
      "  [0.         0.10980392 0.        ]\n",
      "  ...\n",
      "  [0.13725491 0.2        0.24705882]\n",
      "  [0.2        0.25882354 0.29411766]\n",
      "  [0.10980392 0.17254902 0.2       ]]\n",
      "\n",
      " [[0.01960784 0.13333334 0.04313726]\n",
      "  [0.         0.09019608 0.        ]\n",
      "  [0.03137255 0.14509805 0.04705882]\n",
      "  ...\n",
      "  [0.         0.05882353 0.10588235]\n",
      "  [0.01960784 0.08235294 0.12941177]\n",
      "  [0.         0.0627451  0.09803922]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.4        0.69803923 0.72156864]\n",
      "  [0.40784314 0.7058824  0.7294118 ]\n",
      "  [0.41568628 0.7058824  0.72156864]\n",
      "  ...\n",
      "  [0.29411766 0.58431375 0.6784314 ]\n",
      "  [0.2784314  0.5764706  0.6666667 ]\n",
      "  [0.30980393 0.60784316 0.69803923]]\n",
      "\n",
      " [[0.38431373 0.6862745  0.69803923]\n",
      "  [0.39607844 0.69803923 0.70980394]\n",
      "  [0.43137255 0.72156864 0.7372549 ]\n",
      "  ...\n",
      "  [0.3254902  0.6156863  0.7176471 ]\n",
      "  [0.3254902  0.61960787 0.72156864]\n",
      "  [0.3647059  0.65882355 0.7607843 ]]\n",
      "\n",
      " [[0.4117647  0.7137255  0.7254902 ]\n",
      "  [0.4        0.7019608  0.7137255 ]\n",
      "  [0.41960785 0.70980394 0.7254902 ]\n",
      "  ...\n",
      "  [0.38431373 0.6745098  0.7764706 ]\n",
      "  [0.3529412  0.64705884 0.7490196 ]\n",
      "  [0.3647059  0.65882355 0.7607843 ]]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read an image using OpenCV\n",
    "image_path = \"My.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if the image was successfully loaded\n",
    "if image is None:\n",
    "    print(\"Error: Unable to load image.\")\n",
    "else:\n",
    "    # Display pixel values before normalization\n",
    "    print(\"Pixel values before normalization (unsigned 8-bit integers):\")\n",
    "    print(image)\n",
    "\n",
    "    # Normalize pixel values to the range [0, 1]\n",
    "    normalized_image = image.astype(np.float32) / 255.0\n",
    "\n",
    "    # Display pixel values after normalization\n",
    "    print(\"\\nPixel values after normalization (floating-point numbers):\")\n",
    "    print(normalized_image)\n",
    "\n",
    "    # Display the image using OpenCV\n",
    "    cv2.imshow(\"Original Image\", image)\n",
    "    cv2.imshow(\"Normalized Image\", normalized_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1822c-b3b0-46f2-b831-a899e2094fc6",
   "metadata": {},
   "source": [
    "# Image Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b4f66e1-bfce-4658-bc82-0230b2f366bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved successfully as: output_image.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Read an image using OpenCV\n",
    "image_path = \"My.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if the image was successfully loaded\n",
    "if image is None:\n",
    "    print(\"Error: Unable to load image.\")\n",
    "else:\n",
    "    # Display the original image\n",
    "    cv2.imshow(\"Original Image\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Write the image to a new file\n",
    "    output_image_path = \"output_image.jpg\"\n",
    "    cv2.imwrite(output_image_path, image)\n",
    "\n",
    "    # Check if the image was successfully written\n",
    "    if cv2.imread(output_image_path) is not None:\n",
    "        print(\"Image saved successfully as:\", output_image_path)\n",
    "    else:\n",
    "        print(\"Error: Unable to save image.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cffc237-10fb-4435-a453-3e0c7eccf869",
   "metadata": {},
   "source": [
    "# Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9806ab76-7962-45b7-9342-6d5acf86778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Read an image using OpenCV\n",
    "image_path = \"My.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if the image was successfully loaded\n",
    "if image is None:\n",
    "    print(\"Error: Unable to load image.\")\n",
    "else:\n",
    "    # Apply Gaussian blur\n",
    "    blurred_image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "\n",
    "    # Apply median blur\n",
    "    median_blurred_image = cv2.medianBlur(image, 5)\n",
    "\n",
    "    # Apply Sobel edge detection\n",
    "    sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    sobel_edges = cv2.magnitude(sobel_x, sobel_y)\n",
    "\n",
    "    # Display the original and processed images\n",
    "    cv2.imshow(\"Original Image\", image)\n",
    "    cv2.imshow(\"Gaussian Blur\", blurred_image)\n",
    "    cv2.imshow(\"Median Blur\", median_blurred_image)\n",
    "    cv2.imshow(\"Sobel Edges\", sobel_edges.astype('uint8'))\n",
    "\n",
    "    # Wait for a key press and close all windows\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0528673-9ddb-4271-80a8-377194deb45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Read an image using OpenCV\n",
    "image_path = \"My.jpg\"\n",
    "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Check if the image was successfully loaded\n",
    "if image is None:\n",
    "    print(\"Error: Unable to load image.\")\n",
    "else:\n",
    "    # Apply simple thresholding\n",
    "    _, binary_threshold = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply adaptive thresholding\n",
    "    adaptive_threshold = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "    # Display the original and processed images\n",
    "    cv2.imshow(\"Original Image\", image)\n",
    "    cv2.imshow(\"Binary Threshold\", binary_threshold)\n",
    "    cv2.imshow(\"Adaptive Threshold\", adaptive_threshold)\n",
    "\n",
    "    # Wait for a key press and close all windows\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b1a2fee-7071-4f3f-b167-3a7b4f0e2a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read an image using OpenCV\n",
    "image_path = \"My.jpg\"\n",
    "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Check if the image was successfully loaded\n",
    "if image is None:\n",
    "    print(\"Error: Unable to load image.\")\n",
    "else:\n",
    "    # Apply erosion\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    eroded_image = cv2.erode(image, kernel, iterations=1)\n",
    "\n",
    "    # Apply dilation\n",
    "    dilated_image = cv2.dilate(image, kernel, iterations=1)\n",
    "\n",
    "    # Apply opening (erosion followed by dilation)\n",
    "    opened_image = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Apply closing (dilation followed by erosion)\n",
    "    closed_image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Display the original and processed images\n",
    "    cv2.imshow(\"Original Image\", image)\n",
    "    cv2.imshow(\"Eroded Image\", eroded_image)\n",
    "    cv2.imshow(\"Dilated Image\", dilated_image)\n",
    "    cv2.imshow(\"Opened Image\", opened_image)\n",
    "    cv2.imshow(\"Closed Image\", closed_image)\n",
    "\n",
    "    # Wait for a key press and close all windows\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c688a169-ede2-4e66-af02-f7f68e3512af",
   "metadata": {},
   "source": [
    "# Feature Detection and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2087411-cb33-4b91-b94c-bea03442ec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varun\\AppData\\Local\\Temp\\ipykernel_11912\\3608857291.py:17: DeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n",
      "  corners = np.int0(corners)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread('My.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Harris corner detection\n",
    "harris_corners = cv2.cornerHarris(gray, 2, 3, 0.04)  # You can adjust parameters here\n",
    "harris_corners = cv2.dilate(harris_corners, None)  # Dilate corner points to enhance visibility\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "image[harris_corners > 0.01 * harris_corners.max()] = [0, 0, 255]  # Draw red dots on corners\n",
    "\n",
    "# Shi-Tomasi corner detection\n",
    "corners = cv2.goodFeaturesToTrack(gray, 25, 0.01, 10)  # Parameters can be adjusted\n",
    "corners = np.int0(corners)\n",
    "\n",
    "for corner in corners:\n",
    "    x, y = corner.ravel()\n",
    "    cv2.circle(image, (x, y), 3, 255, -1)  # Draw circles on corners\n",
    "\n",
    "cv2.imshow('Harris & Shi-Tomasi Corner Detection', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "731f7772-cb18-419d-9646-fe5366a5f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread('My.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SIFT (Scale-Invariant Feature Transform)\n",
    "sift = cv2.SIFT_create()\n",
    "keypoints_sift, descriptors_sift = sift.detectAndCompute(gray, None)\n",
    "\n",
    "# SURF (Speeded-Up Robust Features)\n",
    "#surf = cv2.xfeatures2d.SURF_create()\n",
    "#keypoints_surf, descriptors_surf = surf.detectAndCompute(gray, None)\n",
    "\n",
    "# ORB (Oriented FAST and Rotated BRIEF)\n",
    "orb = cv2.ORB_create()\n",
    "keypoints_orb, descriptors_orb = orb.detectAndCompute(gray, None)\n",
    "\n",
    "# BRISK (Binary Robust Invariant Scalable Keypoints)\n",
    "brisk = cv2.BRISK_create()\n",
    "keypoints_brisk, descriptors_brisk = brisk.detectAndCompute(gray, None)\n",
    "\n",
    "# Draw keypoints on the image\n",
    "image_sift = cv2.drawKeypoints(image, keypoints_sift, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "#image_surf = cv2.drawKeypoints(image, keypoints_surf, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "image_orb = cv2.drawKeypoints(image, keypoints_orb, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "image_brisk = cv2.drawKeypoints(image, keypoints_brisk, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Display images\n",
    "cv2.imshow('SIFT Keypoints', image_sift)\n",
    "#cv2.imshow('SURF Keypoints', image_surf)\n",
    "cv2.imshow('ORB Keypoints', image_orb)\n",
    "cv2.imshow('BRISK Keypoints', image_brisk)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41660648-c3f3-4f0e-a708-30d7c603def9",
   "metadata": {},
   "source": [
    "# Object Detection and Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74a8b17f-2c4c-4933-b839-08a9cab0841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image_path = \"My.jpg\"\n",
    "window_name = f\"Detected Objects in {image_path}\"\n",
    "original_image = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to grayscale for easier computation\n",
    "image_grey = cv2.cvtColor(original_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "cascade_classifier = cv2.CascadeClassifier(\n",
    "    f\"{cv2.data.haarcascades}haarcascade_eye.xml\")\n",
    "detected_objects = cascade_classifier.detectMultiScale(image_grey, minSize=(50, 50))\n",
    "\n",
    "# Draw rectangles on the detected objects\n",
    "if len(detected_objects) != 0:\n",
    "    for (x, y, width, height) in detected_objects:\n",
    "        cv2.rectangle(original_image, (x, y),\n",
    "                      (x + height, y + width),\n",
    "                      (0, 255, 0), 2)\n",
    "\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_KEEPRATIO)\n",
    "cv2.imshow(window_name, original_image)\n",
    "cv2.resizeWindow(window_name, 400, 400)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3993c08-2372-4f10-8f37-80f4cc741d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 4us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Conv2D, Reshape\n",
    "\n",
    "# Define the model architecture\n",
    "def create_object_detection_model(input_shape, num_classes):\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = Conv2D(1024, (3, 3), activation='relu')(x)\n",
    "    predictions = Conv2D(num_classes, (1, 1), activation='softmax')(x)\n",
    "    predictions = Reshape((-1, num_classes))(predictions)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 20  # Number of classes\n",
    "model = create_object_detection_model(input_shape, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fcfa777-4419-4d5d-8338-b2380183518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\varun/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 528M/528M [16:17<00:00, 566kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Define the model architecture\n",
    "class ObjectDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ObjectDetectionModel, self).__init__()\n",
    "        self.base_model = models.vgg16(pretrained=True)\n",
    "        self.features = self.base_model.features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        return x.view(x.size(0), -1, x.size(-1))\n",
    "\n",
    "# Example usage\n",
    "num_classes = 20  # Number of classes\n",
    "model = ObjectDetectionModel(num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54fd9b7-6d65-4cce-9368-29cdc0d7ee65",
   "metadata": {},
   "source": [
    "#  Image Transformation and Geometric Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd79530f-5c27-4b80-94bc-8f3f14a5873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def resize_image(image, new_width=None, new_height=None, scale_factor=None):\n",
    "    \"\"\"\n",
    "    Resize the image to the specified dimensions.\n",
    "\n",
    "    Args:\n",
    "    - image: Input image.\n",
    "    - new_width: Desired width of the output image.\n",
    "    - new_height: Desired height of the output image.\n",
    "    - scale_factor: Scaling factor to be applied to the image.\n",
    "\n",
    "    Returns:\n",
    "    - Resized image.\n",
    "    \"\"\"\n",
    "    if new_width is not None and new_height is not None:\n",
    "        # Resize based on specific width and height\n",
    "        resized_image = cv2.resize(image, (new_width, new_height))\n",
    "    elif scale_factor is not None:\n",
    "        # Resize based on scale factor\n",
    "        resized_image = cv2.resize(image, None, fx=scale_factor, fy=scale_factor)\n",
    "    else:\n",
    "        raise ValueError(\"Either new_width and new_height or scale_factor must be provided.\")\n",
    "\n",
    "    return resized_image\n",
    "\n",
    "# Load an image\n",
    "image = cv2.imread('My.jpg')\n",
    "\n",
    "# Resize the image to a specific width and height\n",
    "resized_image_wh = resize_image(image, new_width=300, new_height=200)\n",
    "\n",
    "# Resize the image based on a scale factor\n",
    "resized_image_sf = resize_image(image, scale_factor=0.5)\n",
    "\n",
    "# Display the original and resized images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Resized Image (Width, Height)', resized_image_wh)\n",
    "cv2.imshow('Resized Image (Scale Factor)', resized_image_sf)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c25332b-c275-4b9c-8186-b6c1d98248a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    \"\"\"\n",
    "    Rotate the image by the specified angle.\n",
    "\n",
    "    Args:\n",
    "    - image: Input image.\n",
    "    - angle: Angle of rotation in degrees.\n",
    "\n",
    "    Returns:\n",
    "    - Rotated image.\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (w, h))\n",
    "    return rotated_image\n",
    "\n",
    "def apply_affine_transformation(image, translation=(0, 0), scale=1.0, shear=0.0, angle=0.0):\n",
    "    \"\"\"\n",
    "    Apply affine transformation to the image.\n",
    "\n",
    "    Args:\n",
    "    - image: Input image.\n",
    "    - translation: Tuple (tx, ty) specifying the translation in the x and y directions.\n",
    "    - scale: Scaling factor.\n",
    "    - shear: Shearing angle in degrees.\n",
    "    - angle: Rotation angle in degrees.\n",
    "\n",
    "    Returns:\n",
    "    - Transformed image.\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "\n",
    "    # Build the transformation matrix\n",
    "    transformation_matrix = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "    transformation_matrix[:, 2] += translation  # Apply translation\n",
    "\n",
    "    # Apply affine transformation\n",
    "    transformed_image = cv2.warpAffine(image, transformation_matrix, (w, h), flags=cv2.INTER_LINEAR)\n",
    "\n",
    "    return transformed_image\n",
    "\n",
    "# Load an image\n",
    "image = cv2.imread('My.jpg')\n",
    "\n",
    "# Rotate the image by 45 degrees\n",
    "rotated_image = rotate_image(image, angle=45)\n",
    "\n",
    "# Apply affine transformation (translation, scale, shear, and rotation)\n",
    "transformed_image = apply_affine_transformation(image, translation=(50, 50), scale=1.5, shear=20, angle=30)\n",
    "\n",
    "# Display the original, rotated, and transformed images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Rotated Image', rotated_image)\n",
    "cv2.imshow('Transformed Image', transformed_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d81780-92ae-4c51-a32c-1004ea314849",
   "metadata": {},
   "source": [
    "# Camera Calibration and 3D Vision:\r\n",
    "Camera Calibration: Functions for camera calibration, including intrinsic and extrinsic parameter estimation using chessboard or circle grid patterns.\r\n",
    "Stereo Vision: OpenCV supports stereo vision techniques for depth estimation and 3D reconstruction using calibrated stereo camera systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55787721-fea2-4d44-8403-16b2ae88abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ..., (6,5,0)\n",
    "objp = np.zeros((6*7, 3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2)\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d point in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "images = glob.glob('*.jpg')\n",
    "\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (7,6), None)\n",
    "\n",
    "    # If found, add object points, image points (after refining them)\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "\n",
    "        corners2 = cv2.cornerSubPix(gray, corners, (11,11), (-1,-1), criteria)\n",
    "        imgpoints.append(corners2)\n",
    "\n",
    "        # Draw and display the corners\n",
    "        img = cv2.drawChessboardCorners(img, (7,6), corners2, ret)\n",
    "        cv2.imshow('img', img)\n",
    "        cv2.waitKey(500)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Calibrate camera\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938e054-10d2-424d-bd9c-a71aa989a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "img = cv2.imread('My.jpg')\n",
    "\n",
    "# Undistort the image using camera matrix and distortion coefficients\n",
    "undistorted_img = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "\n",
    "# Perform other 3D vision tasks such as depth estimation or 3D reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ff616f-c64e-4338-b534-beee7cf98413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load left and right images\n",
    "left_image = cv2.imread('My.jpg', 0)\n",
    "right_image = cv2.imread('My.jpg', 0)\n",
    "\n",
    "# Stereo calibration (you need to provide calibration parameters)\n",
    "# For stereo calibration, you typically capture a set of images of a calibration pattern\n",
    "# and use them to compute camera matrices and distortion coefficients\n",
    "# Once you have these parameters, you can use them to rectify your stereo images.\n",
    "\n",
    "# Stereo rectification\n",
    "# Once you have the calibration parameters, you can rectify your stereo images to ensure corresponding\n",
    "# points in the two images lie along the same horizontal lines. This simplifies stereo correspondence\n",
    "# by making it a 1D search problem instead of a 2D search problem.\n",
    "\n",
    "# Perform stereo matching to find correspondences between the left and right images\n",
    "stereo = cv2.StereoBM_create(numDisparities=16, blockSize=15)  # Adjust parameters as needed\n",
    "disparity = stereo.compute(left_image, right_image)\n",
    "\n",
    "# Normalize the disparity map for visualization\n",
    "disparity = cv2.normalize(disparity, disparity, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "# Display the disparity map\n",
    "cv2.imshow('Disparity Map', disparity)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9076a1d5-2812-433a-8dc8-43f618f5dfe6",
   "metadata": {},
   "source": [
    "# Video Processing:\r\n",
    "Video Input and Output: Functions for reading and writing videos, and capturing frames from live camera streams.\r\n",
    "Video Analysis: Techniques for object tracking, motion detection, and optical flow estimation in video sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8721e775-7395-43a8-a903-8cdc9fdd6d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Read video from file\n",
    "video_capture = cv2.VideoCapture('My.mp4')\n",
    "\n",
    "# Check if the video file is opened successfully\n",
    "if not video_capture.isOpened():\n",
    "    print(\"Error: Could not open video file.\")\n",
    "    exit()\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
    "total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Define video writer\n",
    "output_video = cv2.VideoWriter('output_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "# Process each frame in the video\n",
    "while video_capture.isOpened():\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform any operations on the frame (e.g., image processing)\n",
    "    # For example, you can convert the frame to grayscale\n",
    "    # gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Write processed frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and writer\n",
    "video_capture.release()\n",
    "output_video.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c627f0-5a85-4d97-aeba-42128b8fa46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('My.mp4')\n",
    "ret,frame=cap.read()\n",
    "x,y,w,h = cv2.selectROI(frame)\n",
    "track_window = (x, y, w, h)\n",
    "roi = frame[y:y+h, x:x+w]\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "mask = cv2.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\n",
    "roi_hist = cv2.calcHist([hsv_roi],[0],mask,[180],[0,180])\n",
    "cv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "while(1):\n",
    "   ret, frame = cap.read()\n",
    "   if ret == True:\n",
    "       hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "       dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "       ret, track_window = cv2.meanShift(dst, track_window, term_crit)\n",
    "       x,y,w,h = track_window\n",
    "       img2 = cv2.rectangle(frame, (x,y), (x+w,y+h), 255,2)\n",
    "       cv2.imshow('img2',img2)\n",
    "       k = cv2.waitKey(30) & 0xff\n",
    "       if k == 27:\n",
    "           break\n",
    "   else:\n",
    "       break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83357c8a-8a66-47fc-b825-c649dd43026c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 47\u001b[0m\n\u001b[0;32m     43\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 47\u001b[0m     motionDetection()\n",
      "Cell \u001b[1;32mIn[10], line 37\u001b[0m, in \u001b[0;36mmotionDetection\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVideo\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame1)\n\u001b[0;32m     36\u001b[0m frame1 \u001b[38;5;241m=\u001b[39m frame2\n\u001b[1;32m---> 37\u001b[0m ret, frame2 \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m50\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m27\u001b[39m:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import cv2\n",
    "import numpy\n",
    "\n",
    "# Defining a function motionDetection\n",
    "def motionDetection():\n",
    "    # capturing video in real time\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # reading frames sequentially\n",
    "    ret, frame1 = cap.read()\n",
    "    ret, frame2 = cap.read()\n",
    "\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # difference between the frames\n",
    "        diff = cv2.absdiff(frame1, frame2)\n",
    "        diff_gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "        blur = cv2.GaussianBlur(diff_gray, (5, 5), 0)\n",
    "        _, thresh = cv2.threshold(blur, 20, 255, cv2.THRESH_BINARY)\n",
    "        dilated = cv2.dilate(thresh, None, iterations=3)\n",
    "        contours, _ = cv2.findContours(\n",
    "            dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        for contour in contours:\n",
    "            (x, y, w, h) = cv2.boundingRect(contour)\n",
    "            if cv2.contourArea(contour) < 900:\n",
    "                continue\n",
    "            cv2.rectangle(frame1, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame1, \"STATUS: {}\".format('MOTION DETECTED'), (10, 60), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1, (217, 10, 10), 2)\n",
    "\n",
    "        # cv.drawContours(frame1, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Video\", frame1)\n",
    "        frame1 = frame2\n",
    "        ret, frame2 = cap.read()\n",
    "\n",
    "        if cv2.waitKey(50) == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    motionDetection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5229fe-1743-43db-a158-53f6aab402fc",
   "metadata": {},
   "source": [
    "# Machine Learning and Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d28d5b-f09c-49f1-8870-d908f449dcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "Prediction: [1]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic data for demonstration\n",
    "X = np.random.rand(100, 10)  # Features\n",
    "y = np.random.randint(0, 2, 100)  # Labels (binary classification)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a machine learning model using scikit-learn\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Now, let's integrate with OpenCV for further image processing or computer vision tasks\n",
    "\n",
    "# Generate a synthetic test image for demonstration\n",
    "test_image = np.random.randint(0, 256, (100, 100, 3), dtype=np.uint8)\n",
    "\n",
    "# Perform some preprocessing or feature extraction on the image (if needed)\n",
    "# For example, you might resize the image or convert it to grayscale\n",
    "\n",
    "# Make predictions on the image using the trained model\n",
    "# Assuming you have extracted features from the image and stored them in 'features'\n",
    "# For the sake of this example, let's assume 'features' is a random feature vector\n",
    "features = np.random.rand(10)  # Random feature vector\n",
    "result = clf.predict([features])\n",
    "\n",
    "# Display the result\n",
    "print(\"Prediction:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fdd8317-ac1b-4e4a-9333-152f87cc24f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1179s\u001b[0m 103us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.8585 - loss: 0.4787\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 871us/step - accuracy: 0.9617 - loss: 0.1262\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 845us/step - accuracy: 0.9723 - loss: 0.0895\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 870us/step - accuracy: 0.9787 - loss: 0.0690\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 847us/step - accuracy: 0.9836 - loss: 0.0534\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - accuracy: 0.9693 - loss: 0.1037\n",
      "Test accuracy: 0.973800003528595\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the architecture of the neural network\n",
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(784,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Load dataset (for example, MNIST)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Flatten the images\n",
    "train_images = train_images.reshape((-1, 784))\n",
    "test_images = test_images.reshape((-1, 784))\n",
    "\n",
    "# Create the model\n",
    "model = create_model()\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "894ee528-691c-4605-b8bf-50e246d405d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 9912422/9912422 [14:52<00:00, 11106.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 28881/28881 [00:06<00:00, 4230.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1648877/1648877 [02:25<00:00, 11326.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 9187.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "[1,  1000] loss: 0.478\n",
      "[2,  1000] loss: 0.214\n",
      "[3,  1000] loss: 0.163\n",
      "[4,  1000] loss: 0.132\n",
      "[5,  1000] loss: 0.117\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 96 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the architecture of the neural network\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load dataset (for example, MNIST)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create the model\n",
    "model = NeuralNet()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:  # Print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Evaluate the model\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.view(-1, 784)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c33a6fe-f157-443d-80a7-44b8fa12c60b",
   "metadata": {},
   "source": [
    "# Graphical User Interface (GUI):\r\n",
    "HighGUI Module: OpenCV's HighGUI module provides functions for creating graphical user interfaces for visualizing images, videos, and processing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f3d57b6-cd74-4327-881e-58b93c38774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def display_image(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Check if image is loaded successfully\n",
    "    if image is None:\n",
    "        print(\"Error: Unable to load image.\")\n",
    "        return\n",
    "\n",
    "    # Display the image in a window\n",
    "    cv2.imshow(\"Image\", image)\n",
    "\n",
    "    # Wait for a key press and close the window when any key is pressed\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Path to the image you want to display\n",
    "image_path = \"My.jpg\"\n",
    "\n",
    "# Display the image\n",
    "display_image(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a731d41-73e4-4eb8-b7ee-91d09e06972c",
   "metadata": {},
   "source": [
    "#  Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "417c0e8e-5272-4051-8484-ad4bf1f2f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def load_and_process_image(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Check if image is loaded successfully\n",
    "    if image is None:\n",
    "        print(\"Error: Unable to load image.\")\n",
    "        return\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Perform some image processing operations\n",
    "    processed_image = cv2.blur(gray_image, (5, 5))\n",
    "\n",
    "    return processed_image\n",
    "\n",
    "def main():\n",
    "    # Path to the image you want to process\n",
    "    image_path = \"My.jpg\"\n",
    "\n",
    "    # Load and process the image\n",
    "    processed_image = load_and_process_image(image_path)\n",
    "\n",
    "    # Display the processed image\n",
    "    cv2.imshow(\"Processed Image\", processed_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f11fe39-a906-4376-9735-8b59b99057e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 0 started\n",
      "Thread 1 started\n",
      "Thread 2 started\n",
      "Thread 3 started\n",
      "Thread 0 finished. Result: 49999995000000\n",
      "Thread 1 finished. Result: 49999995000000\n",
      "Thread 2 finished. Result: 49999995000000\n",
      "Thread 3 finished. Result: 49999995000000\n",
      "All threads have finished execution.\n",
      "Execution time: 1.5782699584960938 seconds\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "# Function to perform some computation (dummy example)\n",
    "def compute_task(data):\n",
    "    result = 0\n",
    "    for i in range(data):\n",
    "        result += i\n",
    "    return result\n",
    "\n",
    "# Function to be executed by each thread\n",
    "def thread_task(data, thread_num):\n",
    "    print(\"Thread {} started\".format(thread_num))\n",
    "    result = compute_task(data)\n",
    "    print(\"Thread {} finished. Result: {}\".format(thread_num, result))\n",
    "\n",
    "def main():\n",
    "    # Data to be processed\n",
    "    data = 10000000\n",
    "\n",
    "    # Number of threads to create\n",
    "    num_threads = 4\n",
    "\n",
    "    # Create and start threads\n",
    "    threads = []\n",
    "    for i in range(num_threads):\n",
    "        thread = threading.Thread(target=thread_task, args=(data, i))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    # Wait for all threads to finish\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    print(\"All threads have finished execution.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    end_time = time.time()\n",
    "    print(\"Execution time:\", end_time - start_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf71aa1-e207-4f83-b555-caa236841203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
