{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "957ef77f-5452-4f02-9227-f6bdaae3c46d",
   "metadata": {},
   "source": [
    "# Video Processing Basics:\r\n",
    "\r\n",
    "Frame extraction\r\n",
    "Frame interpolation\r\n",
    "Frame alignment\r\n",
    "Video compression and decompression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1065fe-dc17-413e-8175-abfeff43af2c",
   "metadata": {},
   "source": [
    "Video processing in computer vision involves analyzing and manipulating video streams to extract useful information. Here's an overview of the basics:\r\n",
    "\r\n",
    "1. **Frame:** A single image in a sequence of images that make up a video.\r\n",
    "\r\n",
    "2. **Frame Rate (fps):** The number of frames displayed per second. Common frame rates are 24, 30, and 60 fps.\r\n",
    "\r\n",
    "3. **Resolution:** The dimensions of the video, typically represented by width x height (e.g., 1920x1080 pixels).\r\n",
    "\r\n",
    "4. **Color Space:** The color representation used in the video, such as RGB (Red, Green, Blue) or YUV (Luminance, Chrominance).\r\n",
    "\r\n",
    "5. **Video Compression:** Reducing the size of video files by removing redundant or irrelevant information. Common video compression standards include MPEG, H.264, and H.265.\r\n",
    "\r\n",
    "6. **Preprocessing:** Techniques applied to frames before further analysis, such as resizing, denoising, or color correction.\r\n",
    "\r\n",
    "7. **Feature Extraction:** Identifying key points or regions in frames that are relevant for further analysis. Common features include edges, corners, or keypoints detected using algorithms like Canny edge detection or Harris corner detection.\r\n",
    "\r\n",
    "8. **Motion Estimation:** Analyzing the movement of objects between frames to track their trajectories. Techniques include optical flow estimation and block matching.\r\n",
    "\r\n",
    "9. **Object Detection and Tracking:** Identifying and following objects of interest across frames. This involves detecting objects in individual frames using techniques like Haar cascades or deep learning-based methods (e.g., YOLO, SSD) and tracking their movements over time.\r\n",
    "\r\n",
    "10. **Background Subtraction:** Separating foreground objects from the background to focus on relevant information. This is commonly used in applications like surveillance and video segmentation.\r\n",
    "\r\n",
    "11. **Video Stabilization:** Removing unwanted camera motion or jitter from the video to produce smoother footage. Techniques include optical flow-based stabilization and gyroscopic data-based stabilization.\r\n",
    "\r\n",
    "12. **Temporal Filtering:** Applying filters to video sequences over time to remove noise or enhance certain characteristics. Examples include temporal averaging and temporal median filtering.\r\n",
    "\r\n",
    "Equations are integral to many aspects of video processing, especially when it comes to algorithms like optical flow estimation or motion tracking. Here are some common equations used in these contexts:\r\n",
    "\r\n",
    "1. **Optical Flow Equation:**\r\n",
    "   \\[\r\n",
    "   I_x \\cdot u + I_y \\cdot v + I_t = 0\r\n",
    "   \\]\r\n",
    "   where \\( I_x \\) and \\( I_y \\) are the spatial gradients of intensity in the x and y directions respectively, \\( I_t \\) is the temporal gradient of intensity, and \\( u \\) and \\( v \\) represent the horizontal and vertical components of optical flow.\r\n",
    "\r\n",
    "2. **Motion Model:**\r\n",
    "   \\[\r\n",
    "   p_{k+1} = p_k + \\Delta t \\cdot v_k + \\frac{1}{2} \\Delta t^2 \\cdot a_k\r\n",
    "   \\]\r\n",
    "   where \\( p_k \\) is the position at time \\( k \\), \\( v_k \\) is the velocity at time \\( k \\), \\( a_k \\) is the acceleration at time \\( k \\), and \\( \\Delta t \\) is the time step.\r\n",
    "\r\n",
    "3. **Kalman Filter Equations:**\r\n",
    "   Prediction step:\r\n",
    "   \\[\r\n",
    "   \\hat{x}_{k|k-1} = F_k \\hat{x}_{k-1|k-1} + B_k u_k\r\n",
    "   \\]\r\n",
    "   \\[\r\n",
    "   P_{k|k-1} = F_k P_{k-1|k-1} F_k^T + Q_k\r\n",
    "   \\]\r\n",
    "   Update step:\r\n",
    "   \\[\r\n",
    "   K_k = P_{k|k-1} H_k^T (H_k P_{k|k-1} H_k^T + R_k)^{-1}\r\n",
    "   \\]\r\n",
    "   \\[\r\n",
    "   \\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k(z_k - H_k \\hat{x}_{k|k-1})\r\n",
    "   \\]\r\n",
    "   \\[\r\n",
    "   P_{k|k} = (I - K_k H_k) P_{k|k-1}\r\n",
    "   \\]\r\n",
    "   where \\( \\hat{x}_{k|k} \\) is the updated state estimate, \\( P_{k|k} \\) is the updated error covariance, \\( F_k \\) is the state transition matrix, \\( B_k \\) is the control-input matrix, \\( u_k \\) is the control vector, \\( Q_k \\) is the process noise covariance, \\( H_k \\) is the observation matrix, \\( R_k \\) is the observation noise covariance, \\( K_k \\) is the Kalman gain, and \\( z_k \\) is the measurement vector.\r\n",
    "\r\n",
    "Understanding these equations and concepts lays the foundation for more advanced video processing techniques and applications in computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf112433-bad7-4591-a44c-3d12ef542efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def extract_frames(video_path, output_dir, frame_rate=1):\n",
    "    # Open the video file\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get the frame rate of the video\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Calculate frame interval based on desired frame rate\n",
    "    frame_interval = int(round(fps / frame_rate))\n",
    "    \n",
    "    # Initialize frame count\n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = video_capture.read()\n",
    "        \n",
    "        # If frame reading was successful\n",
    "        if ret:\n",
    "            # Increment frame count\n",
    "            frame_count += 1\n",
    "            \n",
    "            # Check if it's time to extract a frame based on frame interval\n",
    "            if frame_count % frame_interval == 0:\n",
    "                # Save the frame to the output directory\n",
    "                output_path = f\"{output_dir}/frame_{frame_count}.jpg\"\n",
    "                cv2.imwrite(output_path, frame)\n",
    "        else:\n",
    "            # Break the loop if no more frames are available\n",
    "            break\n",
    "    \n",
    "    # Release the video capture object\n",
    "    video_capture.release()\n",
    "\n",
    "# Example usage\n",
    "video_path = \"My.mp4\"\n",
    "output_dir = \"output_frames\"\n",
    "extract_frames(video_path, output_dir, frame_rate=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10af0b7d-ba92-46ba-b494-9176db8c0bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def interpolate_frames(video_path, output_path, new_frame_rate=60):\n",
    "    # Open the video file\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get the frame rate of the video\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Calculate the frame interval for interpolation\n",
    "    frame_interval = int(round(fps / new_frame_rate))\n",
    "    \n",
    "    # Initialize the optical flow object\n",
    "    optical_flow = cv2.optflow.createOptFlow_DualTVL1()\n",
    "    \n",
    "    # Initialize variables for frame interpolation\n",
    "    prev_frame = None\n",
    "    prev_gray = None\n",
    "    frame_count = 0\n",
    "    \n",
    "    # Initialize video writer object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, new_frame_rate, (int(video_capture.get(3)), int(video_capture.get(4))))\n",
    "    \n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = video_capture.read()\n",
    "        \n",
    "        if ret:\n",
    "            frame_count += 1\n",
    "            \n",
    "            # Convert frame to grayscale\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Calculate optical flow if not the first frame\n",
    "            if prev_frame is not None:\n",
    "                flow = optical_flow.calc(prev_gray, gray, None)\n",
    "                \n",
    "                # Interpolate frames based on optical flow\n",
    "                interpolated_frame = cv2.remap(prev_frame, -flow[..., 0], -flow[..., 1], cv2.INTER_LINEAR)\n",
    "                \n",
    "                # Write the interpolated frame to the output video\n",
    "                out.write(interpolated_frame)\n",
    "            \n",
    "            # Update previous frame and grayscale image\n",
    "            prev_frame = frame.copy()\n",
    "            prev_gray = gray.copy()\n",
    "            \n",
    "            # Skip frames based on frame interval\n",
    "            for _ in range(frame_interval - 1):\n",
    "                video_capture.grab()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Release video objects\n",
    "    video_capture.release()\n",
    "    out.release()\n",
    "\n",
    "# Example usage\n",
    "input_video_path = \"My.mp4\"\n",
    "output_video_path = \"output_video_interpolated.mp4\"\n",
    "interpolate_frames(input_video_path, output_video_path, new_frame_rate=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d04f4af1-9500-47a9-b55f-e1bcf45a29d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def align_frames(frame1, frame2):\n",
    "    # Convert frames to grayscale\n",
    "    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect features in both frames\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(gray1, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(gray2, None)\n",
    "\n",
    "    # Match features between frames\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    # Extract matched keypoints\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Compute homography\n",
    "    homography, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "    # Warp frame 1 onto frame 2 using the computed homography\n",
    "    aligned_frame = cv2.warpPerspective(frame1, homography, (frame2.shape[1], frame2.shape[0]))\n",
    "\n",
    "    return aligned_frame\n",
    "\n",
    "# Example usage\n",
    "frame1 = cv2.imread('My.jpg')\n",
    "frame2 = cv2.imread('My.jpg')\n",
    "\n",
    "aligned_frame = align_frames(frame1, frame2)\n",
    "\n",
    "# Display the aligned frame\n",
    "cv2.imshow('Aligned Frame', aligned_frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f967b875-f9c3-432b-aae5-28bf3ec6a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def compress_video(input_video, output_video, codec='XVID', fps=30, quality=95):\n",
    "    # Open input video\n",
    "    input_cap = cv2.VideoCapture(input_video)\n",
    "    if not input_cap.isOpened():\n",
    "        print(\"Error: Couldn't open input video.\")\n",
    "        return\n",
    "    \n",
    "    # Get input video properties\n",
    "    width = int(input_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(input_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Define codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "    output_writer = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "    # Read and compress frames\n",
    "    while True:\n",
    "        ret, frame = input_cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Compress frame\n",
    "        output_writer.write(frame)\n",
    "\n",
    "    # Release resources\n",
    "    input_cap.release()\n",
    "    output_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def decompress_video(input_video, output_video):\n",
    "    # Open input video\n",
    "    input_cap = cv2.VideoCapture(input_video)\n",
    "    if not input_cap.isOpened():\n",
    "        print(\"Error: Couldn't open input video.\")\n",
    "        return\n",
    "    \n",
    "    # Get input video properties\n",
    "    fps = input_cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(input_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(input_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create VideoWriter object\n",
    "    output_writer = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*'XVID'), fps, (width, height))\n",
    "\n",
    "    # Decompress and write frames\n",
    "    while True:\n",
    "        ret, frame = input_cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Write frame\n",
    "        output_writer.write(frame)\n",
    "\n",
    "    # Release resources\n",
    "    input_cap.release()\n",
    "    output_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "input_video = 'My.mp4'\n",
    "compressed_video = 'compressed_video.mp4'\n",
    "decompressed_video = 'decompressed_video.mp4'\n",
    "\n",
    "# Compress video\n",
    "compress_video(input_video, compressed_video)\n",
    "\n",
    "# Decompress video\n",
    "decompress_video(compressed_video, decompressed_video)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f540f-b8f2-4154-89ae-b96afb67ecc1",
   "metadata": {},
   "source": [
    "# Lists of all the topics in Video processing basics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e891c7f-7548-4a95-ac13-1dda1728ada9",
   "metadata": {},
   "source": [
    "Video processing basics in computer vision cover a wide range of topics. Here's a list of some fundamental topics within video processing:\r\n",
    "\r\n",
    "1. **Frame Capture**: \r\n",
    "   - Capturing individual frames from a video stream.\r\n",
    "\r\n",
    "2. **Frame Display**: \r\n",
    "   - Displaying individual frames or sequences of frames.\r\n",
    "\r\n",
    "3. **Frame Manipulation**: \r\n",
    "   - Basic operations on frames like resizing, cropping, rotating, and flipping.\r\n",
    "\r\n",
    "4. **Color Spaces**: \r\n",
    "   - Understanding and working with different color spaces like RGB, HSV, YUV, etc., for better color manipulation.\r\n",
    "\r\n",
    "5. **Frame Subtraction**: \r\n",
    "   - Background subtraction to detect moving objects.\r\n",
    "\r\n",
    "6. **Temporal Filtering**: \r\n",
    "   - Applying filters across frames to reduce noise or enhance features temporally.\r\n",
    "\r\n",
    "7. **Optical Flow**: \r\n",
    "   - Estimating the motion of objects between frames.\r\n",
    "\r\n",
    "8. **Object Tracking**: \r\n",
    "   - Tracking the movement of objects across frames.\r\n",
    "\r\n",
    "9. **Video Compression**: \r\n",
    "   - Techniques to reduce the size of video files for storage or transmission.\r\n",
    "\r\n",
    "10. **Video Decompression**: \r\n",
    "    - Techniques to decode compressed video data back into individual frames.\r\n",
    "\r\n",
    "11. **Motion Detection**: \r\n",
    "    - Detecting and analyzing motion in video sequences.\r\n",
    "\r\n",
    "12. **Video Stabilization**: \r\n",
    "    - Reducing shakiness or jitter in videos caused by camera motion.\r\n",
    "\r\n",
    "13. **Video Enhancement**: \r\n",
    "    - Enhancing video quality through techniques like denoising, deblurring, and contrast adjustment.\r\n",
    "\r\n",
    "14. **Video Segmentation**: \r\n",
    "    - Partitioning a video into segments based on object boundaries or motion.\r\n",
    "\r\n",
    "15. **Frame Interpolation**: \r\n",
    "    - Generating intermediate frames between existing frames to smoothen motion or increase frame rate.\r\n",
    "\r\n",
    "16. **Video Summarization**: \r\n",
    "    - Generating concise representations of videos by selecting key frames or segments.\r\n",
    "\r\n",
    "17. **Video Annotation**: \r\n",
    "    - Adding metadata or labels to video frames for analysis or visualization.\r\n",
    "\r\n",
    "These topics provide a foundation for understanding and working with video data in computer vision applications. Each topic may have various techniques, algorithms, and tools associated with it, forming the building blocks for more advanced video processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b9ea4-e2d4-4fb7-91a7-2d0b33cd0738",
   "metadata": {},
   "source": [
    "# Frame Capture:\r\n",
    "\r\n",
    "Capturing individual frames from a video stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b124ae87-3fa1-4be7-89c8-e0e5468e3be4",
   "metadata": {},
   "source": [
    "Frame capture refers to the process of extracting individual frames from a video stream. This process is commonly used in various applications such as video editing, computer vision, and video compression. \r\n",
    "\r\n",
    "Here's a breakdown of the general steps involved in frame capture:\r\n",
    "\r\n",
    "1. **Frame Extraction**: Each frame of a video is a still image that represents a specific moment in time. To capture frames, you need to extract these images from the video stream. This can be done using software libraries or frameworks that provide functions for video processing.\r\n",
    "\r\n",
    "2. **Frame Rate**: The frame rate of a video determines how many frames are displayed per second. Common frame rates include 24 frames per second (fps) for film, 30 fps for television, and 60 fps for many digital videos. The frame rate is typically denoted as FPS.\r\n",
    "\r\n",
    "3. **Frame Resolution**: The resolution of a frame refers to the dimensions of the image, typically measured in pixels. Common resolutions include 1920x1080 (Full HD) and 3840x2160 (4K UHD). The resolution determines the level of detail in each frame.\r\n",
    "\r\n",
    "4. **Sampling Rate**: When capturing frames, you may need to specify a sampling rate, which determines how often frames are extracted from the video stream. For example, you might capture every frame (1:1 sampling), or you might capture every nth frame to reduce the number of frames processed.\r\n",
    "\r\n",
    "Equations:\r\n",
    "\r\n",
    "1. **Total Frames**: The total number of frames in a video can be calculated by multiplying the frame rate (FPS) by the duration of the video (in seconds). \r\n",
    "   \\[ \\text{Total Frames} = \\text{Frame Rate} \\times \\text{Duration (seconds)} \\]\r\n",
    "\r\n",
    "2. **Frame Time**: The time corresponding to a particular frame can be calculated using the frame index (starting from 0) and the frame rate.\r\n",
    "   \\[ \\text{Frame Time (seconds)} = \\frac{\\text{Frame Index}}{\\text{Frame Rate}} \\]\r\n",
    "\r\n",
    "3. **Frame Index from Time**: If you have a specific time in seconds and want to find the corresponding frame index, you can use the following equation:\r\n",
    "   \\[ \\text{Frame Index} = \\text{Frame Rate} \\times \\text{Time (seconds)} \\]\r\n",
    "\r\n",
    "Frame capture is a fundamental operation in video processing and analysis, and understanding these concepts allows for efficient manipulation and analysis of video data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6efff5-938f-4567-a51b-b773c681472e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not open video file.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def capture_frames(video_path, output_path, sampling_rate=1):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    # Read and save frames\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Check if frame is read correctly\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Apply sampling rate\n",
    "        if frame_count % sampling_rate == 0:\n",
    "            # Save the frame\n",
    "            frame_output_path = output_path.format(frame_count)\n",
    "            cv2.imwrite(frame_output_path, frame)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    print(\"Frames captured successfully.\")\n",
    "\n",
    "# Example usage:\n",
    "video_path = 'My.mp4'\n",
    "output_path = 'frames/frame_{}.jpg'  # Output path format, {} will be replaced by frame index\n",
    "sampling_rate = 10  # Capture every 10th frame\n",
    "\n",
    "capture_frames(video_path, output_path, sampling_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d39f99-d5a2-47ad-89aa-26d5e48f7f5f",
   "metadata": {},
   "source": [
    "# Frame Display:\r\n",
    "\r\n",
    "Displaying individual frames or sequences of frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca10ed22-d193-4ba3-9edc-6284aa446f58",
   "metadata": {},
   "source": [
    "Displaying individual frames or sequences of frames involves presenting a series of images in a sequential order to convey motion, simulate movement, or illustrate a process. This concept is fundamental in various fields such as animation, film, video games, and scientific visualization. Here's an overview of the key aspects involved:\r\n",
    "\r\n",
    "1. **Frame**: A frame is a single image in a sequence of images. In animation and video, each frame represents a specific moment in time. Frames are typically displayed at a constant rate, measured in frames per second (fps), to create the illusion of continuous motion when viewed in succession.\r\n",
    "\r\n",
    "2. **Frame Rate (fps)**: The frame rate determines how many frames are displayed per second. Common frame rates include 24 fps (standard for film), 30 fps (common for television and online video), and 60 fps (common for video games and high-definition video). Higher frame rates result in smoother motion but require more computational resources.\r\n",
    "\r\n",
    "3. **Resolution**: The resolution of each frame refers to the number of pixels it contains, typically measured in width x height (e.g., 1920x1080 for Full HD). Higher resolutions result in sharper and more detailed images but require more storage space and computational power.\r\n",
    "\r\n",
    "4. **Compression**: To reduce file size and transmission bandwidth, frames are often compressed using various algorithms such as JPEG, MPEG, or H.264. Compression techniques exploit redundancies in the image data to represent it more efficiently.\r\n",
    "\r\n",
    "5. **Display Devices**: Frames are ultimately displayed on various devices such as monitors, projectors, or screens. Different devices have different display capabilities, including resolution, color depth, refresh rate, and aspect ratio, which may affect how frames are rendered.\r\n",
    "\r\n",
    "6. **Interpolation**: In computer graphics and animation, interpolation techniques are often used to generate intermediate frames between keyframes. This process, known as tweening, helps create smooth motion transitions by automatically generating frames based on the positions and attributes of keyframes.\r\n",
    "\r\n",
    "7. **Equations**: Several mathematical equations and algorithms are involved in frame display and animation, including:\r\n",
    "\r\n",
    "   - **Motion Blur**: Simulates the blurring effect that occurs when objects move quickly across the screen. Motion blur equations typically involve the velocity of the moving object and the exposure time of each frame.\r\n",
    "   \r\n",
    "   - **Bezier Curves**: Used in animation to define smooth paths for objects to follow between keyframes. Bezier curves are described by polynomial equations that control the shape of the curve.\r\n",
    "   \r\n",
    "   - **Physics Simulations**: In simulations of physical phenomena such as fluid dynamics or rigid body dynamics, equations of motion (e.g., Newton's laws) are used to calculate the position and velocity of objects over time, which are then rendered as frames.\r\n",
    "\r\n",
    "Frame display involves a combination of artistic creativity, technical knowledge, and computational algorithms to create compelling visual experiences across various media platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed2d6758-8bb5-49e9-98b6-20918201f535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Couldn't open video file\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def display_frames(video_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Couldn't open video file\")\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # If there are no more frames to read, break the loop\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Display the frame\n",
    "        cv2.imshow('Frame', frame)\n",
    "        \n",
    "        # Wait for a key press and exit if 'q' is pressed\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release the video capture object and close all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Path to the video file\n",
    "video_path = 'My.mp4'\n",
    "\n",
    "# Call the function to display frames\n",
    "display_frames(video_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd828b4-10db-46e1-b744-36f85acee356",
   "metadata": {},
   "source": [
    "# Frame Manipulation:\r\n",
    "\r\n",
    "Basic operations on frames like resizing, cropping, rotating, and flipping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e6d043-b90a-435f-8a8d-2f7a8e94ff15",
   "metadata": {},
   "source": [
    "Frame manipulation involves basic operations on frames or images, such as resizing, cropping, rotating, and flipping. These operations are fundamental in image processing and computer vision tasks, allowing us to preprocess images before further analysis or presentation. Here's a breakdown of each operation along with relevant information and equations:\n",
    "\n",
    "### 1. Resizing:\n",
    "\n",
    "Resizing an image involves changing its dimensions while preserving its aspect ratio or stretching it to fit new dimensions. This operation is useful for adjusting the size of images for display, processing, or storage.\n",
    "\n",
    "**Equations:**\n",
    "- **Nearest Neighbor Interpolation:** Simplest method, where each pixel value in the output image is determined by the nearest pixel in the input image.\n",
    "- **Bilinear Interpolation:** More sophisticated method, where the output pixel value is a weighted average of the four nearest pixels in the input image.\n",
    "\n",
    "### 2. Cropping:\n",
    "\n",
    "Cropping involves selecting a region of interest (ROI) from an image and discarding the rest. This operation is commonly used to remove unwanted parts of an image or focus on specific features.\n",
    "\n",
    "**Equations:** No specific equations for cropping, but the process involves selecting a rectangular region defined by its coordinates (top-left and bottom-right corners) and extracting pixels within that region.\n",
    "\n",
    "### 3. Rotating:\n",
    "\n",
    "Rotating an image involves changing its orientation by a specified angle. This operation is useful for correcting image alignment or extracting features from different perspectives.\n",
    "\n",
    "**Equations:**\n",
    "- **Rotation Matrix:** To rotate an image by an angle θ around its center, we use a rotation matrix:\n",
    "\n",
    "\\[ \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix} \\]\n",
    "\n",
    "### 4. Flipping:\n",
    "\n",
    "Flipping an image involves reversing its pixel values horizontally, vertically, or both. This operation is commonly used for data augmentation or mirroring images.\n",
    "\n",
    "**Equations:** No specific equations for flipping, but the process involves reversing pixel values along one or both axes.\n",
    "\n",
    "### Implementation in Python (using OpenCV):\n",
    "\n",
    "Here's an example Python code snippet demonstrating these operations using OpenCV:\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "\n",
    "# Read an image\n",
    "image = cv2.imread('image.jpg')\n",
    "\n",
    "# Resize image\n",
    "resized_image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "# Crop image\n",
    "cropped_image = image[y1:y2, x1:x2]\n",
    "\n",
    "# Rotate image\n",
    "(h, w) = image.shape[:2]\n",
    "center = (w // 2, h // 2)\n",
    "M = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "rotated_image = cv2.warpAffine(image, M, (w, h))\n",
    "\n",
    "# Flip image\n",
    "flipped_image = cv2.flip(image, flip_code)\n",
    "```\n",
    "\n",
    "Make sure to replace `'image.jpg'` with the path to your input image. Adjust parameters like `new_width`, `new_height`, `x1`, `y1`, `x2`, `y2`, `angle`, `scale`, and `flip_code` according to your requirements. This code demonstrates the basic operations of resizing, cropping, rotating, and flipping frames/images using OpenCV in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3acd42c-8bb8-4404-a36c-5680cc136820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Read an image\n",
    "image = cv2.imread('My.jpg')\n",
    "\n",
    "new_width = 5\n",
    "new_height = 7\n",
    "y1 = 2\n",
    "y2 = 3\n",
    "x1 = 2\n",
    "x2 = 3\n",
    "angle = 20\n",
    "scale = 4\n",
    "flip_code = 2\n",
    "\n",
    "# Resize image\n",
    "resized_image = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "# Crop image\n",
    "cropped_image = image[y1:y2, x1:x2]\n",
    "\n",
    "# Rotate image\n",
    "(h, w) = image.shape[:2]\n",
    "center = (w // 2, h // 2)\n",
    "M = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "rotated_image = cv2.warpAffine(image, M, (w, h))\n",
    "\n",
    "# Flip image\n",
    "flipped_image = cv2.flip(image, flip_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38dee677-097d-4ac8-9ad7-e17d421223ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def resize_frame(frame, width=None, height=None):\n",
    "    if width is None and height is None:\n",
    "        return frame\n",
    "    if width is None:\n",
    "        r = height / frame.shape[0]\n",
    "        dim = (int(frame.shape[1] * r), height)\n",
    "    else:\n",
    "        r = width / frame.shape[1]\n",
    "        dim = (width, int(frame.shape[0] * r))\n",
    "    resized = cv2.resize(frame, dim, interpolation=cv2.INTER_AREA)\n",
    "    return resized\n",
    "\n",
    "def crop_frame(frame, x, y, w, h):\n",
    "    return frame[y:y+h, x:x+w]\n",
    "\n",
    "def rotate_frame(frame, angle):\n",
    "    (h, w) = frame.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(frame, M, (w, h))\n",
    "    return rotated\n",
    "\n",
    "def flip_frame(frame, direction):\n",
    "    if direction == 'horizontal':\n",
    "        return cv2.flip(frame, 1)\n",
    "    elif direction == 'vertical':\n",
    "        return cv2.flip(frame, 0)\n",
    "    elif direction == 'both':\n",
    "        return cv2.flip(frame, -1)\n",
    "    else:\n",
    "        return frame\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Read a sample image\n",
    "    image_path = 'My.jpg'\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Resize\n",
    "    resized_image = resize_frame(image, width=400)\n",
    "\n",
    "    # Crop\n",
    "    cropped_image = crop_frame(resized_image, x=100, y=50, w=200, h=300)\n",
    "\n",
    "    # Rotate\n",
    "    rotated_image = rotate_frame(cropped_image, angle=45)\n",
    "\n",
    "    # Flip\n",
    "    flipped_image = flip_frame(rotated_image, direction='horizontal')\n",
    "\n",
    "    # Display the original and manipulated frames\n",
    "    cv2.imshow('Original Image', image)\n",
    "    cv2.imshow('Resized Image', resized_image)\n",
    "    cv2.imshow('Cropped Image', cropped_image)\n",
    "    cv2.imshow('Rotated Image', rotated_image)\n",
    "    cv2.imshow('Flipped Image', flipped_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0e5bb8-37e8-41e1-b99f-4cba1277081a",
   "metadata": {},
   "source": [
    "# Color Spaces:\r\n",
    "\r\n",
    "Understanding and working with different color spaces like RGB, HSV, YUV, etc., for better color manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1763331a-2c58-4e4d-a1a3-c36d4f262c5f",
   "metadata": {},
   "source": [
    "Color spaces are essential in video processing for representing and manipulating colors. Different color spaces have unique advantages and are suited for various tasks such as image analysis, segmentation, and manipulation. Here's an overview of some commonly used color spaces in video processing:\r\n",
    "\r\n",
    "### 1. RGB (Red, Green, Blue):\r\n",
    "RGB is the most common color space, where each pixel is represented by three color channels: red, green, and blue. It is widely used in digital imaging systems, displays, and cameras. RGB is additive, meaning colors are formed by combining different intensities of red, green, and blue light.\r\n",
    "\r\n",
    "### 2. HSV (Hue, Saturation, Value):\r\n",
    "HSV color space represents colors based on three components: hue, saturation, and value. Hue represents the color type (e.g., red, green, blue), saturation controls the intensity or purity of the color, and value represents the brightness or intensity of the color. HSV is often used for color segmentation and detection tasks due to its intuitive representation of color.\r\n",
    "\r\n",
    "### 3. YUV (Luma, Chrominance):\r\n",
    "YUV color space separates the luminance (brightness) information (Y) from the chrominance (color) information (U and V). The Y component represents the grayscale image, while U and V represent the chrominance components that encode color information. YUV is commonly used in video compression and transmission systems, such as MPEG and JPEG, as it provides better compression efficiency by separating brightness from color.\r\n",
    "\r\n",
    "### 4. LAB (Lightness, A, B):\r\n",
    "LAB color space consists of three components: lightness (L), and two color-opponent channels (A and B). The A channel represents the green to red color spectrum, while the B channel represents the blue to yellow color spectrum. LAB is perceptually uniform, making it suitable for color correction and image editing tasks.\r\n",
    "\r\n",
    "### 5. CMYK (Cyan, Magenta, Yellow, Black):\r\n",
    "CMYK color space is primarily used in color printing and represents colors using four components: cyan, magenta, yellow, and black (key). CMYK is subtractive, meaning colors are formed by subtracting different color components from white light. It is commonly used in printing processes to reproduce a wide range of colors.\r\n",
    "\r\n",
    "### 6. XYZ (CIE 1931 Color Space):\r\n",
    "XYZ color space is based on the human visual system's response to light and is defined by the International Commission on Illumination (CIE). It provides a device-independent representation of color and is used as a reference color space for color science and color matching applications.\r\n",
    "\r\n",
    "### Conclusion:\r\n",
    "Understanding and working with different color spaces is essential in video processing for various color manipulation tasks. Each color space has its advantages and applications, and choosing the right color space depends on the specific requirements of the task at hand. By leveraging the properties of different color spaces, video processing applications can achieve better color manipulation and analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c2110ee-12e1-43c3-be17-93f3f3a31200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read the original image\n",
    "original_image = cv2.imread('My.jpg')\n",
    "\n",
    "# Convert the image to different color spaces\n",
    "hsv_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2HSV)\n",
    "yuv_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2YUV)\n",
    "\n",
    "# Display the original image and converted images\n",
    "cv2.imshow('Original Image', original_image)\n",
    "cv2.imshow('HSV Image', hsv_image)\n",
    "cv2.imshow('YUV Image', yuv_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c060316-6414-43bd-a9f7-e170c1f077be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open video file\n",
    "video_capture = cv2.VideoCapture('My.mp4')\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break  # Break the loop if there are no more frames\n",
    "    \n",
    "    # Convert the frame to different color spaces\n",
    "    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    yuv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)\n",
    "    \n",
    "    # Display the frames\n",
    "    cv2.imshow('RGB', frame)\n",
    "    cv2.imshow('HSV', hsv_frame)\n",
    "    cv2.imshow('YUV', yuv_frame)\n",
    "    \n",
    "    # Check for key press to exit\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6379262e-2357-440e-852a-608096dbf55b",
   "metadata": {},
   "source": [
    "# Frame Subtraction:\r\n",
    "\r\n",
    "Background subtraction to detect moving objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e1d15-ba9f-47e7-a522-4eb951d1fed6",
   "metadata": {},
   "source": [
    "### Frame Subtraction: Background Subtraction for Detecting Moving Objects\r\n",
    "\r\n",
    "Frame subtraction, also known as background subtraction, is a fundamental technique in computer vision and video processing for detecting moving objects in video sequences. It involves comparing each frame of a video to a background model to identify regions that differ significantly, indicating movement. Here's a detailed explanation of the process, including the necessary equations and methods.\r\n",
    "\r\n",
    "#### Basic Concept\r\n",
    "\r\n",
    "The basic idea of frame subtraction is to create a background model \\( B \\) and then subtract it from each new frame \\( F_t \\) at time \\( t \\) to get the foreground mask \\( M_t \\), which highlights the moving objects.\r\n",
    "\r\n",
    "\\[ M_t(x, y) = |F_t(x, y) - B(x, y)| \\]\r\n",
    "\r\n",
    "where:\r\n",
    "- \\( F_t(x, y) \\) is the pixel value at position \\((x, y)\\) in the current frame at time \\( t \\).\r\n",
    "- \\( B(x, y) \\) is the pixel value at the same position in the background model.\r\n",
    "- \\( M_t(x, y) \\) is the resulting mask pixel value, representing the detected motion.\r\n",
    "\r\n",
    "#### Thresholding\r\n",
    "\r\n",
    "To determine whether a pixel has changed significantly, a threshold \\( T \\) is applied to the difference:\r\n",
    "\r\n",
    "\\[ M_t(x, y) = \\begin{cases} \r\n",
    "1 & \\text{if } |F_t(x, y) - B(x, y)| > T \\\\\r\n",
    "0 & \\text{otherwise}\r\n",
    "\\end{cases} \\]\r\n",
    "\r\n",
    "This thresholding step converts the difference image into a binary mask where moving objects are highlighted.\r\n",
    "\r\n",
    "#### Background Model Update\r\n",
    "\r\n",
    "The background model \\( B \\) needs to be updated to adapt to changes in the scene over time. One common method is the running average:\r\n",
    "\r\n",
    "\\[ B_t(x, y) = \\alpha F_t(x, y) + (1 - \\alpha) B_{t-1}(x, y) \\]\r\n",
    "\r\n",
    "where:\r\n",
    "- \\( \\alpha \\) is the learning rate, a small positive constant (e.g., 0.01).\r\n",
    "\r\n",
    "#### Advanced Techniques\r\n",
    "\r\n",
    "##### 1. Mixture of Gaussians (MOG)\r\n",
    "\r\n",
    "A more advanced background subtraction technique uses a mixture of Gaussians to model the background. Each pixel is modeled as a mixture of \\( K \\) Gaussian distributions.\r\n",
    "\r\n",
    "\\[ P(F_t(x, y)) = \\sum_{i=1}^{K} w_{i,t} \\eta(F_t(x, y); \\mu_{i,t}, \\Sigma_{i,t}) \\]\r\n",
    "\r\n",
    "where:\r\n",
    "- \\( w_{i,t} \\) is the weight of the \\( i \\)-th Gaussian at time \\( t \\).\r\n",
    "- \\( \\eta \\) is the Gaussian probability density function.\r\n",
    "- \\( \\mu_{i,t} \\) and \\( \\Sigma_{i,t} \\) are the mean and covariance of the \\( i \\)-th Gaussian, respectively.\r\n",
    "\r\n",
    "The parameters of the Gaussians are updated iteratively, and a pixel is classified as foreground if it does not match any of the \\( K \\) distributions.\r\n",
    "\r\n",
    "##### 2. Kernel Density Estimation (KDE)\r\n",
    "\r\n",
    "Kernel Density Estimation is a non-parametric way to estimate the probability density function of the background. For each pixel, the density is estimated based on a window of previous frames.\r\n",
    "\r\n",
    "##### 3. Codebook Model\r\n",
    "\r\n",
    "In the codebook approach, a pixel's history is represented by a set of codewords (a codebook). Each codeword represents a different appearance of the pixel, and a pixel is classified as foreground if its value does not match any codeword.\r\n",
    "\r\n",
    "### Implementation Steps\r\n",
    "\r\n",
    "1. **Initialization**: Initialize the background model \\( B \\). This can be done by averaging the first few frames.\r\n",
    "\r\n",
    "2. **Foreground Detection**: For each new frame \\( F_t \\):\r\n",
    "   - Compute the difference \\( D_t = |F_t - B| \\).\r\n",
    "   - Apply thresholding to get the binary mask \\( M_t \\).\r\n",
    "   - Optionally, apply morphological operations (e.g., dilation, erosion) to clean up the mask.\r\n",
    "\r\n",
    "3. **Background Model Update**: Update the background model using a chosen method (e.g., running average, MOG).\r\n",
    "\r\n",
    "### Applications\r\n",
    "\r\n",
    "- **Surveillance**: Detecting intruders in a monitored area.\r\n",
    "- **Traffic Monitoring**: Identifying and tracking vehicles.\r\n",
    "- **Human-Computer Interaction**: Gesture recognition and motion capture.\r\n",
    "- **Environmental Monitoring**: Observing changes in natural environments.\r\n",
    "\r\n",
    "### Summary\r\n",
    "\r\n",
    "Frame subtraction is a powerful tool for detecting moving objects in video sequences. By subtracting a background model from each frame and applying thresholding, moving objects can be highlighted effectively. Advanced techniques like Mixture of Gaussians and Kernel Density Estimation provide more robust background modeling for complex scenes. Implementing these methods involves initializing the background model, detecting the foreground in each frame, and updating the background model over time.\r\n",
    "\r\n",
    "Understanding and choosing the appropriate background subtraction method depends on the specific application and the characteristics of the video data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c638c302-51c1-43b3-be07-c1a9ced03deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    # Open a video file or capture from webcam\n",
    "    cap = cv2.VideoCapture('output.mp4')  # Use 0 for webcam\n",
    "\n",
    "    # Check if video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    # Read the first frame to initialize the background model\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read the first frame.\")\n",
    "        return\n",
    "\n",
    "    # Convert the first frame to grayscale\n",
    "    background = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Use GaussianBlur to remove noise and improve background subtraction\n",
    "    background = cv2.GaussianBlur(background, (21, 21), 0)\n",
    "\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to grayscale\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Blur the frame to reduce noise\n",
    "        gray_frame = cv2.GaussianBlur(gray_frame, (21, 21), 0)\n",
    "\n",
    "        # Compute the absolute difference between the current frame and the background model\n",
    "        diff_frame = cv2.absdiff(background, gray_frame)\n",
    "\n",
    "        # Apply a binary threshold to the difference image\n",
    "        _, thresh_frame = cv2.threshold(diff_frame, 25, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Dilate the thresholded image to fill in holes, making the object detection more robust\n",
    "        thresh_frame = cv2.dilate(thresh_frame, None, iterations=2)\n",
    "\n",
    "        # Find contours of the detected objects\n",
    "        contours, _ = cv2.findContours(thresh_frame.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Draw bounding boxes around the detected objects\n",
    "        for contour in contours:\n",
    "            if cv2.contourArea(contour) < 500:\n",
    "                continue  # Ignore small contours to reduce noise\n",
    "            (x, y, w, h) = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Frame', frame)\n",
    "        cv2.imshow('Thresh', thresh_frame)\n",
    "        cv2.imshow('Difference', diff_frame)\n",
    "\n",
    "        # Update the background model using a running average\n",
    "        background = cv2.addWeighted(background, 0.5, gray_frame, 0.5, 0)\n",
    "\n",
    "        # Break the loop when 'q' key is pressed\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close any OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74956619-45e3-490b-9d9a-e61924cb15a8",
   "metadata": {},
   "source": [
    "# Temporal Filtering:\r\n",
    "\r\n",
    "Applying filters across frames to reduce noise or enhance features temporally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51c419f-ace6-4a58-84ad-85b8e9c314cc",
   "metadata": {},
   "source": [
    "Temporal filtering in video processing involves applying filters across multiple frames to reduce noise, enhance features, or achieve other effects over time. This approach leverages the temporal redundancy in video sequences to improve the quality and robustness of the resulting output.\r\n",
    "\r\n",
    "## Key Concepts and Techniques\r\n",
    "\r\n",
    "### 1. **Temporal Averaging**\r\n",
    "Temporal averaging smooths the video by averaging pixel values over several frames. This helps to reduce noise but may introduce motion blur.\r\n",
    "\r\n",
    "**Equation:**\r\n",
    "\\[ I_t(x, y) = \\frac{1}{2k+1} \\sum_{i=-k}^{k} I_{t+i}(x, y) \\]\r\n",
    "where \\( I_t(x, y) \\) is the pixel value at position \\((x, y)\\) in frame \\(t\\), and \\(2k+1\\) is the number of frames considered for averaging.\r\n",
    "\r\n",
    "### 2. **Exponential Moving Average (EMA)**\r\n",
    "EMA gives more weight to recent frames, making it more responsive to changes while still reducing noise.\r\n",
    "\r\n",
    "**Equation:**\r\n",
    "\\[ I'_t(x, y) = \\alpha I_t(x, y) + (1 - \\alpha) I'_{t-1}(x, y) \\]\r\n",
    "where \\( I'_t(x, y) \\) is the filtered pixel value, and \\(\\alpha\\) is the smoothing factor (0 < \\(\\alpha\\) < 1).\r\n",
    "\r\n",
    "### 3. **Kalman Filtering**\r\n",
    "Kalman filter is an optimal recursive filter that estimates the state of a dynamic system from noisy measurements.\r\n",
    "\r\n",
    "**State Equation:**\r\n",
    "\\[ \\mathbf{x}_t = \\mathbf{F} \\mathbf{x}_{t-1} + \\mathbf{B} \\mathbf{u}_t + \\mathbf{w}_t \\]\r\n",
    "**Measurement Equation:**\r\n",
    "\\[ \\mathbf{z}_t = \\mathbf{H} \\mathbf{x}_t + \\mathbf{v}_t \\]\r\n",
    "where:\r\n",
    "- \\(\\mathbf{x}_t\\) is the state vector at time \\(t\\).\r\n",
    "- \\(\\mathbf{F}\\) is the state transition model.\r\n",
    "- \\(\\mathbf{u}_t\\) is the control input vector.\r\n",
    "- \\(\\mathbf{B}\\) is the control-input model.\r\n",
    "- \\(\\mathbf{w}_t\\) is the process noise.\r\n",
    "- \\(\\mathbf{z}_t\\) is the measurement vector.\r\n",
    "- \\(\\mathbf{H}\\) is the observation model.\r\n",
    "- \\(\\mathbf{v}_t\\) is the measurement noise.\r\n",
    "\r\n",
    "### 4. **Gaussian Temporal Filtering**\r\n",
    "This applies a Gaussian filter over the temporal dimension, effectively performing weighted averaging with a Gaussian kernel.\r\n",
    "\r\n",
    "**Equation:**\r\n",
    "\\[ I_t(x, y) = \\sum_{i=-k}^{k} I_{t+i}(x, y) \\cdot g(i) \\]\r\n",
    "where \\( g(i) \\) is the Gaussian weight for frame \\( t+i \\), calculated as:\r\n",
    "\\[ g(i) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{i^2}{2 \\sigma^2}} \\]\r\n",
    "\r\n",
    "### 5. **Bilateral Filtering**\r\n",
    "Bilateral filtering preserves edges while smoothing the image. It can be extended to the temporal domain by considering both spatial and temporal dimensions.\r\n",
    "\r\n",
    "**Equation:**\r\n",
    "\\[ I_t(x, y) = \\frac{1}{W} \\sum_{i=-k}^{k} \\sum_{(x', y') \\in \\Omega} I_{t+i}(x', y') \\cdot f_d(x', y', x, y) \\cdot f_r(I_{t+i}(x', y'), I_t(x, y)) \\]\r\n",
    "where:\r\n",
    "- \\( \\Omega \\) is the spatial neighborhood.\r\n",
    "- \\( f_d \\) is the spatial distance weight.\r\n",
    "- \\( f_r \\) is the range distance weight.\r\n",
    "- \\( W \\) is the normalization factor.\r\n",
    "\r\n",
    "### 6. **Optical Flow-Based Filtering**\r\n",
    "This method uses optical flow to align frames before applying filters, which helps to handle motion better.\r\n",
    "\r\n",
    "**Optical Flow Calculation:**\r\n",
    "\\[ I_t(x, y) = I_{t-1}(x + u(x, y), y + v(x, y)) \\]\r\n",
    "where \\( u(x, y) \\) and \\( v(x, y) \\) are the horizontal and vertical components of the optical flow vector at \\((x, y)\\).\r\n",
    "\r\n",
    "## Applications\r\n",
    "1. **Noise Reduction**: Temporal filters are widely used to reduce sensor noise in videos.\r\n",
    "2. **Motion Deblurring**: Temporal filtering helps in reducing motion blur by leveraging multiple frames.\r\n",
    "3. **Object Tracking**: Kalman filters and optical flow techniques are integral to tracking objects over time.\r\n",
    "4. **Video Stabilization**: Temporal filtering can help in stabilizing shaky video footage.\r\n",
    "5. **Feature Enhancement**: Enhancing specific features over time, such as improving the visibility of moving objects.\r\n",
    "\r\n",
    "## Implementation in Python (using OpenCV and NumPy)\r\n",
    "\r\n",
    "### Temporal Averaging\r\n",
    "```python\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "def temporal_averaging(frames, k):\r\n",
    "    num_frames = len(frames)\r\n",
    "    avg_frames = []\r\n",
    "    for i in range(k, num_frames - k):\r\n",
    "        avg_frame = np.mean(frames[i-k:i+k+1], axis=0)\r\n",
    "        avg_frames.append(avg_frame.astype(np.uint8))\r\n",
    "    return avg_frames\r\n",
    "\r\n",
    "# Example usage\r\n",
    "cap = cv2.VideoCapture('video.mp4')\r\n",
    "frames = []\r\n",
    "\r\n",
    "while cap.isOpened():\r\n",
    "    ret, frame = cap.read()\r\n",
    "    if not ret:\r\n",
    "        break\r\n",
    "    frames.append(frame)\r\n",
    "\r\n",
    "cap.release()\r\n",
    "k = 2\r\n",
    "avg_frames = temporal_averaging(frames, k)\r\n",
    "\r\n",
    "for i, frame in enumerate(avg_frames):\r\n",
    "    cv2.imshow('Averaged Frame', frame)\r\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\r\n",
    "        break\r\n",
    "\r\n",
    "cv2.destroyAllWindows()\r\n",
    "```\r\n",
    "\r\n",
    "### Exponential Moving Average\r\n",
    "```python\r\n",
    "def exponential_moving_average(frames, alpha):\r\n",
    "    ema_frames = []\r\n",
    "    ema_frame = frames[0].astype(np.float32)\r\n",
    "    \r\n",
    "    for frame in frames:\r\n",
    "        ema_frame = alpha * frame + (1 - alpha) * ema_frame\r\n",
    "        ema_frames.append(ema_frame.astype(np.uint8))\r\n",
    "        \r\n",
    "    return ema_frames\r\n",
    "\r\n",
    "# Example usage\r\n",
    "alpha = 0.5\r\n",
    "ema_frames = exponential_moving_average(frames, alpha)\r\n",
    "\r\n",
    "for i, frame in enumerate(ema_frames):\r\n",
    "    cv2.imshow('EMA Frame', frame)\r\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\r\n",
    "        break\r\n",
    "\r\n",
    "cv2.destroyAllWindows()\r\n",
    "```\r\n",
    "\r\n",
    "### Kalman Filtering (OpenCV Kalman Filter example for object tracking)\r\n",
    "```python\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "cap = cv2.VideoCapture('video.mp4')\r\n",
    "\r\n",
    "# Create Kalman filter\r\n",
    "kalman = cv2.KalmanFilter(4, 2)\r\n",
    "kalman.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)\r\n",
    "kalman.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)\r\n",
    "kalman.processNoiseCov = np.eye(4, dtype=np.float32) * 0.03\r\n",
    "\r\n",
    "while cap.isOpened():\r\n",
    "    ret, frame = cap.read()\r\n",
    "    if not ret:\r\n",
    "        break\r\n",
    "    \r\n",
    "    # Simulated measurement (replace with actual measurement in real scenarios)\r\n",
    "    measurement = np.array([[np.float32(x)], [np.float32(y)]])\r\n",
    "    \r\n",
    "    # Correct the state with the measurement\r\n",
    "    kalman.correct(measurement)\r\n",
    "    \r\n",
    "    # Predict the next state\r\n",
    "    predicted = kalman.predict()\r\n",
    "    \r\n",
    "    # Extract predicted position\r\n",
    "    x, y = int(predicted[0]), int(predicted[1])\r\n",
    "    \r\n",
    "    # Draw the prediction\r\n",
    "    cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)\r\n",
    "    \r\n",
    "    cv2.imshow('Kalman Filter', frame)\r\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\r\n",
    "        break\r\n",
    "\r\n",
    "cap.release()\r\n",
    "cv2.destroyAllWindows()\r\n",
    "```\r\n",
    "\r\n",
    "### Optical Flow-Based Filtering\r\n",
    "```python\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "cap = cv2.VideoCapture('video.mp4')\r\n",
    "ret, prev_frame = cap.read()\r\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\r\n",
    "frames = []\r\n",
    "\r\n",
    "while cap.isOpened():\r\n",
    "    ret, frame = cap.read()\r\n",
    "    if not ret:\r\n",
    "        break\r\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\r\n",
    "    prev_gray = gray\r\n",
    "\r\n",
    "    # Warp the current frame according to the optical flow\r\n",
    "    h, w = frame.shape[:2]\r\n",
    "    flow_map = np.column_stack((np.repeat(np.arange(h), w), np.tile(np.arange(w), h))) + flow.reshape(-1, 2)\r\n",
    "    flow_map = np.round(flow_map).astype(int)\r\n",
    "    flow_map[:, 0] = np.clip(flow_map[:, 0], 0, h - 1)\r\n",
    "    flow_map[:, 1] = np.clip(flow_map[:, 1], 0, w - 1)\r\n",
    "    warped_frame = frame[flow_map[:, 0], flow_map[:, 1]].reshape(h, w, 3)\r\n",
    "    \r\n",
    "    frames.append(warped_frame)\r\n",
    "\r\n",
    "cap.release()\r\n",
    "\r\n",
    "for i\r\n",
    "\r\n",
    ", frame in enumerate(frames):\r\n",
    "    cv2.imshow('Optical Flow Filtered Frame', frame)\r\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\r\n",
    "        break\r\n",
    "\r\n",
    "cv2.destroyAllWindows()\r\n",
    "```\r\n",
    "\r\n",
    "These implementations cover the basics of temporal filtering in video processing. Depending on the specific application and the type of noise or artifacts present in the video, different filtering techniques and parameters may be more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec6ad300-04e9-42bf-843b-79f8ffadfbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def temporal_averaging(frames, k):\n",
    "    num_frames = len(frames)\n",
    "    avg_frames = []\n",
    "    for i in range(k, num_frames - k):\n",
    "        avg_frame = np.mean(frames[i-k:i+k+1], axis=0)\n",
    "        avg_frames.append(avg_frame.astype(np.uint8))\n",
    "    return avg_frames\n",
    "\n",
    "# Example usage\n",
    "cap = cv2.VideoCapture('video.mp4')\n",
    "frames = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frames.append(frame)\n",
    "\n",
    "cap.release()\n",
    "k = 2\n",
    "avg_frames = temporal_averaging(frames, k)\n",
    "\n",
    "for i, frame in enumerate(avg_frames):\n",
    "    cv2.imshow('Averaged Frame', frame)\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a673b8c5-8ec8-4288-bec8-509f9d350cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture('video.mp4')\n",
    "ret, prev_frame = cap.read()\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "frames = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    prev_gray = gray\n",
    "\n",
    "    # Warp the current frame according to the optical flow\n",
    "    h, w = frame.shape[:2]\n",
    "    flow_map = np.column_stack((np.repeat(np.arange(h), w), np.tile(np.arange(w), h))) + flow.reshape(-1, 2)\n",
    "    flow_map = np.round(flow_map).astype(int)\n",
    "    flow_map[:, 0] = np.clip(flow_map[:, 0], 0, h - 1)\n",
    "    flow_map[:, 1] = np.clip(flow_map[:, 1], 0, w - 1)\n",
    "    warped_frame = frame[flow_map[:, 0], flow_map[:, 1]].reshape(h, w, 3)\n",
    "    \n",
    "    frames.append(warped_frame)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "for i, frame in enumerate(frames):\n",
    "    cv2.imshow('Optical Flow Filtered Frame', frame)\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0754e05-8312-4fff-9647-920387a50150",
   "metadata": {},
   "source": [
    "# Optical Flow:\r\n",
    "\r\n",
    "Estimating the motion of objects between frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc7ba1-3f1a-4589-b095-a2ce2a784a8d",
   "metadata": {},
   "source": [
    "Optical flow is a crucial concept in computer vision and image processing, referring to the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and the scene. Estimating optical flow involves determining the motion of each pixel in an image sequence, which has various applications including video compression, object tracking, and autonomous navigation.\r\n",
    "\r\n",
    "### Key Concepts and Equations in Optical Flow\r\n",
    "\r\n",
    "1. **Optical Flow Constraint Equation**:\r\n",
    "   The basic principle of optical flow is the brightness constancy assumption, which states that the intensity of a point in the image remains constant over time despite its movement. Mathematically, for a pixel at location \\((x, y)\\) in an image \\(I\\) at time \\(t\\), the intensity \\(I(x, y, t)\\) is assumed to be constant as it moves to a new position \\((x + \\Delta x, y + \\Delta y)\\) at time \\(t + \\Delta t\\):\r\n",
    "\r\n",
    "   \\[\r\n",
    "   I(x, y, t) = I(x + \\Delta x, y + \\Delta y, t + \\Delta t)\r\n",
    "   \\]\r\n",
    "\r\n",
    "   Taking the first-order Taylor expansion of \\(I(x + \\Delta x, y + \\Delta y, t + \\Delta t)\\) and assuming \\(\\Delta x\\), \\(\\Delta y\\), and \\(\\Delta t\\) are small, we get:\r\n",
    "\r\n",
    "   \\[\r\n",
    "   I(x + \\Delta x, y + \\Delta y, t + \\Delta t) \\approx I(x, y, t) + \\frac{\\partial I}{\\partial x} \\Delta x + \\frac{\\partial I}{\\partial y} \\Delta y + \\frac{\\partial I}{\\partial t} \\Delta t\r\n",
    "   \\]\r\n",
    "\r\n",
    "   Since \\(I(x, y, t) = I(x + \\Delta x, y + \\Delta y, t + \\Delta t)\\), we obtain:\r\n",
    "\r\n",
    "   \\[\r\n",
    "   \\frac{\\partial I}{\\partial x} \\Delta x + \\frac{\\partial I}{\\partial y} \\Delta y + \\frac{\\partial I}{\\partial t} \\Delta t = 0\r\n",
    "   \\]\r\n",
    "\r\n",
    "   Dividing by \\(\\Delta t\\), we get the optical flow constraint equation:\r\n",
    "\r\n",
    "   \\[\r\n",
    "   I_x u + I_y v + I_t = 0\r\n",
    "   \\]\r\n",
    "\r\n",
    "   where \\(I_x = \\frac{\\partial I}{\\partial x}\\), \\(I_y = \\frac{\\partial I}{\\partial y}\\), \\(I_t = \\frac{\\partial I}{\\partial t}\\), \\(u = \\frac{\\Delta x}{\\Delta t}\\), and \\(v = \\frac{\\Delta y}{\\Delta t}\\) are the horizontal and vertical components of the optical flow velocity, respectively.\r\n",
    "\r\n",
    "2. **Aperture Problem**:\r\n",
    "   The optical flow constraint equation provides one equation with two unknowns (u and v), leading to an under-determined system known as the aperture problem. To resolve this, additional constraints or assumptions are needed.\r\n",
    "\r\n",
    "3. **Methods for Optical Flow Estimation**:\r\n",
    "   - **Lucas-Kanade Method**:\r\n",
    "     This method assumes that the flow is essentially constant in a small neighborhood around each pixel. By combining the optical flow constraint equations for all the pixels in the neighborhood, a system of linear equations is formed, which can be solved using least squares:\r\n",
    "\r\n",
    "     \\[\r\n",
    "     A^T A \\mathbf{v} = A^T \\mathbf{b}\r\n",
    "     \\]\r\n",
    "\r\n",
    "     where \\(A\\) is a matrix containing gradients \\(I_x\\) and \\(I_y\\), and \\(\\mathbf{b}\\) is a vector containing the negative temporal gradient \\(-I_t\\).\r\n",
    "\r\n",
    "   - **Horn-Schunck Method**:\r\n",
    "     This method introduces a global smoothness constraint, which assumes that the flow field varies smoothly over the entire image. The method minimizes an energy function that includes both the optical flow constraint and a smoothness term:\r\n",
    "\r\n",
    "     \\[\r\n",
    "     E = \\int \\int \\left( (I_x u + I_y v + I_t)^2 + \\alpha^2 \\left( \\left| \\nabla u \\right|^2 + \\left| \\nabla v \\right|^2 \\right) \\right) dx dy\r\n",
    "     \\]\r\n",
    "\r\n",
    "     Here, \\(\\alpha\\) is a regularization parameter balancing the data and smoothness terms. The corresponding Euler-Lagrange equations lead to iterative solutions for \\(u\\) and \\(v\\).\r\n",
    "\r\n",
    "4. **Pyramidal Approach**:\r\n",
    "   To handle large motions and reduce computational complexity, a pyramidal approach can be used. This involves creating a pyramid of images at multiple scales (resolutions) and computing optical flow from the coarsest to the finest level, refining the flow estimates progressively.\r\n",
    "\r\n",
    "### Applications of Optical Flow\r\n",
    "\r\n",
    "- **Video Compression**: Predicting motion between frames can significantly reduce the amount of data required to encode video sequences.\r\n",
    "- **Object Tracking**: Tracking the movement of objects across frames is a direct application of optical flow.\r\n",
    "- **Motion Detection**: Optical flow can be used to detect and segment moving objects from the background.\r\n",
    "- **Autonomous Navigation**: Optical flow provides crucial information about the environment for navigation and obstacle avoidance in robotics and autonomous vehicles.\r\n",
    "- **Medical Imaging**: Analyzing the movement of organs and tissues in medical scans.\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "Optical flow is a fundamental tool in computer vision for estimating the motion of objects between frames. While the basic optical flow constraint equation lays the foundation, practical implementations often require additional constraints and methods to handle the under-determined nature of the problem and to achieve robust performance in real-world scenarios. Techniques such as the Lucas-Kanade and Horn-Schunck methods, along with multi-scale approaches, are commonly used to estimate optical flow effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4e6835-67da-4c60-8ddb-dd46fe2f36dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\video\\src\\lkpyramid.cpp:1260: error: (-215:Assertion failed) (npoints = prevPtsMat.checkVector(2, CV_32F, true)) >= 0 in function 'cv::`anonymous-namespace'::SparsePyrLKOpticalFlowImpl::calc'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m frame_gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Calculate optical flow\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m p1, st, err \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcalcOpticalFlowPyrLK(old_gray, frame_gray, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlk_params)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Select good points\u001b[39;00m\n\u001b[0;32m     28\u001b[0m good_new \u001b[38;5;241m=\u001b[39m p1[st \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\video\\src\\lkpyramid.cpp:1260: error: (-215:Assertion failed) (npoints = prevPtsMat.checkVector(2, CV_32F, true)) >= 0 in function 'cv::`anonymous-namespace'::SparsePyrLKOpticalFlowImpl::calc'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Capture video from webcam or file\n",
    "cap = cv2.VideoCapture('video.mp4')  # Change 'video.mp4' to 0 for webcam\n",
    "\n",
    "# Parameters for Lucas-Kanade optical flow\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
    "                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Take first frame\n",
    "ret, old_frame = cap.read()\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate optical flow\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, None, None, **lk_params)\n",
    "\n",
    "    # Select good points\n",
    "    good_new = p1[st == 1]\n",
    "    good_old = p1[st == 1]\n",
    "\n",
    "    # Draw the tracks\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel().astype(int)\n",
    "        c, d = old.ravel().astype(int)\n",
    "        mask = cv2.line(mask, (a, b), (c, d), (0, 255, 0), 2)\n",
    "        frame = cv2.circle(frame, (a, b), 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Overlay the optical flow on the original frame\n",
    "    img = cv2.add(frame, mask)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', img)\n",
    "\n",
    "    # Update the previous frame and previous points\n",
    "    old_gray = frame_gray.copy()\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ddb799d-ca25-4c38-a309-dc4632988c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def block_matching_motion_estimation(ref_frame, curr_frame, block_size=16, search_range=7):\n",
    "    height, width = ref_frame.shape\n",
    "    motion_vectors = np.zeros((height // block_size, width // block_size, 2), dtype=int)\n",
    "\n",
    "    for y in range(0, height, block_size):\n",
    "        for x in range(0, width, block_size):\n",
    "            best_match = (0, 0)\n",
    "            min_sad = float('inf')\n",
    "            \n",
    "            current_block = curr_frame[y:y + block_size, x:x + block_size]\n",
    "\n",
    "            for dy in range(-search_range, search_range + 1):\n",
    "                for dx in range(-search_range, search_range + 1):\n",
    "                    ref_y = y + dy\n",
    "                    ref_x = x + dx\n",
    "\n",
    "                    if ref_y < 0 or ref_y + block_size > height or ref_x < 0 or ref_x + block_size > width:\n",
    "                        continue\n",
    "\n",
    "                    reference_block = ref_frame[ref_y:ref_y + block_size, ref_x:ref_x + block_size]\n",
    "\n",
    "                    sad = np.sum(np.abs(current_block - reference_block))\n",
    "\n",
    "                    if sad < min_sad:\n",
    "                        min_sad = sad\n",
    "                        best_match = (dy, dx)\n",
    "            \n",
    "            motion_vectors[y // block_size, x // block_size] = best_match\n",
    "\n",
    "    return motion_vectors\n",
    "\n",
    "def apply_motion_compensation(ref_frame, motion_vectors, block_size=16):\n",
    "    height, width = ref_frame.shape\n",
    "    compensated_frame = np.zeros_like(ref_frame)\n",
    "\n",
    "    for y in range(0, height, block_size):\n",
    "        for x in range(0, width, block_size):\n",
    "            dy, dx = motion_vectors[y // block_size, x // block_size]\n",
    "            ref_y = y + dy\n",
    "            ref_x = x + dx\n",
    "            compensated_frame[y:y + block_size, x:x + block_size] = ref_frame[ref_y:ref_y + block_size, ref_x:ref_x + block_size]\n",
    "\n",
    "    return compensated_frame\n",
    "\n",
    "# Load video frames (example with OpenCV)\n",
    "cap = cv2.VideoCapture('video.mp4')\n",
    "ret, ref_frame = cap.read()\n",
    "ref_frame_gray = cv2.cvtColor(ref_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, curr_frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    curr_frame_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    motion_vectors = block_matching_motion_estimation(ref_frame_gray, curr_frame_gray)\n",
    "    compensated_frame = apply_motion_compensation(ref_frame_gray, motion_vectors)\n",
    "\n",
    "    # Display the original and compensated frames\n",
    "    cv2.imshow('Original Frame', curr_frame_gray)\n",
    "    cv2.imshow('Compensated Frame', compensated_frame)\n",
    "\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    ref_frame_gray = curr_frame_gray\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a164877a-4e0c-44ea-8e58-5e051cc367ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def draw_tracks(frame, tracks):\n",
    "    for track in tracks:\n",
    "        for i in range(1, len(track)):\n",
    "            pt1 = tuple(map(int, track[i - 1]))\n",
    "            pt2 = tuple(map(int, track[i]))\n",
    "            cv2.line(frame, pt1, pt2, (0, 255, 0), 2)\n",
    "    return frame\n",
    "\n",
    "# Parameters for ShiTomasi corner detection\n",
    "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "\n",
    "# Parameters for Lucas-Kanade optical flow\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
    "                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "cap = cv2.VideoCapture('video.mp4')\n",
    "ret, old_frame = cap.read()\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "\n",
    "# Create an array to store the tracks\n",
    "tracks = [[] for _ in range(len(p0))]\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate optical flow\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "    # Select good points\n",
    "    good_new = p1[st == 1]\n",
    "    good_old = p0[st == 1]\n",
    "\n",
    "    # Update the tracks\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel()\n",
    "        c, d = old.ravel()\n",
    "        tracks[i].append((a, b))\n",
    "        pt1 = tuple(map(int, (a, b)))\n",
    "        pt2 = tuple(map(int, (c, d)))\n",
    "        mask = cv2.line(mask, pt1, pt2, (0, 255, 0), 2)\n",
    "        frame = cv2.circle(frame, pt1, 5, (0, 255, 0), -1)\n",
    "\n",
    "    img = cv2.add(frame, mask)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', img)\n",
    "\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "    # Update the previous frame and previous points\n",
    "    old_gray = frame_gray.copy()\n",
    "    p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38f1926f-a6e1-4263-84ca-5a814a19a12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Capture video from webcam or file\n",
    "cap = cv2.VideoCapture('video.mp4')  # Change 'video.mp4' to 0 for webcam\n",
    "\n",
    "# Parameters for ShiTomasi corner detection\n",
    "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "\n",
    "# Parameters for Lucas-Kanade optical flow\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
    "                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Take first frame and find corners\n",
    "ret, old_frame = cap.read()\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate optical flow\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "    # Select good points\n",
    "    good_new = p1[st == 1]\n",
    "    good_old = p0[st == 1]\n",
    "\n",
    "    # Draw the tracks\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel()\n",
    "        c, d = old.ravel()\n",
    "        a, b, c, d = int(a), int(b), int(c), int(d)  # Convert points to integers\n",
    "        mask = cv2.line(mask, (a, b), (c, d), (0, 255, 0), 2)\n",
    "        frame = cv2.circle(frame, (a, b), 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Overlay the optical flow on the original frame\n",
    "    img = cv2.add(frame, mask)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', img)\n",
    "\n",
    "    # Update the previous frame and previous points\n",
    "    old_gray = frame_gray.copy()\n",
    "    p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec5bcb-7bac-4a63-bae5-d8b792a5eaa8",
   "metadata": {},
   "source": [
    "# Object Tracking:\r\n",
    "\r\n",
    "Tracking the movement of objects across frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f57876-b836-41b2-aa5a-4e6128c38caf",
   "metadata": {},
   "source": [
    "Object tracking in computer vision involves locating and following a specific object or multiple objects in a video sequence. Here's a detailed overview of object tracking, including techniques, equations, and considerations:\r\n",
    "\r\n",
    "### 1. Object Tracking Techniques:\r\n",
    "\r\n",
    "#### 1.1. **Template Matching**:\r\n",
    "   - Compares a template image to regions in a larger image to find matches.\r\n",
    "   - Measures similarity using methods like Cross-correlation or Sum of Squared Differences (SSD).\r\n",
    "\r\n",
    "#### 1.2. **Optical Flow**:\r\n",
    "   - Estimates the motion of objects by analyzing pixel intensity changes between consecutive frames.\r\n",
    "   - Common algorithms include Lucas-Kanade method and Horn-Schunck method.\r\n",
    "\r\n",
    "#### 1.3. **Kalman Filters**:\r\n",
    "   - Predicts the next state of an object based on its previous state and motion model.\r\n",
    "   - Incorporates measurements to correct predictions and estimate the most likely state.\r\n",
    "\r\n",
    "#### 1.4. **Particle Filters (Sequential Monte Carlo)**:\r\n",
    "   - Represents the posterior probability density of the object's state using a set of particles.\r\n",
    "   - Samples particles from the state space and updates their weights based on measurements.\r\n",
    "\r\n",
    "#### 1.5. **Deep Learning-based Tracking**:\r\n",
    "   - Utilizes deep neural networks to learn representations for tracking tasks.\r\n",
    "   - Techniques include Siamese Networks, Fully Convolutional Networks (FCNs), and Recurrent Neural Networks (RNNs).\r\n",
    "\r\n",
    "### 2. Equations and Concepts:\r\n",
    "\r\n",
    "#### 2.1. **Motion Model**:\r\n",
    "   - Represents how an object's state evolves over time.\r\n",
    "   - Common motion models include constant velocity (CV) and constant acceleration (CA) models.\r\n",
    "\r\n",
    "#### 2.2. **State Estimation**:\r\n",
    "   - Predicts the current state of the object based on previous states and motion model.\r\n",
    "   - Typically implemented using Kalman filters or particle filters.\r\n",
    "\r\n",
    "#### 2.3. **Measurement Model**:\r\n",
    "   - Describes how measurements are related to the object's state.\r\n",
    "   - Involves converting object properties (e.g., position) into observable quantities (e.g., pixel coordinates).\r\n",
    "\r\n",
    "#### 2.4. **Resampling (Particle Filters)**:\r\n",
    "   - Updates the set of particles based on their weights to ensure a representative sample.\r\n",
    "   - High-weight particles are more likely to survive, while low-weight particles may be replaced.\r\n",
    "\r\n",
    "### 3. Implementation Considerations:\r\n",
    "\r\n",
    "#### 3.1. **Initialization**:\r\n",
    "   - Choosing an initial bounding box or region of interest (ROI) for the object to track.\r\n",
    "\r\n",
    "#### 3.2. **Motion Model Selection**:\r\n",
    "   - Determining the appropriate motion model based on the object's dynamics.\r\n",
    "\r\n",
    "#### 3.3. **Measurement Update**:\r\n",
    "   - Incorporating measurements from the current frame to refine object state estimates.\r\n",
    "\r\n",
    "#### 3.4. **Handling Occlusions and Track Loss**:\r\n",
    "   - Dealing with situations where the object is temporarily invisible or leaves the frame.\r\n",
    "\r\n",
    "#### 3.5. **Performance Optimization**:\r\n",
    "   - Employing techniques to improve tracking speed and accuracy, such as feature selection and parallelization.\r\n",
    "\r\n",
    "Object tracking is a fundamental task in many computer vision applications, including surveillance, human-computer interaction, and autonomous navigation. By understanding various tracking techniques and their underlying principles, developers can implement robust and efficient tracking systems tailored to specific use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d5e7f9-a76d-4309-8b6a-f0154d81c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the main image\n",
    "main_image = cv2.imread('My.jpg')\n",
    "if main_image is None:\n",
    "    print(\"Error: Could not read the main image.\")\n",
    "    exit()\n",
    "\n",
    "# Convert to grayscale\n",
    "gray_main = cv2.cvtColor(main_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Load the template image\n",
    "template = cv2.imread('My.jpg', 0)\n",
    "if template is None:\n",
    "    print(\"Error: Could not read the template image.\")\n",
    "    exit()\n",
    "\n",
    "# Get the width and height of the template\n",
    "w, h = template.shape[::-1]\n",
    "\n",
    "# Perform template matching using cv2.matchTemplate\n",
    "res = cv2.matchTemplate(gray_main, template, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "# Define a threshold to find matches\n",
    "threshold = 0.8\n",
    "loc = np.where(res >= threshold)\n",
    "\n",
    "# Draw rectangles around the matched regions\n",
    "for pt in zip(*loc[::-1]):\n",
    "    cv2.rectangle(main_image, pt, (pt[0] + w, pt[1] + h), (0, 255, 255), 2)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Detected', main_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d8b0a4-e2d2-4566-b7ea-1f7af7febc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read the video from file\n",
    "cap = cv2.VideoCapture('video.mp4')\n",
    "\n",
    "# Parameters for Lucas-Kanade optical flow\n",
    "lk_params = dict(winSize=(15, 15),\n",
    "                 maxLevel=2,\n",
    "                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Take first frame and find corners in it\n",
    "ret, old_frame = cap.read()\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7))\n",
    "\n",
    "# Create some random colors\n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate optical flow\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "    # Select good points\n",
    "    good_new = p1[st == 1]\n",
    "    good_old = p0[st == 1]\n",
    "\n",
    "    # Draw the tracks\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = map(int, new.ravel())\n",
    "        c, d = map(int, old.ravel())\n",
    "        mask = cv2.line(mask, (a, b), (c, d), color[i].tolist(), 2)\n",
    "        frame = cv2.circle(frame, (a, b), 5, color[i].tolist(), -1)\n",
    "    img = cv2.add(frame, mask)\n",
    "\n",
    "    cv2.imshow('frame', img)\n",
    "    if cv2.waitKey(30) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "    # Now update the previous frame and previous points\n",
    "    old_gray = frame_gray.copy()\n",
    "    p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837cc506-8a6b-4c82-b1f0-67a1a8d9675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def horn_schunck(I1, I2, alpha, Niter):\n",
    "    I1 = I1.astype(float)\n",
    "    I2 = I2.astype(float)\n",
    "\n",
    "    # Set initial values for u, v\n",
    "    u = np.zeros(I1.shape)\n",
    "    v = np.zeros(I1.shape)\n",
    "\n",
    "    # Estimate derivatives\n",
    "    Ix = cv2.Sobel(I1, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    Iy = cv2.Sobel(I1, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    It = I2 - I1\n",
    "\n",
    "    # Averaging kernel\n",
    "    kernel = np.array([[1/12, 1/6, 1/12],\n",
    "                       [1/6,  0,   1/6],\n",
    "                       [1/12, 1/6, 1/12]], float)\n",
    "\n",
    "    for _ in range(Niter):\n",
    "        u_avg = cv2.filter2D(u, -1, kernel)\n",
    "        v_avg = cv2.filter2D(v, -1, kernel)\n",
    "        P = (Ix * u_avg + Iy * v_avg + It) / (alpha**2 + Ix**2 + Iy**2)\n",
    "        u = u_avg - Ix * P\n",
    "        v = v_avg - Iy * P\n",
    "\n",
    "    return u, v\n",
    "\n",
    "# Read the video from file\n",
    "cap = cv2.VideoCapture('video.mp4')\n",
    "\n",
    "ret, frame1 = cap.read()\n",
    "frame1_gray = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "while True:\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame2_gray = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    u, v = horn_schunck(frame1_gray, frame2_gray, alpha=1, Niter=100)\n",
    "\n",
    "    # Visualize the flow vectors\n",
    "    hsv = np.zeros((frame1_gray.shape[0], frame1_gray.shape[1], 3), dtype=np.uint8)\n",
    "    hsv[..., 1] = 255\n",
    "\n",
    "    mag, ang = cv2.cartToPolar(u, v)\n",
    "    hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    cv2.imshow('frame', rgb)\n",
    "    if cv2.waitKey(30) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "    frame1_gray = frame2_gray\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4605868f-21cc-4494-adb3-1dc7e1988f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initial state (location and velocity)\n",
    "x = np.array([[0],  # initial position\n",
    "              [0]]) # initial velocity\n",
    "\n",
    "# State transition matrix (assuming constant velocity model)\n",
    "F = np.array([[1, 1],  # [1, delta_t] for position update\n",
    "              [0, 1]]) # [0, 1] for velocity update\n",
    "\n",
    "# Measurement matrix (we can only measure position)\n",
    "H = np.array([[1, 0]])\n",
    "\n",
    "# Measurement noise covariance\n",
    "R = np.array([[1]])\n",
    "\n",
    "# Process noise covariance\n",
    "Q = np.array([[1, 0],\n",
    "              [0, 1]])\n",
    "\n",
    "# Initial estimation error covariance\n",
    "P = np.array([[1, 0],\n",
    "              [0, 1]])\n",
    "\n",
    "# Control input (acceleration, for example)\n",
    "u = np.array([[0], \n",
    "              [0]])\n",
    "\n",
    "# Control matrix\n",
    "B = np.array([[0.5], \n",
    "              [1]])\n",
    "\n",
    "# Identity matrix\n",
    "I = np.eye(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "864200b8-0ae0-4b77-be5c-dcb2eee5514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, P, F, Q, u, B):\n",
    "    x = F @ x + B @ u  # State prediction\n",
    "    P = F @ P @ F.T + Q  # Covariance prediction\n",
    "    return x, P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "002bade3-0de3-471d-b9f0-93172b17ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(x, P, Z, H, R):\n",
    "    y = Z - H @ x  # Measurement residual\n",
    "    S = H @ P @ H.T + R  # Residual covariance\n",
    "    K = P @ H.T @ np.linalg.inv(S)  # Kalman gain\n",
    "    x = x + K @ y  # State update\n",
    "    P = (I - K @ H) @ P  # Covariance update\n",
    "    return x, P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d3761e8-a464-4864-967a-e572622b5941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State estimate: \n",
      " [[0.75]\n",
      " [0.25]]\n",
      "Covariance estimate: \n",
      " [[0.75 0.25]\n",
      " [0.25 1.75]]\n",
      "State estimate: \n",
      " [[1.8 ]\n",
      " [0.65]]\n",
      "Covariance estimate: \n",
      " [[0.8  0.4 ]\n",
      " [0.4  1.95]]\n",
      "State estimate: \n",
      " [[2.9009009 ]\n",
      " [0.88288288]]\n",
      "Covariance estimate: \n",
      " [[0.81981982 0.42342342]\n",
      " [0.42342342 1.95495495]]\n",
      "State estimate: \n",
      " [[3.96153846]\n",
      " [0.97435897]]\n",
      "Covariance estimate: \n",
      " [[0.82211538 0.42307692]\n",
      " [0.42307692 1.94871795]]\n",
      "State estimate: \n",
      " [[4.98858773]\n",
      " [1.00142653]]\n",
      "Covariance estimate: \n",
      " [[0.82196862 0.42225392]\n",
      " [0.42225392 1.94721826]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initial state (location and velocity)\n",
    "x = np.array([[0],  # initial position\n",
    "              [0]]) # initial velocity\n",
    "\n",
    "# State transition matrix (assuming constant velocity model)\n",
    "F = np.array([[1, 1],  # [1, delta_t] for position update\n",
    "              [0, 1]]) # [0, 1] for velocity update\n",
    "\n",
    "# Measurement matrix (we can only measure position)\n",
    "H = np.array([[1, 0]])\n",
    "\n",
    "# Measurement noise covariance\n",
    "R = np.array([[1]])\n",
    "\n",
    "# Process noise covariance\n",
    "Q = np.array([[1, 0],\n",
    "              [0, 1]])\n",
    "\n",
    "# Initial estimation error covariance\n",
    "P = np.array([[1, 0],\n",
    "              [0, 1]])\n",
    "\n",
    "# Control input (acceleration, for example)\n",
    "u = np.array([[0]])  # assuming no control input\n",
    "\n",
    "# Control matrix\n",
    "B = np.array([[0], \n",
    "              [0]])  # assuming no control input effect on position and velocity\n",
    "\n",
    "# Identity matrix\n",
    "I = np.eye(2)\n",
    "\n",
    "def predict(x, P, F, Q, u, B):\n",
    "    x = F @ x + B @ u  # State prediction\n",
    "    P = F @ P @ F.T + Q  # Covariance prediction\n",
    "    return x, P\n",
    "\n",
    "def update(x, P, Z, H, R):\n",
    "    y = Z - H @ x  # Measurement residual\n",
    "    S = H @ P @ H.T + R  # Residual covariance\n",
    "    K = P @ H.T @ np.linalg.inv(S)  # Kalman gain\n",
    "    x = x + K @ y  # State update\n",
    "    P = (I - K @ H) @ P  # Covariance update\n",
    "    return x, P\n",
    "\n",
    "# Simulated measurements (for example, noisy position measurements)\n",
    "measurements = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Run Kalman Filter\n",
    "for z in measurements:\n",
    "    x, P = predict(x, P, F, Q, u, B)\n",
    "    x, P = update(x, P, np.array([[z]]), H, R)\n",
    "    print(\"State estimate: \\n\", x)\n",
    "    print(\"Covariance estimate: \\n\", P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d21b48b-92f2-4f51-8f0f-3b2399360643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated state: [ 0.0454986  -0.02005369]\n",
      "Estimated state: [0.41256888 0.28438928]\n",
      "Estimated state: [0.82182439 0.76469088]\n",
      "Estimated state: [1.39775968 1.39647948]\n",
      "Estimated state: [1.8895624  1.69085101]\n",
      "Estimated state: [2.24594535 1.66451034]\n",
      "Estimated state: [2.49969525 1.71447346]\n",
      "Estimated state: [2.71463496 1.82491497]\n",
      "Estimated state: [2.90542744 1.97404892]\n",
      "Estimated state: [3.08366087 2.14263308]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ParticleFilter:\n",
    "    def __init__(self, num_particles, state_dim, process_noise_std, measurement_noise_std, state_transition_fn, measurement_fn):\n",
    "        self.num_particles = num_particles\n",
    "        self.state_dim = state_dim\n",
    "        self.particles = np.random.randn(num_particles, state_dim)  # Initialize particles randomly\n",
    "        self.weights = np.ones(num_particles) / num_particles  # Initialize weights uniformly\n",
    "        self.process_noise_std = process_noise_std\n",
    "        self.measurement_noise_std = measurement_noise_std\n",
    "        self.state_transition_fn = state_transition_fn\n",
    "        self.measurement_fn = measurement_fn\n",
    "\n",
    "    def predict(self):\n",
    "        # Predict the next state of each particle\n",
    "        for i in range(self.num_particles):\n",
    "            noise = np.random.randn(self.state_dim) * self.process_noise_std\n",
    "            self.particles[i] = self.state_transition_fn(self.particles[i]) + noise\n",
    "\n",
    "    def update(self, measurement):\n",
    "        # Update the weights based on the measurement likelihood\n",
    "        for i in range(self.num_particles):\n",
    "            predicted_measurement = self.measurement_fn(self.particles[i])\n",
    "            self.weights[i] = self.measurement_likelihood(measurement, predicted_measurement)\n",
    "        self.weights += 1.e-300  # Avoid division by zero\n",
    "        self.weights /= np.sum(self.weights)  # Normalize the weights\n",
    "\n",
    "    def measurement_likelihood(self, measurement, predicted_measurement):\n",
    "        # Assuming Gaussian noise\n",
    "        error = measurement - predicted_measurement\n",
    "        return np.exp(-0.5 * np.dot(error, error) / (self.measurement_noise_std**2)) / (np.sqrt(2 * np.pi) * self.measurement_noise_std)\n",
    "\n",
    "    def resample(self):\n",
    "        # Resample particles based on their weights\n",
    "        indices = np.random.choice(self.num_particles, size=self.num_particles, p=self.weights)\n",
    "        self.particles = self.particles[indices]\n",
    "        self.weights.fill(1.0 / self.num_particles)\n",
    "\n",
    "    def estimate(self):\n",
    "        # Estimate the state as the mean of the particles\n",
    "        return np.average(self.particles, weights=self.weights, axis=0)\n",
    "\n",
    "# Example usage:\n",
    "# Define state transition and measurement functions\n",
    "def state_transition(state):\n",
    "    # Simple linear motion model: x_next = x + v\n",
    "    return state\n",
    "\n",
    "def measurement_fn(state):\n",
    "    # Measurement model: z = x\n",
    "    return state\n",
    "\n",
    "# Create a particle filter\n",
    "num_particles = 1000\n",
    "state_dim = 2  # For example, 2D state [x, y]\n",
    "process_noise_std = 0.1\n",
    "measurement_noise_std = 1.0\n",
    "\n",
    "pf = ParticleFilter(num_particles, state_dim, process_noise_std, measurement_noise_std, state_transition, measurement_fn)\n",
    "\n",
    "# Simulate a series of measurements\n",
    "measurements = [np.array([i, i]) for i in range(10)]\n",
    "\n",
    "for measurement in measurements:\n",
    "    pf.predict()\n",
    "    pf.update(measurement)\n",
    "    pf.resample()\n",
    "    estimate = pf.estimate()\n",
    "    print(f\"Estimated state: {estimate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4ca83d1-bd2e-4e30-b649-ef0ad8a0040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class TrackingDataset(Dataset):\n",
    "    def __init__(self, video_path, transform=None):\n",
    "        self.video_path = video_path\n",
    "        self.transform = transform\n",
    "        self.frames = self.load_video_frames(video_path)\n",
    "        self.pairs, self.labels = self.create_pairs(self.frames)\n",
    "\n",
    "    def load_video_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "    def create_pairs(self, frames):\n",
    "        pairs = []\n",
    "        labels = []\n",
    "        num_frames = len(frames)\n",
    "        for i in range(num_frames - 1):\n",
    "            pairs.append([frames[i], frames[i + 1]])\n",
    "            labels.append(1)\n",
    "        return pairs, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1, img2 = self.pairs[idx]\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        label = self.labels[idx]\n",
    "        return img1, img2, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = TrackingDataset('video.mp4', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7587f570-c0ed-4d89-9d24-88e8ee26a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=10)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=7)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=4)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4)\n",
    "        self.fc1 = nn.Linear(256*6*6, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 1)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), (2, 2)))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), (2, 2)))\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), (2, 2)))\n",
    "        x = F.relu(F.max_pool2d(self.conv4(x), (2, 2)))\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        return euclidean_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4949ddda-2696-4bec-a86d-3f801b688249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SiameseNetwork().to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for img1, img2, label in dataloader:\n",
    "        img1, img2, label = img1.to(device), img2.to(device), label.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        #output1 = model.forward_once(img1)\n",
    "        #output2 = model.forward_once(img2)\n",
    "        #loss = criterion(output1, output2, label)\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "\n",
    "    #print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c55f1-6213-4b34-9145-4a84161c8299",
   "metadata": {},
   "source": [
    "# Video Compression:\r\n",
    "\r\n",
    "Techniques to reduce the size of video files for storage or transmission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db011a-8648-4279-a1b8-eafd807ae5ef",
   "metadata": {},
   "source": [
    "## Video Compression: Techniques and Principles\r\n",
    "\r\n",
    "### Introduction\r\n",
    "\r\n",
    "Video compression is the process of reducing the size of video files for storage or transmission while maintaining an acceptable level of quality. This involves encoding the video data in such a way that it requires fewer bits. Video compression techniques are essential for streaming services, video conferencing, and digital broadcasting. The main goal is to minimize the bit rate while preserving the perceptual quality of the video.\r\n",
    "\r\n",
    "### Key Concepts in Video Compression\r\n",
    "\r\n",
    "1. **Spatial Redundancy**: Refers to the redundancy within a single frame. Compression techniques exploit the fact that neighboring pixels in a frame are often similar.\r\n",
    "\r\n",
    "2. **Temporal Redundancy**: Refers to the redundancy between consecutive frames. Compression techniques utilize the similarity between frames to reduce the data required to represent them.\r\n",
    "\r\n",
    "3. **Psycho-visual Redundancy**: Takes advantage of the human visual system’s insensitivity to certain changes, allowing for lossy compression that removes less noticeable details.\r\n",
    "\r\n",
    "### Basic Components of Video Compression\r\n",
    "\r\n",
    "1. **Encoder and Decoder (Codec)**: A codec is a device or software that encodes and decodes digital video. Popular codecs include H.264, H.265 (HEVC), and VP9.\r\n",
    "\r\n",
    "2. **Bit Rate**: The amount of data processed per unit of time, typically measured in bits per second (bps). Lower bit rates reduce file size but can affect quality.\r\n",
    "\r\n",
    "3. **Frame Rate**: The number of frames displayed per second (fps). Common frame rates are 24, 30, and 60 fps.\r\n",
    "\r\n",
    "4. **Resolution**: The number of pixels in each dimension that can be displayed, e.g., 1920x1080 (Full HD).\r\n",
    "\r\n",
    "### Compression Techniques\r\n",
    "\r\n",
    "#### 1. **Lossless Compression**\r\n",
    "\r\n",
    "Lossless compression reduces file size without losing any information, allowing the original video to be perfectly reconstructed. It’s less effective in reducing file size compared to lossy compression.\r\n",
    "\r\n",
    "- **Run-Length Encoding (RLE)**: Encodes sequences of identical values by storing the value and its count.\r\n",
    "- **Huffman Coding**: Uses variable-length codes for different symbols, with more common symbols using shorter codes.\r\n",
    "- **Lempel-Ziv-Welch (LZW)**: Builds a dictionary of commonly occurring patterns to replace repeated patterns with shorter codes.\r\n",
    "\r\n",
    "#### 2. **Lossy Compression**\r\n",
    "\r\n",
    "Lossy compression significantly reduces file size by removing some information, which can lead to a loss in quality. It exploits psycho-visual redundancy.\r\n",
    "\r\n",
    "- **Transform Coding**: Converts spatial domain data to frequency domain data using transforms like the Discrete Cosine Transform (DCT) or Discrete Wavelet Transform (DWT).\r\n",
    "  \r\n",
    "  **DCT Equation**:\r\n",
    "  \\[\r\n",
    "  F(u,v) = \\frac{1}{4} C(u)C(v) \\sum_{x=0}^{N-1} \\sum_{y=0}^{N-1} f(x,y) \\cos \\left[\\frac{(2x+1)u\\pi}{2N}\\right] \\cos \\left[\\frac{(2y+1)v\\pi}{2N}\\right]\r\n",
    "  \\]\r\n",
    "  where \\( C(u) \\) and \\( C(v) \\) are normalization factors, and \\( f(x,y) \\) is the pixel value at coordinates \\( (x,y) \\).\r\n",
    "\r\n",
    "- **Quantization**: Reduces the precision of the transformed coefficients, which is the primary source of loss in lossy compression.\r\n",
    "\r\n",
    "  **Quantization Equation**:\r\n",
    "  \\[\r\n",
    "  Q(u,v) = \\left\\lfloor \\frac{F(u,v)}{Q(u,v)} \\right\\rfloor\r\n",
    "  \\]\r\n",
    "  where \\( Q(u,v) \\) is a quantization matrix.\r\n",
    "\r\n",
    "- **Entropy Coding**: Further compresses the quantized coefficients using methods like Huffman coding or Arithmetic coding.\r\n",
    "\r\n",
    "#### 3. **Predictive Coding**\r\n",
    "\r\n",
    "Exploits temporal redundancy by predicting future frames based on previous ones.\r\n",
    "\r\n",
    "- **Intra-frame Coding**: Encodes each frame independently (used for keyframes).\r\n",
    "- **Inter-frame Coding**: Encodes the difference between frames.\r\n",
    "\r\n",
    "  **Motion Compensation**:\r\n",
    "  \\[\r\n",
    "  P_t = F_t - M(F_{t-1}, v)\r\n",
    "  \\]\r\n",
    "  where \\( P_t \\) is the predicted frame, \\( F_t \\) is the current frame, \\( F_{t-1} \\) is the previous frame, and \\( M \\) is the motion vector.\r\n",
    "\r\n",
    "### Advanced Techniques\r\n",
    "\r\n",
    "- **Motion Estimation and Compensation**: Identifies and compensates for the motion of objects between frames.\r\n",
    "  \r\n",
    "  **Motion Vector Calculation**:\r\n",
    "  \\[\r\n",
    "  MV = (dx, dy)\r\n",
    "  \\]\r\n",
    "  where \\( dx \\) and \\( dy \\) represent the displacement in horizontal and vertical directions, respectively.\r\n",
    "\r\n",
    "- **Rate-Distortion Optimization**: Balances the trade-off between bit rate and distortion.\r\n",
    "  \r\n",
    "  **Rate-Distortion Function**:\r\n",
    "  \\[\r\n",
    "  D(R) = D_0 e^{-\\lambda R}\r\n",
    "  \\]\r\n",
    "  where \\( D \\) is the distortion, \\( R \\) is the bit rate, \\( D_0 \\) is the initial distortion, and \\( \\lambda \\) is a constant.\r\n",
    "\r\n",
    "### Popular Video Compression Standards\r\n",
    "\r\n",
    "1. **H.264/AVC**: Widely used for its high compression efficiency and good quality.\r\n",
    "2. **H.265/HEVC**: Provides better compression than H.264, supporting higher resolutions like 4K.\r\n",
    "3. **VP9**: An open and royalty-free codec used mainly for web video streaming.\r\n",
    "4. **AV1**: A newer open-source codec designed to succeed VP9, offering better compression rates.\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "Video compression is a vital technology enabling efficient storage and transmission of video content. By leveraging techniques that reduce spatial, temporal, and psycho-visual redundancy, significant reductions in file size can be achieved. Both lossy and lossless methods have their applications, with advanced techniques like motion compensation and rate-distortion optimization playing crucial roles in modern codecs. Understanding these principles and equations helps in appreciating how video compression works to deliver high-quality video at manageable data rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d06e5f-7603-45c5-a4b7-d4e59a3fcc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def compress_video(input_video_path, output_video_path, codec='XVID', fps=None, resolution=None, bit_rate=None):\n",
    "    # Open the input video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file: {input_video_path}\")\n",
    "        return\n",
    "\n",
    "    # Get the original video properties\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS) if fps is None else fps\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Set the resolution\n",
    "    width, height = resolution if resolution else (original_width, original_height)\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, original_fps, (width, height))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Optionally, resize the frame\n",
    "        if resolution:\n",
    "            frame = cv2.resize(frame, (width, height))\n",
    "        \n",
    "        # Write the frame to the output file\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release everything if job is finished\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if bit_rate:\n",
    "        # Adjust bitrate using FFmpeg\n",
    "        import os\n",
    "        temp_file = output_video_path.replace('.avi', '_temp.avi')\n",
    "        os.rename(output_video_path, temp_file)\n",
    "        ffmpeg_command = f'ffmpeg -i {temp_file} -b:v {bit_rate} -bufsize {bit_rate} {output_video_path}'\n",
    "        os.system(ffmpeg_command)\n",
    "        os.remove(temp_file)\n",
    "\n",
    "# Example usage\n",
    "compress_video('video.mp4', 'output_compressed.avi', codec='XVID', resolution=(640, 480), bit_rate='500k')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97cdfff-77db-4f4e-bb95-0bc047212eb7",
   "metadata": {},
   "source": [
    "# Video Decompression:\r\n",
    "\r\n",
    "Techniques to decode compressed video data back into individual frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b3e9ec-1291-4497-a19b-91f446e14fe5",
   "metadata": {},
   "source": [
    "Video decompression is the process of converting compressed video data back into its original format, typically a sequence of individual frames. This process is crucial for viewing and editing videos efficiently. There are various techniques for video decompression, with the most common ones being based on lossless or lossy compression algorithms. Here, I'll provide an overview of these techniques along with equations where relevant.\r\n",
    "\r\n",
    "### Lossless Compression Techniques:\r\n",
    "\r\n",
    "1. **Run-Length Encoding (RLE):**\r\n",
    "   - RLE is a simple form of compression that replaces sequences of identical pixels with a count value and the pixel value itself.\r\n",
    "   - Equation: Let \\( C \\) represent the count of repeated pixels and \\( P \\) represent the pixel value. The compressed data can be represented as \\( (C, P) \\).\r\n",
    "\r\n",
    "2. **Huffman Coding:**\r\n",
    "   - Huffman coding assigns variable-length codes to different symbols (e.g., pixel values) based on their frequencies in the data. More frequent symbols are assigned shorter codes.\r\n",
    "   - Equation: The compression ratio achieved by Huffman coding depends on the frequency distribution of symbols in the data.\r\n",
    "\r\n",
    "### Lossy Compression Techniques:\r\n",
    "\r\n",
    "1. **Discrete Cosine Transform (DCT):**\r\n",
    "   - DCT is widely used in lossy compression techniques like JPEG and MPEG. It transforms blocks of pixels from the spatial domain to the frequency domain.\r\n",
    "   - Equation: \r\n",
    "     \\[ F(u,v) = \\frac{2}{N}C(u)C(v)\\sum_{x=0}^{N-1}\\sum_{y=0}^{N-1}f(x,y)\\cos\\left[\\frac{(2x+1)u\\pi}{2N}\\right]\\cos\\left[\\frac{(2y+1)v\\pi}{2N}\\right] \\]\r\n",
    "     Where \\( F(u,v) \\) is the DCT coefficient at frequency \\( (u,v) \\), \\( f(x,y) \\) is the pixel value at location \\( (x,y) \\), and \\( C(u) \\) and \\( C(v) \\) are normalization factors.\r\n",
    "\r\n",
    "2. **Quantization:**\r\n",
    "   - After DCT, quantization is applied to reduce the precision of DCT coefficients based on a quantization matrix.\r\n",
    "   - Equation: \\( Q_{ij} \\) represents the quantization matrix. The quantized DCT coefficient \\( F_q(u,v) \\) is calculated as \\( F_q(u,v) = \\text{round}\\left(\\frac{F(u,v)}{Q(u,v)}\\right) \\).\r\n",
    "\r\n",
    "3. **Motion Compensation (for Interframe Compression):**\r\n",
    "   - In video compression standards like MPEG, motion compensation is used to exploit temporal redundancy between consecutive frames.\r\n",
    "   - Equation: The motion vectors representing the displacement between blocks in consecutive frames are estimated using techniques like block matching.\r\n",
    "\r\n",
    "4. **Entropy Coding:**\r\n",
    "   - Entropy coding techniques like Arithmetic coding or Modified Huffman coding are used to further compress the quantized DCT coefficients or other transformed data.\r\n",
    "   - Equation: The compression ratio depends on the efficiency of the entropy coding scheme.\r\n",
    "\r\n",
    "### Overall Decompression Process:\r\n",
    "\r\n",
    "1. **Bitstream Parsing:**\r\n",
    "   - The compressed video data is parsed to extract compressed frames, motion vectors, and other necessary information.\r\n",
    "\r\n",
    "2. **Entropy Decoding:**\r\n",
    "   - Entropy-coded data is decoded using the corresponding decoding algorithm to recover the quantized DCT coefficients or other transformed data.\r\n",
    "\r\n",
    "3. **Inverse Quantization:**\r\n",
    "   - The quantized DCT coefficients are multiplied by the quantization matrix to obtain approximate DCT coefficients.\r\n",
    "\r\n",
    "4. **Inverse DCT:**\r\n",
    "   - Inverse DCT is applied to convert the approximate DCT coefficients back to spatial domain blocks.\r\n",
    "\r\n",
    "5. **Motion Compensation (for Interframe Compression):**\r\n",
    "   - Motion vectors are used to predict the current frame from previously decoded frames.\r\n",
    "\r\n",
    "6. **Frame Reconstruction:**\r\n",
    "   - Predicted frames and residual information (if any) are combined to reconstruct the original frames.\r\n",
    "\r\n",
    "Video decompression involves a combination of these techniques, tailored to the specific compression standard used (e.g., MPEG-2, H.264, H.265). The effectiveness of decompression depends on the compression ratio achieved and the quality of reconstructed frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38a70cc4-7319-4cb8-b353-57c65e775d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1: [100, 100, 100, 150, 150, 100]\n",
      "Frame 2: [50, 200, 200, 200, 100]\n",
      "Frame 3: [80, 80, 120, 120, 200]\n"
     ]
    }
   ],
   "source": [
    "class VideoDecompressor:\n",
    "    def __init__(self, compressed_data):\n",
    "        self.compressed_data = compressed_data\n",
    "        self.decompressed_frames = []\n",
    "\n",
    "    def decompress(self):\n",
    "        for compressed_frame in self.compressed_data:\n",
    "            decompressed_frame = self._rle_decode(compressed_frame)\n",
    "            self.decompressed_frames.append(decompressed_frame)\n",
    "\n",
    "    def _rle_decode(self, compressed_frame):\n",
    "        decompressed_frame = []\n",
    "        for count, pixel_value in compressed_frame:\n",
    "            decompressed_frame.extend([pixel_value] * count)\n",
    "        return decompressed_frame\n",
    "\n",
    "# Example compressed video data (RLE-encoded)\n",
    "compressed_data = [\n",
    "    [(3, 100), (2, 150), (1, 100)],\n",
    "    [(1, 50), (3, 200), (1, 100)],\n",
    "    [(2, 80), (2, 120), (1, 200)]\n",
    "]\n",
    "\n",
    "# Create a VideoDecompressor instance and decompress the video\n",
    "decompressor = VideoDecompressor(compressed_data)\n",
    "decompressor.decompress()\n",
    "\n",
    "# Print decompressed frames\n",
    "for i, frame in enumerate(decompressor.decompressed_frames):\n",
    "    print(f\"Frame {i+1}: {frame}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a73ca07-c94b-4db6-8aba-6d8eb390e148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [1, 1, 1, 2, 2, 3, 3, 3, 3]\n",
      "Encoded Data: [(3, 1), (2, 2), (4, 3)]\n",
      "Decoded Data: [1, 1, 1, 2, 2, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "def run_length_encoding(data):\n",
    "    encoded_data = []\n",
    "    count = 1\n",
    "    for i in range(1, len(data)):\n",
    "        if data[i] == data[i - 1]:\n",
    "            count += 1\n",
    "        else:\n",
    "            encoded_data.append((count, data[i - 1]))\n",
    "            count = 1\n",
    "    encoded_data.append((count, data[-1]))  # Add the last pixel sequence\n",
    "    return encoded_data\n",
    "\n",
    "def run_length_decoding(encoded_data):\n",
    "    decoded_data = []\n",
    "    for count, pixel in encoded_data:\n",
    "        decoded_data.extend([pixel] * count)\n",
    "    return decoded_data\n",
    "\n",
    "# Example usage:\n",
    "original_data = [1, 1, 1, 2, 2, 3, 3, 3, 3]\n",
    "encoded_data = run_length_encoding(original_data)\n",
    "decoded_data = run_length_decoding(encoded_data)\n",
    "\n",
    "print(\"Original Data:\", original_data)\n",
    "print(\"Encoded Data:\", encoded_data)\n",
    "print(\"Decoded Data:\", decoded_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45afd1cf-61f9-486a-9d91-f4e449d91df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def decompress_video(input_file, output_file):\n",
    "    # Open the compressed video file\n",
    "    cap = cv2.VideoCapture(input_file)\n",
    "    \n",
    "    # Check if the video file opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties (frame width, height, frame rate, etc.)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec for output video\n",
    "    out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    # Decompress each frame and write it to the output video file\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Display frame or process it\n",
    "            # (You can perform any processing here before writing it to the output file)\n",
    "            \n",
    "            # Write the frame to the output video file\n",
    "            out.write(frame)\n",
    "            \n",
    "            # Display the decompressed frame\n",
    "            cv2.imshow('Decompressed Video', frame)\n",
    "            \n",
    "            # Exit on pressing 'q' key\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Release the video objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage:\n",
    "input_file = 'video.mp4'\n",
    "output_file = 'decompressed_video.avi'\n",
    "decompress_video(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "066e28a4-da1a-47df-9718-01e2967e18ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: ABBCCCDDDDEEEEE\n",
      "Encoded Data: 010011011000000101010101111111111\n",
      "Decoded Data: ABBCCCDDDDEEEEE\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class HuffmanNode:\n",
    "    def __init__(self, symbol, frequency):\n",
    "        self.symbol = symbol\n",
    "        self.frequency = frequency\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.frequency < other.frequency\n",
    "\n",
    "def build_huffman_tree(freq_dict):\n",
    "    priority_queue = [HuffmanNode(symbol, freq) for symbol, freq in freq_dict.items()]\n",
    "    heapq.heapify(priority_queue)\n",
    "    \n",
    "    while len(priority_queue) > 1:\n",
    "        left = heapq.heappop(priority_queue)\n",
    "        right = heapq.heappop(priority_queue)\n",
    "        \n",
    "        merged = HuffmanNode(None, left.frequency + right.frequency)\n",
    "        merged.left = left\n",
    "        merged.right = right\n",
    "        \n",
    "        heapq.heappush(priority_queue, merged)\n",
    "    \n",
    "    return priority_queue[0]\n",
    "\n",
    "def build_code_table(node, code='', code_table={}):\n",
    "    if node is not None:\n",
    "        if node.symbol is not None:\n",
    "            code_table[node.symbol] = code\n",
    "        build_code_table(node.left, code + '0', code_table)\n",
    "        build_code_table(node.right, code + '1', code_table)\n",
    "    return code_table\n",
    "\n",
    "def huffman_encode(data):\n",
    "    freq_dict = Counter(data)\n",
    "    huffman_tree = build_huffman_tree(freq_dict)\n",
    "    code_table = build_code_table(huffman_tree)\n",
    "    encoded_data = ''.join(code_table[symbol] for symbol in data)\n",
    "    return encoded_data, code_table\n",
    "\n",
    "def huffman_decode(encoded_data, code_table):\n",
    "    reverse_code_table = {code: symbol for symbol, code in code_table.items()}\n",
    "    decoded_data = ''\n",
    "    code = ''\n",
    "    for bit in encoded_data:\n",
    "        code += bit\n",
    "        if code in reverse_code_table:\n",
    "            decoded_data += reverse_code_table[code]\n",
    "            code = ''\n",
    "    return decoded_data\n",
    "\n",
    "# Example usage:\n",
    "data = \"ABBCCCDDDDEEEEE\"\n",
    "encoded_data, code_table = huffman_encode(data)\n",
    "decoded_data = huffman_decode(encoded_data, code_table)\n",
    "\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Encoded Data:\", encoded_data)\n",
    "print(\"Decoded Data:\", decoded_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a81dc79-6ce3-4909-a795-d04ed5807cd6",
   "metadata": {},
   "source": [
    "# Motion Detection:\r\n",
    "\r\n",
    "Detecting and analyzing motion in video sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63513a87-6167-432d-bab1-589b320745f8",
   "metadata": {},
   "source": [
    "## Motion Detection: An Overview\r\n",
    "\r\n",
    "### Introduction\r\n",
    "Motion detection is a process of identifying a change in position of an object relative to its surroundings or the change in the surroundings relative to an object. In the context of video sequences, motion detection involves analyzing a sequence of images to determine the presence, location, and nature of moving objects within the scene. This technology is widely used in various fields such as surveillance, automotive safety, human-computer interaction, and video compression.\r\n",
    "\r\n",
    "### Key Concepts\r\n",
    "\r\n",
    "#### 1. **Frame Differencing**\r\n",
    "This is the simplest method for motion detection. It involves subtracting consecutive frames in a video sequence to detect changes.\r\n",
    "\r\n",
    "**Equation:**\r\n",
    "\\[ D(x,y,t) = |I(x,y,t) - I(x,y,t-1)| \\]\r\n",
    "where \\(D(x,y,t)\\) is the difference image, \\(I(x,y,t)\\) is the intensity of the pixel at position \\((x,y)\\) in frame \\(t\\), and \\(I(x,y,t-1)\\) is the intensity of the same pixel in the previous frame.\r\n",
    "\r\n",
    "#### 2. **Background Subtraction**\r\n",
    "This method involves creating a model of the background and then detecting moving objects as those that deviate significantly from this background.\r\n",
    "\r\n",
    "**Steps:**\r\n",
    "- **Background Modeling:** Create a background model \\(B(x,y)\\).\r\n",
    "- **Foreground Detection:** Identify moving objects by comparing the current frame \\(I(x,y,t)\\) with the background model.\r\n",
    "\r\n",
    "**Equation:**\r\n",
    "\\[ F(x,y,t) = |I(x,y,t) - B(x,y)| \\]\r\n",
    "where \\(F(x,y,t)\\) is the foreground mask.\r\n",
    "\r\n",
    "#### 3. **Optical Flow**\r\n",
    "Optical flow is a method that estimates the motion of objects based on the apparent motion of brightness patterns in the image sequence.\r\n",
    "\r\n",
    "**Equation:**\r\n",
    "\\[ I_x u + I_y v + I_t = 0 \\]\r\n",
    "where \\(I_x\\), \\(I_y\\), and \\(I_t\\) are the partial derivatives of the image intensity with respect to \\(x\\), \\(y\\), and time \\(t\\) respectively, and \\(u\\) and \\(v\\) are the components of the optical flow vector.\r\n",
    "\r\n",
    "### Techniques and Algorithms\r\n",
    "\r\n",
    "#### 1. **Lucas-Kanade Method**\r\n",
    "This is a popular optical flow algorithm that assumes that the flow is essentially constant in a local neighborhood of the pixel under consideration.\r\n",
    "\r\n",
    "**Equation:**\r\n",
    "\\[ \\sum_{i} \\sum_{j} \\left[ I_x(i,j) I_x(i,j) \\right] u + \\sum_{i} \\sum_{j} \\left[ I_x(i,j) I_y(i,j) \\right] v = - \\sum_{i} \\sum_{j} \\left[ I_x(i,j) I_t(i,j) \\right] \\]\r\n",
    "\\[ \\sum_{i} \\sum_{j} \\left[ I_x(i,j) I_y(i,j) \\right] u + \\sum_{i} \\sum_{j} \\left[ I_y(i,j) I_y(i,j) \\right] v = - \\sum_{i} \\sum_{j} \\left[ I_y(i,j) I_t(i,j) \\right] \\]\r\n",
    "\r\n",
    "#### 2. **Gaussian Mixture Model (GMM)**\r\n",
    "This is used for background subtraction, where the background is modeled using a mixture of Gaussians.\r\n",
    "\r\n",
    "**Equation:**\r\n",
    "\\[ P(x) = \\sum_{i=1}^{K} \\omega_i \\cdot \\eta(x; \\mu_i, \\Sigma_i) \\]\r\n",
    "where \\(P(x)\\) is the probability of pixel value \\(x\\), \\(\\omega_i\\) is the weight, \\(\\mu_i\\) is the mean, \\(\\Sigma_i\\) is the covariance matrix of the \\(i\\)-th Gaussian component, and \\(\\eta\\) is the Gaussian probability density function.\r\n",
    "\r\n",
    "### Applications\r\n",
    "\r\n",
    "1. **Surveillance Systems:** Detecting intruders, monitoring activity, and tracking objects.\r\n",
    "2. **Autonomous Vehicles:** Detecting pedestrians, other vehicles, and obstacles.\r\n",
    "3. **Human-Computer Interaction:** Gesture recognition and activity monitoring.\r\n",
    "4. **Video Compression:** Identifying regions of interest for efficient encoding.\r\n",
    "\r\n",
    "### Challenges\r\n",
    "\r\n",
    "1. **Illumination Changes:** Variations in lighting can affect detection accuracy.\r\n",
    "2. **Complex Backgrounds:** Dynamic backgrounds can cause false detections.\r\n",
    "3. **Object Occlusion:** Objects that are partially or fully occluded can be challenging to detect and track.\r\n",
    "4. **Real-Time Processing:** High computational demands for processing video in real-time.\r\n",
    "\r\n",
    "### Advanced Techniques\r\n",
    "\r\n",
    "#### 1. **Deep Learning**\r\n",
    "Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) are used for more robust motion detection and tracking by learning spatial and temporal features directly from data.\r\n",
    "\r\n",
    "#### 2. **Motion History Images (MHI)**\r\n",
    "MHIs are a way of encoding motion information into a single image by accumulating binary motion images over time.\r\n",
    "\r\n",
    "**Equation:**\r\n",
    "\\[ H(x,y,t) = \r\n",
    "\\begin{cases} \r\n",
    "\\tau & \\text{if } D(x,y,t) = 1 \\\\\r\n",
    "\\max(0, H(x,y,t-1) - 1) & \\text{otherwise}\r\n",
    "\\end{cases} \\]\r\n",
    "where \\(H(x,y,t)\\) is the MHI and \\(\\tau\\) is the maximum duration that a motion is recorded.\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "Motion detection is a crucial aspect of video analysis, enabling numerous applications from security to automation. The choice of technique depends on the specific requirements and constraints of the application, with each method offering different strengths and addressing various challenges. Understanding the underlying principles and equations allows for the effective implementation and improvement of motion detection systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189b4a87-b5bb-4446-af00-f01fdb9d4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open the video capture\n",
    "cap = cv2.VideoCapture('video.mp4')\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Read the first frame\n",
    "ret, prev_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error: Could not read video frame.\")\n",
    "    exit()\n",
    "\n",
    "# Convert the frame to grayscale\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "while True:\n",
    "    # Read the next frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Compute the absolute difference between current frame and previous frame\n",
    "    diff = cv2.absdiff(prev_gray, gray)\n",
    "\n",
    "    # Apply a binary threshold to get a binary image\n",
    "    _, thresh = cv2.threshold(diff, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Display the results\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.imshow('Motion Detection', thresh)\n",
    "\n",
    "    # Update the previous frame\n",
    "    prev_gray = gray\n",
    "\n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5223f4a2-ee2b-4c8f-b891-c4e961741a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open the video capture\n",
    "cap = cv2.VideoCapture('video.mp4')\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Create the background subtractor\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "while True:\n",
    "    # Read the next frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply the background subtractor to get the foreground mask\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    # Display the results\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.imshow('Foreground Mask', fgmask)\n",
    "\n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a02965-4e59-4223-b199-0118490763cb",
   "metadata": {},
   "source": [
    "# Video Stabilization:\r\n",
    "\r\n",
    "Reducing shakiness or jitter in videos caused by camera motion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b6fbd8-4e6e-463d-a807-ff203ce161e8",
   "metadata": {},
   "source": [
    "Video stabilization is a technique used to reduce the effects of camera motion, such as shaking or jitter, in videos. It aims to produce smoother and more visually pleasing footage by compensating for undesired motion. Here's a comprehensive overview of video stabilization, including the methods, techniques, and equations involved:\r\n",
    "\r\n",
    "### 1. Types of Camera Motion:\r\n",
    "   - **Global Motion**: Entire frame moves due to camera panning, tilting, or zooming.\r\n",
    "   - **Local Motion**: Relative motion within the frame caused by shaking or vibrations.\r\n",
    "\r\n",
    "### 2. Techniques for Video Stabilization:\r\n",
    "\r\n",
    "#### 2.1. Global Motion Compensation:\r\n",
    "   - **Translation Model**: Estimates global motion parameters (translation vectors) to compensate for panning or tilting.\r\n",
    "   - **Homography Model**: Computes a homography matrix to handle more complex global transformations like zooming or rotation.\r\n",
    "\r\n",
    "#### 2.2. Local Motion Compensation:\r\n",
    "   - **Feature-Based Methods**: Tracks keypoints or feature points across frames and computes transformations to align them.\r\n",
    "   - **Block-Based Methods**: Divides frames into blocks and estimates motion vectors for each block to stabilize.\r\n",
    "\r\n",
    "### 3. Equations:\r\n",
    "\r\n",
    "#### 3.1. Global Motion Compensation:\r\n",
    "   - **Translation Model**: \r\n",
    "     - \\(x' = x + \\Delta x\\)\r\n",
    "     - \\(y' = y + \\Delta y\\)\r\n",
    "   - **Homography Model**: \r\n",
    "     - \\(s \\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = H \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\)\r\n",
    "     - \\(H = K[R|t] = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\end{bmatrix}\\)\r\n",
    "\r\n",
    "#### 3.2. Local Motion Compensation:\r\n",
    "   - **Optical Flow**: \r\n",
    "     - \\(I_x u + I_y v + I_t = 0\\)\r\n",
    "     - Where \\(I_x\\), \\(I_y\\) are partial derivatives of intensity with respect to x and y, and \\(I_t\\) is the derivative with respect to time.\r\n",
    "   - **Lucas-Kanade Algorithm**:\r\n",
    "     - \\(A^T A x = A^T b\\)\r\n",
    "     - Where \\(A\\) is the Jacobian matrix of image gradients, \\(x\\) is the motion vector, and \\(b\\) is the difference between pixel values in consecutive frames.\r\n",
    "\r\n",
    "### 4. Common Challenges:\r\n",
    "   - **Noise**: Stabilization algorithms may amplify noise, leading to jittery or unnatural-looking videos.\r\n",
    "   - **Computational Complexity**: Some methods require significant computational resources, making real-time stabilization challenging.\r\n",
    "   - **Ghosting Artifacts**: Improper motion estimation can introduce ghosting or smearing artifacts in stabilized videos.\r\n",
    "\r\n",
    "### 5. Practical Implementation:\r\n",
    "   - **Pre-processing**: Convert videos to appropriate formats, resize for efficiency, and apply noise reduction if necessary.\r\n",
    "   - **Motion Estimation**: Choose suitable algorithms based on the type and intensity of motion.\r\n",
    "   - **Filtering**: Apply filters or smoothing techniques to reduce jitter while preserving scene details.\r\n",
    "   - **Post-processing**: Adjust parameters, crop borders, and apply additional effects for better aesthetics.\r\n",
    "\r\n",
    "### 6. Available Libraries and Tools:\r\n",
    "   - **OpenCV**: Provides a comprehensive set of functions and algorithms for video stabilization, including global and local motion compensation.\r\n",
    "   - **FFmpeg**: Offers video stabilization filters and command-line tools for batch processing and integration into video pipelines.\r\n",
    "   - **Adobe Premiere Pro, Final Cut Pro, etc.**: Commercial video editing software often includes built-in stabilization features with user-friendly interfaces.\r\n",
    "\r\n",
    "Video stabilization is an essential tool in modern video production, enabling filmmakers, content creators, and researchers to produce high-quality, professional-looking videos even under challenging conditions. It combines mathematical principles, computer vision techniques, and signal processing algorithms to achieve smooth and stable footage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25372c82-c93d-4b2c-9f42-c02ab4505737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def stabilize_video(input_video_path, output_video_path):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    # Check if the video file is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return\n",
    "    \n",
    "    # Get the first frame\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read the first frame.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize variables for motion estimation\n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    prev_pts = cv2.goodFeaturesToTrack(prev_gray, maxCorners=200, qualityLevel=0.01, minDistance=30, blockSize=3)\n",
    "    prev_pts = np.float32(prev_pts).reshape(-1, 1, 2)\n",
    "    \n",
    "    # Create a VideoWriter object to save the stabilized video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n",
    "    \n",
    "    # Process each frame\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert the current frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Perform optical flow to estimate motion between frames\n",
    "        curr_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, gray, prev_pts, None)\n",
    "        \n",
    "        # Filter out points with low status\n",
    "        good_new = curr_pts[status == 1]\n",
    "        good_old = prev_pts[status == 1]\n",
    "        \n",
    "        # Compute the transformation matrix using RANSAC\n",
    "        M, _ = cv2.estimateAffinePartial2D(good_old, good_new)\n",
    "        \n",
    "        # Apply the transformation to stabilize the frame\n",
    "        stabilized_frame = cv2.warpAffine(frame, M, (frame.shape[1], frame.shape[0]))\n",
    "        \n",
    "        # Write the stabilized frame to the output video\n",
    "        out.write(stabilized_frame)\n",
    "        \n",
    "        # Update the variables for the next iteration\n",
    "        prev_gray = gray.copy()\n",
    "        prev_pts = good_new.reshape(-1, 1, 2)\n",
    "    \n",
    "    # Release VideoCapture and VideoWriter objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "input_video_path = 'video.mp4'\n",
    "output_video_path = 'output_stabilized_video.avi'\n",
    "stabilize_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e42641e-82ac-434b-ae1c-22491a6bf4d5",
   "metadata": {},
   "source": [
    "# Video Enhancement:\r\n",
    "\r\n",
    "Enhancing video quality through techniques like denoising, deblurring, and contrast adjustment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a97abe-b5ed-4a3b-a914-7fed8bcb5026",
   "metadata": {},
   "source": [
    "Video enhancement refers to the process of improving the quality of a video by applying various techniques to enhance its visual appearance. This typically involves reducing noise, removing blur, adjusting contrast, and enhancing details. Below, I'll cover the main techniques used in video enhancement, along with equations where applicable:\r\n",
    "\r\n",
    "### 1. Denoising:\r\n",
    "   - **Objective**: Reduce noise in the video caused by low light conditions, high ISO settings, or electronic interference.\r\n",
    "   - **Techniques**: \r\n",
    "     - **Temporal Filtering**: Use temporal information across consecutive frames to distinguish noise from actual content.\r\n",
    "     - **Spatial Filtering**: Apply\r\n",
    "\r\n",
    "spatial filters such as Gaussian, median, or bilateral filtering to remove noise while preserving image details.\r\n",
    "\r\n",
    "   - **Equations**:\r\n",
    "     - **Gaussian Filter**:\r\n",
    "       \\[ G(x, y) = \\frac{1}{2\\pi\\sigma^2} \\exp{\\left(-\\frac{x^2 + y^2}{2\\sigma^2}\\right)} \\]\r\n",
    "     - **Median Filter**:\r\n",
    "       \\[ I_{\\text{med}}(x, y) = \\text{median}(I(x + i, y + j)) \\quad \\text{for} \\quad i, j \\in \\{-k, ..., k\\} \\]\r\n",
    "\r\n",
    "### 2. Deblurring:\r\n",
    "   - **Objective**: Remove blur caused by camera motion or out-of-focus conditions.\r\n",
    "   - **Techniques**: \r\n",
    "     - **Wiener Filtering**: Utilize a mathematical model to estimate the blur kernel and deconvolve it from the image.\r\n",
    "     - **Lucy-Richardson Deconvolution**: Iterate between the blurred image and its estimate to refine the deblurred result.\r\n",
    "\r\n",
    "### 3. Contrast Adjustment:\r\n",
    "   - **Objective**: Enhance the contrast of the video to improve visibility and make it more visually appealing.\r\n",
    "   - **Techniques**:\r\n",
    "     - **Histogram Equalization**: Spread out the intensity values across the histogram to utilize the full dynamic range.\r\n",
    "     - **Contrast Stretching**: Linearly expand the intensity range between the minimum and maximum values.\r\n",
    "\r\n",
    "### 4. Detail Enhancement:\r\n",
    "   - **Objective**: Enhance fine details and textures in the video to make it appear sharper.\r\n",
    "   - **Techniques**:\r\n",
    "     - **Unsharp Masking**: Subtract a blurred version of the image from the original to enhance edges and details.\r\n",
    "     - **High-Pass Filtering**: Accentuate high-frequency components using filters like Laplacian or Sobel.\r\n",
    "\r\n",
    "### 5. Color Correction:\r\n",
    "   - **Objective**: Adjust the color balance and tone of the video to achieve a desired look or correct for lighting conditions.\r\n",
    "   - **Techniques**:\r\n",
    "     - **White Balance Adjustment**: Scale the intensity of color channels to remove color casts.\r\n",
    "     - **Color Grading**: Apply creative color adjustments to enhance the overall aesthetic of the video.\r\n",
    "\r\n",
    "### 6. Video Super-Resolution:\r\n",
    "   - **Objective**: Increase the spatial resolution of the video to improve clarity and sharpness.\r\n",
    "   - **Techniques**:\r\n",
    "     - **Single-Image Super-Resolution**: Use deep learning models to infer high-resolution details from low-resolution frames.\r\n",
    "     - **Optical Flow-based Methods**: Utilize motion information across frames to enhance spatial resolution.\r\n",
    "\r\n",
    "### 7. Artifact Removal:\r\n",
    "   - **Objective**: Remove visual artifacts such as compression artifacts, flickering, or banding.\r\n",
    "   - **Techniques**:\r\n",
    "     - **Temporal Filtering**: Apply temporal averaging or interpolation to smooth out artifacts over time.\r\n",
    "     - **Frequency Filtering**: Use frequency-domain techniques like Fourier or wavelet transforms to identify and remove specific artifacts.\r\n",
    "\r\n",
    "### 8. Video Fusion:\r\n",
    "   - **Objective**: Combine multiple video streams or frames to generate a high-quality output.\r\n",
    "   - **Techniques**:\r\n",
    "     - **Multi-Exposure Fusion**: Blend differently exposed frames to create an image with balanced brightness and contrast.\r\n",
    "     - **Multi-Frame Super-Resolution**: Combine information from multiple frames to enhance spatial resolution and reduce noise.\r\n",
    "\r\n",
    "### 9. Motion Compensation:\r\n",
    "   - **Objective**: Compensate for motion in the video to stabilize or align frames.\r\n",
    "   - **Techniques**:\r\n",
    "     - **Optical Flow-based Stabilization**: Estimate motion vectors between frames and warp or interpolate frames to align them.\r\n",
    "     - **Global Motion Estimation**: Model and compensate for large-scale motion such as camera panning or rotation.\r\n",
    "\r\n",
    "### Conclusion:\r\n",
    "Video enhancement techniques play a crucial role in improving the quality and visual appeal of videos across various applications, including surveillance, entertainment, and scientific imaging. By applying a combination of denoising, deblurring, contrast adjustment, detail enhancement, and other techniques, it's possible to significantly enhance the clarity, sharpness, and overall quality of video content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc2b630-3962-4393-b14a-c2dede3dd2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def enhance_video(input_video_path, output_video_path):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    # Check if the video file is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return\n",
    "    \n",
    "    # Create a VideoWriter object to save the enhanced video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, frame_size)\n",
    "    \n",
    "    # Process each frame\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Apply denoising using bilateral filter\n",
    "        denoised_frame = cv2.bilateralFilter(frame, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "        \n",
    "        # Apply contrast adjustment using histogram equalization\n",
    "        lab_frame = cv2.cvtColor(denoised_frame, cv2.COLOR_BGR2LAB)\n",
    "        l_channel, a_channel, b_channel = cv2.split(lab_frame)\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "        enhanced_l_channel = clahe.apply(l_channel)\n",
    "        enhanced_lab_frame = cv2.merge([enhanced_l_channel, a_channel, b_channel])\n",
    "        enhanced_frame = cv2.cvtColor(enhanced_lab_frame, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        # Write the enhanced frame to the output video\n",
    "        out.write(enhanced_frame)\n",
    "    \n",
    "    # Release VideoCapture and VideoWriter objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "input_video_path = 'video.mp4'\n",
    "output_video_path = 'output_enhanced_video.avi'\n",
    "enhance_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b64a8-ce23-460a-83ed-8101d5830c5b",
   "metadata": {},
   "source": [
    "# Video Segmentation:\r\n",
    "\r\n",
    "Partitioning a video into segments based on object boundaries or motion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0315aa2f-f660-4d6d-915a-c3d827930099",
   "metadata": {},
   "source": [
    "Video segmentation involves partitioning a video into segments, either by identifying object boundaries or by tracking motion. This process is crucial for various applications, such as object detection, activity recognition, video editing, and autonomous driving.\r\n",
    "\r\n",
    "### Types of Video Segmentation\r\n",
    "\r\n",
    "1. **Temporal Segmentation**:\r\n",
    "   - Divides a video into distinct temporal segments, often corresponding to different scenes or events.\r\n",
    "   - Techniques include shot boundary detection and scene change detection.\r\n",
    "\r\n",
    "2. **Spatial Segmentation**:\r\n",
    "   - Segments individual frames into regions based on spatial characteristics like color, texture, and edges.\r\n",
    "   - Often used as a precursor to object recognition.\r\n",
    "\r\n",
    "3. **Spatio-Temporal Segmentation**:\r\n",
    "   - Considers both spatial and temporal dimensions to segment moving objects across multiple frames.\r\n",
    "   - Combines motion information with spatial features to improve segmentation accuracy.\r\n",
    "\r\n",
    "### Methods of Video Segmentation\r\n",
    "\r\n",
    "#### 1. Manual Annotation\r\n",
    "   - Human annotators label the objects or regions in each frame.\r\n",
    "   - Extremely accurate but time-consuming and impractical for large datasets.\r\n",
    "\r\n",
    "#### 2. Semi-Automatic Methods\r\n",
    "   - Combine manual input with automated algorithms.\r\n",
    "   - Human operators correct or refine segments generated by algorithms.\r\n",
    "\r\n",
    "#### 3. Fully Automatic Methods\r\n",
    "   - Rely on algorithms to segment videos without human intervention.\r\n",
    "   - Include classical methods (e.g., clustering, thresholding) and modern methods (e.g., deep learning).\r\n",
    "\r\n",
    "### Techniques and Algorithms\r\n",
    "\r\n",
    "1. **Thresholding**:\r\n",
    "   - Simple method based on pixel intensity.\r\n",
    "   - \\( \\text{Binary Threshold} : I(x,y) > T \\) where \\( I(x,y) \\) is the pixel intensity and \\( T \\) is the threshold.\r\n",
    "\r\n",
    "2. **Clustering**:\r\n",
    "   - Groups similar pixels together.\r\n",
    "   - K-means clustering is commonly used, where \\( \\mu_i \\) are the centroids:\r\n",
    "     \\[ J = \\sum_{i=1}^{k} \\sum_{x_j \\in S_i} \\| x_j - \\mu_i \\|^2 \\]\r\n",
    "\r\n",
    "3. **Graph-Based Methods**:\r\n",
    "   - Represent pixels as nodes and edges represent similarity.\r\n",
    "   - Normalized cuts minimize the disassociation between segments:\r\n",
    "     \\[ \\text{Ncut}(A,B) = \\frac{\\text{cut}(A,B)}{\\text{assoc}(A,V)} + \\frac{\\text{cut}(A,B)}{\\text{assoc}(B,V)} \\]\r\n",
    "\r\n",
    "4. **Optical Flow**:\r\n",
    "   - Estimates motion between consecutive frames.\r\n",
    "   - Horn-Schunck method uses the brightness constancy constraint:\r\n",
    "     \\[ I_x u + I_y v + I_t = 0 \\]\r\n",
    "     where \\( I_x, I_y, I_t \\) are image gradients, and \\( u, v \\) are the flow vectors.\r\n",
    "\r\n",
    "5. **Superpixel Segmentation**:\r\n",
    "   - Groups pixels into perceptually meaningful atomic regions.\r\n",
    "   - Simple Linear Iterative Clustering (SLIC) superpixels minimize the distance metric \\( D \\):\r\n",
    "     \\[ D = \\sqrt{ \\frac{d_c^2}{N_c^2} + \\frac{d_s^2}{N_s^2} } \\]\r\n",
    "     where \\( d_c \\) is color distance and \\( d_s \\) is spatial distance.\r\n",
    "\r\n",
    "6. **Deep Learning Methods**:\r\n",
    "   - Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) for feature extraction and temporal coherence.\r\n",
    "   - Fully Convolutional Networks (FCNs) for pixel-wise classification.\r\n",
    "   - Example architectures: Mask R-CNN, U-Net, SegNet.\r\n",
    "\r\n",
    "### Applications of Video Segmentation\r\n",
    "\r\n",
    "1. **Object Tracking**:\r\n",
    "   - Track objects across frames for surveillance and autonomous driving.\r\n",
    "\r\n",
    "2. **Video Editing**:\r\n",
    "   - Automatic editing tools for content creation.\r\n",
    "\r\n",
    "3. **Activity Recognition**:\r\n",
    "   - Identify human activities by analyzing segmented motion patterns.\r\n",
    "\r\n",
    "4. **Medical Imaging**:\r\n",
    "   - Segment anatomical structures in medical videos for diagnosis and treatment planning.\r\n",
    "\r\n",
    "### Challenges\r\n",
    "\r\n",
    "- **Complex Motion**: Handling complex and non-rigid motion patterns.\r\n",
    "- **Occlusion**: Dealing with objects that partially or fully occlude each other.\r\n",
    "- **Real-Time Processing**: Achieving real-time performance for applications like autonomous driving.\r\n",
    "- **Generalization**: Ensuring models work well across diverse video types and environments.\r\n",
    "\r\n",
    "### Evaluation Metrics\r\n",
    "\r\n",
    "- **Intersection over Union (IoU)**:\r\n",
    "  \\[ \\text{IoU} = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}} \\]\r\n",
    "\r\n",
    "- **Precision and Recall**:\r\n",
    "  \\[ \\text{Precision} = \\frac{TP}{TP + FP} \\]\r\n",
    "  \\[ \\text{Recall} = \\frac{TP}{TP + FN} \\]\r\n",
    "\r\n",
    "- **Boundary F1 Score**:\r\n",
    "  - Measures the accuracy of the predicted boundary against the ground truth.\r\n",
    "\r\n",
    "In summary, video segmentation is a multifaceted problem involving temporal, spatial, and spatio-temporal aspects. It leverages various algorithms ranging from simple thresholding to advanced deep learning techniques to partition videos into meaningful segments for further analysis and application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "468fa0e2-cd22-4e7b-97af-5dfa48bdf2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to C:\\Users\\varun/.cache\\torch\\hub\\checkpoints\\maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 170M/170M [05:11<00:00, 571kB/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# Load a pretrained segmentation model\n",
    "model = models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the video\n",
    "video_path = 'video.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    input_frame = transform(frame).unsqueeze(0)\n",
    "\n",
    "    # Perform segmentation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_frame)\n",
    "\n",
    "    # Post-process and visualize the segmentation results\n",
    "    # (This part depends on the model's output format and desired visualization)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b176bd4f-78ec-4fcc-ba81-d1e90a9e7910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected shot boundaries at frames: []\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def calculate_histogram(frame):\n",
    "    \"\"\"\n",
    "    Calculate the histogram of a frame.\n",
    "    \"\"\"\n",
    "    hist = cv2.calcHist([frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()\n",
    "\n",
    "def detect_shot_boundaries(video_path, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detects shot boundaries in a video using histogram differences.\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        threshold (float): Threshold for detecting shot boundaries.\n",
    "    Returns:\n",
    "        List of frame indices where shot boundaries occur.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, prev_frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Failed to read video\")\n",
    "        return []\n",
    "    \n",
    "    prev_hist = calculate_histogram(prev_frame)\n",
    "    frame_idx = 0\n",
    "    shot_boundaries = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        curr_hist = calculate_histogram(frame)\n",
    "        hist_diff = cv2.compareHist(prev_hist, curr_hist, cv2.HISTCMP_BHATTACHARYYA)\n",
    "        \n",
    "        if hist_diff > threshold:\n",
    "            shot_boundaries.append(frame_idx)\n",
    "        \n",
    "        prev_hist = curr_hist\n",
    "        frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return shot_boundaries\n",
    "\n",
    "# Example usage\n",
    "video_path = 'video.mp4'\n",
    "shot_boundaries = detect_shot_boundaries(video_path, threshold=0.5)\n",
    "print(\"Detected shot boundaries at frames:\", shot_boundaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24445ca-53ca-4384-8358-210b7f9cfba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected shot boundaries at frames: []\n"
     ]
    }
   ],
   "source": [
    "def calculate_edge_change_ratio(prev_frame, curr_frame):\n",
    "    \"\"\"\n",
    "    Calculate the Edge Change Ratio (ECR) between two frames.\n",
    "    \"\"\"\n",
    "    prev_edges = cv2.Canny(prev_frame, 100, 200)\n",
    "    curr_edges = cv2.Canny(curr_frame, 100, 200)\n",
    "    \n",
    "    diff = cv2.absdiff(prev_edges, curr_edges)\n",
    "    non_zero_diff = np.count_nonzero(diff)\n",
    "    \n",
    "    ecr = non_zero_diff / (prev_frame.shape[0] * prev_frame.shape[1])\n",
    "    return ecr\n",
    "\n",
    "def detect_shot_boundaries_with_ecr(video_path, hist_threshold=0.5, ecr_threshold=0.02):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, prev_frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Failed to read video\")\n",
    "        return []\n",
    "    \n",
    "    prev_hist = calculate_histogram(prev_frame)\n",
    "    frame_idx = 0\n",
    "    shot_boundaries = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        curr_hist = calculate_histogram(frame)\n",
    "        hist_diff = cv2.compareHist(prev_hist, curr_hist, cv2.HISTCMP_BHATTACHARYYA)\n",
    "        ecr = calculate_edge_change_ratio(prev_frame, frame)\n",
    "        \n",
    "        if hist_diff > hist_threshold and ecr > ecr_threshold:\n",
    "            shot_boundaries.append(frame_idx)\n",
    "        \n",
    "        prev_hist = curr_hist\n",
    "        prev_frame = frame\n",
    "        frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return shot_boundaries\n",
    "\n",
    "# Example usage\n",
    "video_path = 'video.mp4'\n",
    "shot_boundaries = detect_shot_boundaries_with_ecr(video_path, hist_threshold=0.5, ecr_threshold=0.02)\n",
    "print(\"Detected shot boundaries at frames:\", shot_boundaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd3f25d-3294-44f1-8634-9bef8ecd0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.segmentation import slic, mark_boundaries\n",
    "from skimage.color import rgb2lab\n",
    "\n",
    "def segment_image_slic(image, n_segments=400, compactness=10):\n",
    "    \"\"\"\n",
    "    Segments an image using the SLIC superpixel algorithm.\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image.\n",
    "        n_segments (int): The approximate number of superpixels.\n",
    "        compactness (float): Balances color proximity and space proximity.\n",
    "    Returns:\n",
    "        segments (numpy.ndarray): The segmented image.\n",
    "    \"\"\"\n",
    "    lab_image = rgb2lab(image)\n",
    "    segments = slic(lab_image, n_segments=n_segments, compactness=compactness)\n",
    "    return segments\n",
    "\n",
    "def apply_edge_detection(image):\n",
    "    \"\"\"\n",
    "    Applies edge detection on the image.\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image.\n",
    "    Returns:\n",
    "        edges (numpy.ndarray): Image with edges detected.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    return edges\n",
    "\n",
    "def draw_contours(image, segments):\n",
    "    \"\"\"\n",
    "    Draws contours on the segmented image.\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image.\n",
    "        segments (numpy.ndarray): Segmented image.\n",
    "    Returns:\n",
    "        image_with_contours (numpy.ndarray): Image with contours drawn.\n",
    "    \"\"\"\n",
    "    contours, _ = cv2.findContours(segments, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    image_with_contours = image.copy()\n",
    "    cv2.drawContours(image_with_contours, contours, -1, (0, 255, 0), 2)\n",
    "    return image_with_contours\n",
    "\n",
    "# Example usage\n",
    "image_path = 'My.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "segments = segment_image_slic(image, n_segments=400, compactness=10)\n",
    "\n",
    "# Convert segments to a format suitable for drawing contours\n",
    "segments_uint8 = (segments * (255 / segments.max())).astype(np.uint8)\n",
    "edges = apply_edge_detection(image)\n",
    "image_with_contours = draw_contours(image, edges)\n",
    "\n",
    "# Display the results\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Superpixels', mark_boundaries(image, segments))\n",
    "cv2.imshow('Edges', edges)\n",
    "cv2.imshow('Contours', image_with_contours)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7960381-1cd8-4641-b058-4046dbc62cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.segmentation import slic, mark_boundaries\n",
    "from skimage.color import rgb2lab\n",
    "\n",
    "def segment_image_slic(frame, n_segments=400, compactness=10):\n",
    "    \"\"\"\n",
    "    Segments an image using the SLIC superpixel algorithm.\n",
    "    Args:\n",
    "        frame (numpy.ndarray): Input frame.\n",
    "        n_segments (int): The approximate number of superpixels.\n",
    "        compactness (float): Balances color proximity and space proximity.\n",
    "    Returns:\n",
    "        segments (numpy.ndarray): The segmented frame.\n",
    "    \"\"\"\n",
    "    lab_frame = rgb2lab(frame)\n",
    "    segments = slic(lab_frame, n_segments=n_segments, compactness=compactness)\n",
    "    return segments\n",
    "\n",
    "def apply_background_subtraction(frame, back_subtractor):\n",
    "    \"\"\"\n",
    "    Applies background subtraction to detect motion.\n",
    "    Args:\n",
    "        frame (numpy.ndarray): Input frame.\n",
    "        back_subtractor (cv2.BackgroundSubtractor): Background subtractor object.\n",
    "    Returns:\n",
    "        fg_mask (numpy.ndarray): Foreground mask.\n",
    "    \"\"\"\n",
    "    fg_mask = back_subtractor.apply(frame)\n",
    "    return fg_mask\n",
    "\n",
    "def combine_segmentation_and_motion(segments, fg_mask):\n",
    "    \"\"\"\n",
    "    Combines spatial segmentation with motion detection.\n",
    "    Args:\n",
    "        segments (numpy.ndarray): Segmented frame.\n",
    "        fg_mask (numpy.ndarray): Foreground mask.\n",
    "    Returns:\n",
    "        combined_mask (numpy.ndarray): Combined segmentation mask.\n",
    "    \"\"\"\n",
    "    combined_mask = np.zeros_like(fg_mask)\n",
    "    for segment_val in np.unique(segments):\n",
    "        segment_mask = (segments == segment_val).astype(np.uint8)\n",
    "        if np.sum(fg_mask[segment_mask == 1]) > 0:\n",
    "            combined_mask[segment_mask == 1] = 255\n",
    "    return combined_mask\n",
    "\n",
    "# Example usage\n",
    "video_path = 'video.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "back_subtractor = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=16, detectShadows=True)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Apply SLIC superpixel segmentation\n",
    "    segments = segment_image_slic(frame, n_segments=400, compactness=10)\n",
    "    \n",
    "    # Apply background subtraction\n",
    "    fg_mask = apply_background_subtraction(frame, back_subtractor)\n",
    "    \n",
    "    # Combine segmentation with motion detection\n",
    "    combined_mask = combine_segmentation_and_motion(segments, fg_mask)\n",
    "    \n",
    "    # Visualize the results\n",
    "    frame_with_boundaries = mark_boundaries(frame, segments)\n",
    "    combined_visual = cv2.bitwise_and(frame, frame, mask=combined_mask)\n",
    "    \n",
    "    cv2.imshow('Original Frame', frame)\n",
    "    cv2.imshow('Superpixels', frame_with_boundaries)\n",
    "    cv2.imshow('Foreground Mask', fg_mask)\n",
    "    cv2.imshow('Combined Segmentation', combined_visual)\n",
    "    \n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbbdee2-830c-4c75-8cce-561fe24be9a3",
   "metadata": {},
   "source": [
    "# Frame Interpolation:\r\n",
    "\r\n",
    "Generating intermediate frames between existing frames to smoothen motion or increase frame rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b1612-f1e5-4560-86ae-d061669c60ab",
   "metadata": {},
   "source": [
    "Frame interpolation is a technique used in video processing to generate intermediate frames between existing frames, with the goal of smoothening motion or increasing the frame rate of a video. This can be particularly useful in scenarios such as converting videos from lower frame rates (e.g., 24fps) to higher frame rates (e.g., 60fps), or in applications like slow-motion video where additional frames are needed to fill in the gaps between original frames.\r\n",
    "\r\n",
    "There are several methods for frame interpolation, ranging from simple linear blending to more sophisticated algorithms involving motion estimation and compensation. One common approach is optical flow-based interpolation, which estimates the motion between adjacent frames and generates intermediate frames based on this motion information.\r\n",
    "\r\n",
    "The basic equation for optical flow estimation is:\r\n",
    "\r\n",
    "\\[I_x \\cdot u + I_y \\cdot v + I_t = 0\\]\r\n",
    "\r\n",
    "Where:\r\n",
    "- \\(I_x\\), \\(I_y\\): Spatial gradients of the image intensity in the x and y directions.\r\n",
    "- \\(u\\), \\(v\\): Horizontal and vertical components of the optical flow vector.\r\n",
    "- \\(I_t\\): Temporal gradient of the image intensity.\r\n",
    "\r\n",
    "Optical flow algorithms solve this equation to estimate the motion vectors (\\(u\\), \\(v\\)) between consecutive frames. Once the motion vectors are obtained, intermediate frames can be generated by warping pixels from one frame to another based on the estimated motion.\r\n",
    "\r\n",
    "One common method for generating intermediate frames is by using a weighted average of neighboring frames. For example, given two frames \\(I_1\\) and \\(I_2\\) with motion vectors \\(u\\) and \\(v\\), the intermediate frame \\(I_{\\text{interpolated}}\\) at time \\(t\\) can be computed as:\r\n",
    "\r\n",
    "\\[I_{\\text{interpolated}}(x, y, t) = (1 - \\alpha) \\cdot I_1(x - u \\cdot t, y - v \\cdot t) + \\alpha \\cdot I_2(x + u \\cdot t, y + v \\cdot t)\\]\r\n",
    "\r\n",
    "Where:\r\n",
    "- \\(x\\), \\(y\\): Spatial coordinates.\r\n",
    "- \\(\\alpha\\): Interpolation factor, typically ranging from 0 to 1, representing the proportion of contribution from each neighboring frame.\r\n",
    "\r\n",
    "This equation blends pixels from the two neighboring frames based on their estimated motion vectors.\r\n",
    "\r\n",
    "More advanced interpolation techniques may involve temporal filtering, motion-compensated prediction, or deep learning-based approaches for better quality and accuracy.\r\n",
    "\r\n",
    "Overall, frame interpolation is a powerful tool for enhancing video quality, improving motion smoothness, and enabling various video processing applications. However, it's important to consider computational complexity and potential artifacts introduced by the interpolation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4dd26c6-b763-4098-9b7b-7a6661813643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def interpolate_frames(frame1, frame2, flow, alpha):\n",
    "    \"\"\"\n",
    "    Interpolate frames using linear blending based on optical flow.\n",
    "\n",
    "    Args:\n",
    "    - frame1: First frame (numpy array).\n",
    "    - frame2: Second frame (numpy array).\n",
    "    - flow: Optical flow vectors (numpy array of shape [height, width, 2]).\n",
    "    - alpha: Interpolation factor (float).\n",
    "\n",
    "    Returns:\n",
    "    - interpolated_frame: Interpolated frame (numpy array).\n",
    "    \"\"\"\n",
    "    # Get dimensions of frames\n",
    "    height, width = frame1.shape[:2]\n",
    "\n",
    "    # Initialize interpolated frame\n",
    "    interpolated_frame = np.zeros_like(frame1)\n",
    "\n",
    "    # Generate intermediate frame by blending pixels from frame1 and frame2 based on flow\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            # Compute corresponding coordinates in frame2 using flow\n",
    "            new_x = int(x + flow[y, x, 0])\n",
    "            new_y = int(y + flow[y, x, 1])\n",
    "\n",
    "            # Ensure the coordinates are within the frame boundaries\n",
    "            if 0 <= new_x < width and 0 <= new_y < height:\n",
    "                # Linear blending based on interpolation factor\n",
    "                interpolated_frame[y, x] = (1 - alpha) * frame1[y, x] + alpha * frame2[new_y, new_x]\n",
    "            else:\n",
    "                # If the coordinates are out of bounds, use only frame1\n",
    "                interpolated_frame[y, x] = frame1[y, x]\n",
    "\n",
    "    return interpolated_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2512a06c-ed8e-4d9d-8815-1e55ff58407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def compute_optical_flow(frame1, frame2):\n",
    "    \"\"\"\n",
    "    Compute optical flow using Lucas-Kanade method.\n",
    "\n",
    "    Args:\n",
    "    - frame1: First frame (numpy array).\n",
    "    - frame2: Second frame (numpy array).\n",
    "\n",
    "    Returns:\n",
    "    - flow: Optical flow vectors (numpy array of shape [height, width, 2]).\n",
    "    \"\"\"\n",
    "    # Convert frames to grayscale\n",
    "    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Parameters for Lucas-Kanade optical flow\n",
    "    lk_params = dict(winSize=(15, 15),\n",
    "                     maxLevel=2,\n",
    "                     criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "    # Compute optical flow\n",
    "    flow, _ = cv2.calcOpticalFlowPyrLK(gray1, gray2, None, None, **lk_params)\n",
    "\n",
    "    return flow\n",
    "\n",
    "# Example usage\n",
    "frame1 = cv2.imread('My.jpg')\n",
    "frame2 = cv2.imread('My.jpg')\n",
    "\n",
    "# Compute optical flow\n",
    "#flow = compute_optical_flow(frame1, frame2)\n",
    "\n",
    "# Interpolation factor\n",
    "alpha = 0.5\n",
    "\n",
    "# Interpolate frames\n",
    "#interpolated_frame = interpolate_frames(frame1, frame2, flow, alpha)\n",
    "\n",
    "# Display or save interpolated frame\n",
    "#cv2.imshow('Interpolated Frame', interpolated_frame)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d1ca04-7663-4771-850a-a9c0708686a8",
   "metadata": {},
   "source": [
    "# Video Summarization:\r\n",
    "\r\n",
    "Generating concise representations of videos by selecting key frames or segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9dc72-85ee-4a06-941a-78a87f35a502",
   "metadata": {},
   "source": [
    "Video summarization is a process that involves creating a concise and informative summary of a video by selecting key frames or segments that capture the most important content. This process is particularly useful for efficiently browsing large video collections, enabling quick access to the most relevant parts of a video, and reducing the time required to view the entire content.\r\n",
    "\r\n",
    "### Types of Video Summarization\r\n",
    "\r\n",
    "1. **Static Video Summarization (Keyframe Extraction):**\r\n",
    "   - **Objective:** Select a set of representative frames (keyframes) from the video that best capture the main content.\r\n",
    "   - **Approach:** Techniques can include clustering, edge detection, motion analysis, and more.\r\n",
    "\r\n",
    "2. **Dynamic Video Summarization (Video Skimming):**\r\n",
    "   - **Objective:** Create a shorter video that includes the most important segments.\r\n",
    "   - **Approach:** Techniques often involve identifying significant scenes based on various criteria such as motion, audio, and semantic content.\r\n",
    "\r\n",
    "### Key Techniques in Video Summarization\r\n",
    "\r\n",
    "#### 1. Clustering-based Methods\r\n",
    "- **K-means Clustering:** Frames are grouped into clusters based on visual features. The centroid of each cluster represents a keyframe.\r\n",
    "- **Equation:**\r\n",
    "  \\[\r\n",
    "  J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\| x - \\mu_i \\|^2\r\n",
    "  \\]\r\n",
    "  where \\( J \\) is the objective function, \\( k \\) is the number of clusters, \\( x \\) is a data point (frame), and \\( \\mu_i \\) is the centroid of cluster \\( C_i \\).\r\n",
    "\r\n",
    "#### 2. Graph-based Methods\r\n",
    "- **Shot Boundary Detection:** The video is divided into shots, and a graph is constructed where nodes represent shots and edges represent similarities.\r\n",
    "- **Dominant Set Clustering:** Finds the most representative frames in the graph.\r\n",
    "- **Equation:**\r\n",
    "  \\[\r\n",
    "  W(i,j) = \\text{similarity}(i,j)\r\n",
    "  \\]\r\n",
    "  where \\( W(i,j) \\) is the weight (similarity) between frames \\( i \\) and \\( j \\).\r\n",
    "\r\n",
    "#### 3. Semantic Analysis\r\n",
    "- **Object and Scene Recognition:** Use of deep learning models to recognize objects and scenes, which helps in identifying important segments.\r\n",
    "- **Text and Speech Analysis:** Transcription and analysis of spoken content to find important segments.\r\n",
    "- **Equation:**\r\n",
    "  \\[\r\n",
    "  \\text{Importance}(f) = \\alpha \\cdot \\text{VisualScore}(f) + \\beta \\cdot \\text{AudioScore}(f) + \\gamma \\cdot \\text{TextScore}(f)\r\n",
    "  \\]\r\n",
    "  where \\( \\alpha, \\beta, \\gamma \\) are weights, and \\( f \\) is a frame or segment.\r\n",
    "\r\n",
    "#### 4. Attention Mechanisms\r\n",
    "- **Deep Learning Models:** Use of attention mechanisms to focus on important parts of the video. Models such as transformers can weigh the importance of different parts of the input.\r\n",
    "- **Equation:**\r\n",
    "  \\[\r\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\r\n",
    "  \\]\r\n",
    "  where \\( Q \\) is the query, \\( K \\) is the key, \\( V \\) is the value, and \\( d_k \\) is the dimension of the key.\r\n",
    "\r\n",
    "### Evaluation Metrics\r\n",
    "- **Precision and Recall:** Measure the accuracy of the selected keyframes or segments compared to a ground truth.\r\n",
    "  \\[\r\n",
    "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\r\n",
    "  \\]\r\n",
    "  \\[\r\n",
    "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\r\n",
    "  \\]\r\n",
    "- **F-Score:** Harmonic mean of precision and recall.\r\n",
    "  \\[\r\n",
    "  F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\r\n",
    "  \\]\r\n",
    "- **User Studies:** Subjective evaluation by human viewers to assess the quality and usefulness of the summaries.\r\n",
    "\r\n",
    "### Applications\r\n",
    "- **Content Browsing:** Quickly navigate through large video datasets.\r\n",
    "- **Video Surveillance:** Identify key events from security footage.\r\n",
    "- **Media and Entertainment:** Generate trailers or highlights for movies and sports.\r\n",
    "- **Education:** Create concise educational content.\r\n",
    "\r\n",
    "### Challenges\r\n",
    "- **Diversity:** Ensuring the summary captures a wide range of content.\r\n",
    "- **Relevance:** Selecting the most relevant frames or segments.\r\n",
    "- **Temporal Coherence:** Maintaining a logical flow in the summarized video.\r\n",
    "- **Scalability:** Efficiently summarizing videos with varying lengths and content types.\r\n",
    "\r\n",
    "### Future Directions\r\n",
    "- **Multimodal Summarization:** Combining visual, auditory, and textual information for richer summaries.\r\n",
    "- **Personalization:** Tailoring summaries based on user preferences and viewing history.\r\n",
    "- **Real-time Summarization:** Developing systems that can summarize live video streams.\r\n",
    "\r\n",
    "In summary, video summarization leverages a variety of techniques from machine learning, computer vision, and natural language processing to create concise and informative representations of videos. By addressing the challenges and improving the techniques, the field continues to evolve towards more efficient and effective summarization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ffa34b-7000-4e32-a4ea-a5092ec15974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\varun\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, interval=30):\n",
    "    \"\"\"Extract frames from a video at regular intervals.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    success, frame = cap.read()\n",
    "    count = 0\n",
    "\n",
    "    while success:\n",
    "        if count % interval == 0:\n",
    "            frames.append(frame)\n",
    "        success, frame = cap.read()\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def compute_histograms(frames):\n",
    "    \"\"\"Compute color histograms for a list of frames.\"\"\"\n",
    "    histograms = []\n",
    "    for frame in frames:\n",
    "        hist = cv2.calcHist([frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "        hist = cv2.normalize(hist, hist).flatten()\n",
    "        histograms.append(hist)\n",
    "    return histograms\n",
    "\n",
    "def cluster_histograms(histograms, n_clusters=5):\n",
    "    \"\"\"Cluster histograms using K-means.\"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(histograms)\n",
    "    return labels\n",
    "\n",
    "def select_key_frames(frames, labels):\n",
    "    \"\"\"Select one key frame from each cluster.\"\"\"\n",
    "    key_frames = []\n",
    "    for cluster in np.unique(labels):\n",
    "        indices = np.where(labels == cluster)[0]\n",
    "        key_frame_idx = indices[len(indices) // 2]\n",
    "        key_frames.append(frames[key_frame_idx])\n",
    "    return key_frames\n",
    "\n",
    "def save_frames(frames, output_dir):\n",
    "    \"\"\"Save key frames to the specified directory.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for i, frame in enumerate(frames):\n",
    "        output_path = os.path.join(output_dir, f\"key_frame_{i+1}.jpg\")\n",
    "        cv2.imwrite(output_path, frame)\n",
    "\n",
    "def summarize_video(video_path, output_dir, frame_interval=30, n_clusters=5):\n",
    "    \"\"\"Summarize the video by selecting key frames.\"\"\"\n",
    "    frames = extract_frames(video_path, frame_interval)\n",
    "    histograms = compute_histograms(frames)\n",
    "    labels = cluster_histograms(histograms, n_clusters)\n",
    "    key_frames = select_key_frames(frames, labels)\n",
    "    save_frames(key_frames, output_dir)\n",
    "\n",
    "# Example usage\n",
    "video_path = 'video.mp4'\n",
    "output_dir = 'path/to/output/directory'\n",
    "summarize_video(video_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb56095-9623-4055-b3fe-51d81842f04f",
   "metadata": {},
   "source": [
    "# Video Annotation:\r\n",
    "\r\n",
    "Adding metadata or labels to video frames for analysis or visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d244d-d36c-4caa-8664-ea449687079f",
   "metadata": {},
   "source": [
    "## Video Annotation: Adding Metadata or Labels to Video Frames\r\n",
    "\r\n",
    "Video annotation is the process of adding metadata or labels to video frames to facilitate analysis, visualization, and various applications like object detection, activity recognition, and machine learning model training. This process involves identifying and marking elements within the video frames, such as objects, actions, and events, to create a structured dataset.\r\n",
    "\r\n",
    "### Types of Video Annotation\r\n",
    "\r\n",
    "1. **Bounding Boxes**:\r\n",
    "   - Rectangular boxes are drawn around objects to define their position and size.\r\n",
    "   - **Equation**: \r\n",
    "     \\[\r\n",
    "     \\text{Bounding Box} = (x, y, w, h)\r\n",
    "     \\]\r\n",
    "     where \\((x, y)\\) is the top-left corner, and \\(w, h\\) are the width and height.\r\n",
    "\r\n",
    "2. **Polygonal Segmentation**:\r\n",
    "   - Polygons are drawn around objects to capture their precise shape.\r\n",
    "   - **Equation**:\r\n",
    "     \\[\r\n",
    "     \\text{Polygon} = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\}\r\n",
    "     \\]\r\n",
    "     where \\((x_i, y_i)\\) are the vertices of the polygon.\r\n",
    "\r\n",
    "3. **Key Points**:\r\n",
    "   - Specific points on an object are marked, often used for facial features or body joints.\r\n",
    "   - **Equation**:\r\n",
    "     \\[\r\n",
    "     \\text{Key Points} = \\{(x_1, y_1), (x_2, y_2), \\ldots, (x_k, y_k)\\}\r\n",
    "     \\]\r\n",
    "     where \\((x_i, y_i)\\) are the coordinates of key points.\r\n",
    "\r\n",
    "4. **Lines and Splines**:\r\n",
    "   - Lines or curves are drawn to annotate boundaries or paths.\r\n",
    "   - **Equation** (for a line):\r\n",
    "     \\[\r\n",
    "     \\text{Line} = \\{(x_1, y_1), (x_2, y_2)\\}\r\n",
    "     \\]\r\n",
    "\r\n",
    "5. **Semantic Segmentation**:\r\n",
    "   - Each pixel is labeled with a class, creating a mask for the object.\r\n",
    "   - **Equation**:\r\n",
    "     \\[\r\n",
    "     \\text{Mask} = M_{ij}\r\n",
    "     \\]\r\n",
    "     where \\(M_{ij}\\) is the class label for the pixel at position \\((i, j)\\).\r\n",
    "\r\n",
    "### Applications of Video Annotation\r\n",
    "\r\n",
    "1. **Object Detection and Tracking**:\r\n",
    "   - Used in autonomous driving, surveillance, and robotics to identify and follow objects.\r\n",
    "\r\n",
    "2. **Action Recognition**:\r\n",
    "   - Understanding and classifying actions in sports, security, and entertainment.\r\n",
    "\r\n",
    "3. **Healthcare**:\r\n",
    "   - Analyzing medical videos for diagnosis and research.\r\n",
    "\r\n",
    "4. **Training Machine Learning Models**:\r\n",
    "   - Creating labeled datasets to train models for computer vision tasks.\r\n",
    "\r\n",
    "### Process of Video Annotation\r\n",
    "\r\n",
    "1. **Data Collection**:\r\n",
    "   - Gather videos from various sources such as cameras, drones, or synthetic data.\r\n",
    "\r\n",
    "2. **Annotation Tools**:\r\n",
    "   - Use tools like CVAT, Labelbox, or VATIC to add annotations.\r\n",
    "   \r\n",
    "3. **Annotation**:\r\n",
    "   - Human annotators or automated systems mark the frames with the required labels.\r\n",
    "   \r\n",
    "4. **Quality Control**:\r\n",
    "   - Review and correct annotations to ensure accuracy and consistency.\r\n",
    "   \r\n",
    "5. **Exporting Annotations**:\r\n",
    "   - Save the annotations in formats like JSON, XML, or CSV for further use.\r\n",
    "\r\n",
    "### Example of a Simple Annotation Workflow\r\n",
    "\r\n",
    "1. **Load Video**:\r\n",
    "   ```python\r\n",
    "   import cv2\r\n",
    "   \r\n",
    "   video_path = \"video.mp4\"\r\n",
    "   cap = cv2.VideoCapture(video_path)\r\n",
    "   ```\r\n",
    "\r\n",
    "2. **Annotate Frames**:\r\n",
    "   ```python\r\n",
    "   annotations = []\r\n",
    "   \r\n",
    "   while cap.isOpened():\r\n",
    "       ret, frame = cap.read()\r\n",
    "       if not ret:\r\n",
    "           break\r\n",
    "       \r\n",
    "       # Assume we use a predefined function to get bounding boxes\r\n",
    "       bounding_boxes = detect_objects(frame)\r\n",
    "       \r\n",
    "       for box in bounding_boxes:\r\n",
    "           annotations.append({\r\n",
    "               'frame': int(cap.get(cv2.CAP_PROP_POS_FRAMES)),\r\n",
    "               'x': box[0],\r\n",
    "               'y': box[1],\r\n",
    "               'width': box[2],\r\n",
    "               'height': box[3]\r\n",
    "           })\r\n",
    "   ```\r\n",
    "\r\n",
    "3. **Save Annotations**:\r\n",
    "   ```python\r\n",
    "   import json\r\n",
    "   \r\n",
    "   with open(\"annotations.json\", \"w\") as f:\r\n",
    "       json.dump(annotations, f)\r\n",
    "   ```\r\n",
    "\r\n",
    "### Mathematical Models and Algorithms\r\n",
    "\r\n",
    "1. **Intersection over Union (IoU)**:\r\n",
    "   - Measures the overlap between two bounding boxes.\r\n",
    "   - **Equation**:\r\n",
    "     \\[\r\n",
    "     \\text{IoU} = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}\r\n",
    "     \\]\r\n",
    "\r\n",
    "2. **Non-Maximum Suppression (NMS)**:\r\n",
    "   - Reduces redundant bounding boxes by selecting the best ones.\r\n",
    "   - **Algorithm**:\r\n",
    "     1. Sort detected boxes by confidence score.\r\n",
    "     2. Select the highest score box and remove boxes with IoU above a threshold.\r\n",
    "     3. Repeat until no more boxes are left.\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "Video annotation is a crucial step in the preparation of data for video analysis and computer vision applications. By accurately labeling video frames, we can train and evaluate models that perform tasks like object detection, tracking, and action recognition. The choice of annotation method and tools depends on the specific requirements of the project and the nature of the video data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5dd920b-1781-4131-b57e-f44e3046dabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed frame 1/210\n",
      "Processed frame 2/210\n",
      "Processed frame 3/210\n",
      "Processed frame 4/210\n",
      "Processed frame 5/210\n",
      "Processed frame 6/210\n",
      "Processed frame 7/210\n",
      "Processed frame 8/210\n",
      "Processed frame 9/210\n",
      "Processed frame 10/210\n",
      "Processed frame 11/210\n",
      "Processed frame 12/210\n",
      "Processed frame 13/210\n",
      "Processed frame 14/210\n",
      "Processed frame 15/210\n",
      "Processed frame 16/210\n",
      "Processed frame 17/210\n",
      "Processed frame 18/210\n",
      "Processed frame 19/210\n",
      "Processed frame 20/210\n",
      "Processed frame 21/210\n",
      "Processed frame 22/210\n",
      "Processed frame 23/210\n",
      "Processed frame 24/210\n",
      "Processed frame 25/210\n",
      "Processed frame 26/210\n",
      "Processed frame 27/210\n",
      "Processed frame 28/210\n",
      "Processed frame 29/210\n",
      "Processed frame 30/210\n",
      "Processed frame 31/210\n",
      "Processed frame 32/210\n",
      "Processed frame 33/210\n",
      "Processed frame 34/210\n",
      "Processed frame 35/210\n",
      "Processed frame 36/210\n",
      "Processed frame 37/210\n",
      "Processed frame 38/210\n",
      "Processed frame 39/210\n",
      "Processed frame 40/210\n",
      "Processed frame 41/210\n",
      "Processed frame 42/210\n",
      "Processed frame 43/210\n",
      "Processed frame 44/210\n",
      "Processed frame 45/210\n",
      "Processed frame 46/210\n",
      "Processed frame 47/210\n",
      "Processed frame 48/210\n",
      "Processed frame 49/210\n",
      "Processed frame 50/210\n",
      "Processed frame 51/210\n",
      "Processed frame 52/210\n",
      "Processed frame 53/210\n",
      "Processed frame 54/210\n",
      "Processed frame 55/210\n",
      "Processed frame 56/210\n",
      "Processed frame 57/210\n",
      "Processed frame 58/210\n",
      "Processed frame 59/210\n",
      "Processed frame 60/210\n",
      "Processed frame 61/210\n",
      "Processed frame 62/210\n",
      "Processed frame 63/210\n",
      "Processed frame 64/210\n",
      "Processed frame 65/210\n",
      "Processed frame 66/210\n",
      "Processed frame 67/210\n",
      "Processed frame 68/210\n",
      "Processed frame 69/210\n",
      "Processed frame 70/210\n",
      "Processed frame 71/210\n",
      "Processed frame 72/210\n",
      "Processed frame 73/210\n",
      "Processed frame 74/210\n",
      "Processed frame 75/210\n",
      "Processed frame 76/210\n",
      "Processed frame 77/210\n",
      "Processed frame 78/210\n",
      "Processed frame 79/210\n",
      "Processed frame 80/210\n",
      "Processed frame 81/210\n",
      "Processed frame 82/210\n",
      "Processed frame 83/210\n",
      "Processed frame 84/210\n",
      "Processed frame 85/210\n",
      "Processed frame 86/210\n",
      "Processed frame 87/210\n",
      "Processed frame 88/210\n",
      "Processed frame 89/210\n",
      "Processed frame 90/210\n",
      "Processed frame 91/210\n",
      "Processed frame 92/210\n",
      "Processed frame 93/210\n",
      "Processed frame 94/210\n",
      "Processed frame 95/210\n",
      "Processed frame 96/210\n",
      "Processed frame 97/210\n",
      "Processed frame 98/210\n",
      "Processed frame 99/210\n",
      "Processed frame 100/210\n",
      "Processed frame 101/210\n",
      "Processed frame 102/210\n",
      "Processed frame 103/210\n",
      "Processed frame 104/210\n",
      "Processed frame 105/210\n",
      "Processed frame 106/210\n",
      "Processed frame 107/210\n",
      "Processed frame 108/210\n",
      "Processed frame 109/210\n",
      "Processed frame 110/210\n",
      "Processed frame 111/210\n",
      "Processed frame 112/210\n",
      "Processed frame 113/210\n",
      "Processed frame 114/210\n",
      "Processed frame 115/210\n",
      "Processed frame 116/210\n",
      "Processed frame 117/210\n",
      "Processed frame 118/210\n",
      "Processed frame 119/210\n",
      "Processed frame 120/210\n",
      "Processed frame 121/210\n",
      "Processed frame 122/210\n",
      "Processed frame 123/210\n",
      "Processed frame 124/210\n",
      "Processed frame 125/210\n",
      "Processed frame 126/210\n",
      "Processed frame 127/210\n",
      "Processed frame 128/210\n",
      "Processed frame 129/210\n",
      "Processed frame 130/210\n",
      "Processed frame 131/210\n",
      "Processed frame 132/210\n",
      "Processed frame 133/210\n",
      "Processed frame 134/210\n",
      "Processed frame 135/210\n",
      "Processed frame 136/210\n",
      "Processed frame 137/210\n",
      "Processed frame 138/210\n",
      "Processed frame 139/210\n",
      "Processed frame 140/210\n",
      "Processed frame 141/210\n",
      "Processed frame 142/210\n",
      "Processed frame 143/210\n",
      "Processed frame 144/210\n",
      "Processed frame 145/210\n",
      "Processed frame 146/210\n",
      "Processed frame 147/210\n",
      "Processed frame 148/210\n",
      "Processed frame 149/210\n",
      "Processed frame 150/210\n",
      "Processed frame 151/210\n",
      "Processed frame 152/210\n",
      "Processed frame 153/210\n",
      "Processed frame 154/210\n",
      "Processed frame 155/210\n",
      "Processed frame 156/210\n",
      "Processed frame 157/210\n",
      "Processed frame 158/210\n",
      "Processed frame 159/210\n",
      "Processed frame 160/210\n",
      "Processed frame 161/210\n",
      "Processed frame 162/210\n",
      "Processed frame 163/210\n",
      "Processed frame 164/210\n",
      "Processed frame 165/210\n",
      "Processed frame 166/210\n",
      "Processed frame 167/210\n",
      "Processed frame 168/210\n",
      "Processed frame 169/210\n",
      "Processed frame 170/210\n",
      "Processed frame 171/210\n",
      "Processed frame 172/210\n",
      "Processed frame 173/210\n",
      "Processed frame 174/210\n",
      "Processed frame 175/210\n",
      "Processed frame 176/210\n",
      "Processed frame 177/210\n",
      "Processed frame 178/210\n",
      "Processed frame 179/210\n",
      "Processed frame 180/210\n",
      "Processed frame 181/210\n",
      "Processed frame 182/210\n",
      "Processed frame 183/210\n",
      "Processed frame 184/210\n",
      "Processed frame 185/210\n",
      "Processed frame 186/210\n",
      "Processed frame 187/210\n",
      "Processed frame 188/210\n",
      "Processed frame 189/210\n",
      "Processed frame 190/210\n",
      "Processed frame 191/210\n",
      "Processed frame 192/210\n",
      "Processed frame 193/210\n",
      "Processed frame 194/210\n",
      "Processed frame 195/210\n",
      "Processed frame 196/210\n",
      "Processed frame 197/210\n",
      "Processed frame 198/210\n",
      "Processed frame 199/210\n",
      "Processed frame 200/210\n",
      "Processed frame 201/210\n",
      "Processed frame 202/210\n",
      "Processed frame 203/210\n",
      "Processed frame 204/210\n",
      "Processed frame 205/210\n",
      "Processed frame 206/210\n",
      "Processed frame 207/210\n",
      "Processed frame 208/210\n",
      "Processed frame 209/210\n",
      "Processed frame 210/210\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Global variables\n",
    "drawing = False\n",
    "ix, iy = -1, -1\n",
    "annotations = []\n",
    "\n",
    "# Mouse callback function\n",
    "def draw_rectangle(event, x, y, flags, param):\n",
    "    global ix, iy, drawing, frame, annotations\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        ix, iy = x, y\n",
    "\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if drawing:\n",
    "            frame_temp = frame.copy()\n",
    "            cv2.rectangle(frame_temp, (ix, iy), (x, y), (0, 255, 0), 2)\n",
    "            cv2.imshow('frame', frame_temp)\n",
    "\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        cv2.rectangle(frame, (ix, iy), (x, y), (0, 255, 0), 2)\n",
    "        annotations.append((ix, iy, x, y))\n",
    "\n",
    "def annotate_video(video_path):\n",
    "    global frame\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_idx = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_idx += 1\n",
    "        cv2.namedWindow('frame')\n",
    "        cv2.setMouseCallback('frame', draw_rectangle)\n",
    "\n",
    "        while True:\n",
    "            cv2.imshow('frame', frame)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "            if key == ord('n'):  # Next frame\n",
    "                annotations.clear()\n",
    "                break\n",
    "            elif key == ord('q'):  # Quit\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                return\n",
    "\n",
    "        print(f'Processed frame {frame_idx}/{frame_count}')\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save annotations\n",
    "    with open('annotations.txt', 'w') as f:\n",
    "        for annot in annotations:\n",
    "            f.write(f'{annot}\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = 'video.mp4'\n",
    "    annotate_video(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a1baa-df6a-4161-9076-f39d075df80e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
